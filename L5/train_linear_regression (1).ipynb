{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "(In the following you need not present your journal in the Qa+b+c+ etc. order. You could just present the final code with test and comments.)\n",
    "\n",
    "## Training Your Own Linear Regressor\n",
    "\n",
    "Create a linear regressor, with a Scikit-learn compatible fit-predict interface. You should implement every detail of the linear regressor in Python, using whatever libraries, say `numpy`, you want (except a linear regressor itself).\n",
    "\n",
    "Below is a primitive _get-started_ skeleton for your implementation. Keep the class name `MyLinReg`, which is used in the test sequence later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 0.4091\n",
      "Epoch 2/1000, Loss: 0.3833\n",
      "Epoch 3/1000, Loss: 0.3619\n",
      "Epoch 4/1000, Loss: 0.3423\n",
      "Epoch 5/1000, Loss: 0.3238\n",
      "Epoch 6/1000, Loss: 0.3059\n",
      "Epoch 7/1000, Loss: 0.2908\n",
      "Epoch 8/1000, Loss: 0.2750\n",
      "Epoch 9/1000, Loss: 0.2600\n",
      "Epoch 10/1000, Loss: 0.2469\n",
      "Epoch 11/1000, Loss: 0.2348\n",
      "Epoch 12/1000, Loss: 0.2236\n",
      "Epoch 13/1000, Loss: 0.2131\n",
      "Epoch 14/1000, Loss: 0.2033\n",
      "Epoch 15/1000, Loss: 0.1935\n",
      "Epoch 16/1000, Loss: 0.1851\n",
      "Epoch 17/1000, Loss: 0.1773\n",
      "Epoch 18/1000, Loss: 0.1705\n",
      "Epoch 19/1000, Loss: 0.1636\n",
      "Epoch 20/1000, Loss: 0.1576\n",
      "Epoch 21/1000, Loss: 0.1521\n",
      "Epoch 22/1000, Loss: 0.1470\n",
      "Epoch 23/1000, Loss: 0.1423\n",
      "Epoch 24/1000, Loss: 0.1390\n",
      "Epoch 25/1000, Loss: 0.1342\n",
      "Epoch 26/1000, Loss: 0.1310\n",
      "Epoch 27/1000, Loss: 0.1285\n",
      "Epoch 28/1000, Loss: 0.1251\n",
      "Epoch 29/1000, Loss: 0.1223\n",
      "Epoch 30/1000, Loss: 0.1204\n",
      "Epoch 31/1000, Loss: 0.1175\n",
      "Epoch 32/1000, Loss: 0.1160\n",
      "Epoch 33/1000, Loss: 0.1139\n",
      "Epoch 34/1000, Loss: 0.1123\n",
      "Epoch 35/1000, Loss: 0.1109\n",
      "Epoch 36/1000, Loss: 0.1099\n",
      "Epoch 37/1000, Loss: 0.1089\n",
      "Epoch 38/1000, Loss: 0.1076\n",
      "Epoch 39/1000, Loss: 0.1069\n",
      "Epoch 40/1000, Loss: 0.1062\n",
      "Epoch 41/1000, Loss: 0.1053\n",
      "Epoch 42/1000, Loss: 0.1049\n",
      "Epoch 43/1000, Loss: 0.1043\n",
      "Epoch 44/1000, Loss: 0.1037\n",
      "Epoch 45/1000, Loss: 0.1032\n",
      "Epoch 46/1000, Loss: 0.1028\n",
      "Epoch 47/1000, Loss: 0.1025\n",
      "Epoch 48/1000, Loss: 0.1022\n",
      "Epoch 49/1000, Loss: 0.1021\n",
      "Epoch 50/1000, Loss: 0.1018\n",
      "Epoch 51/1000, Loss: 0.1016\n",
      "Epoch 52/1000, Loss: 0.1013\n",
      "Epoch 53/1000, Loss: 0.1016\n",
      "Epoch 54/1000, Loss: 0.1009\n",
      "Epoch 55/1000, Loss: 0.1009\n",
      "Epoch 56/1000, Loss: 0.1008\n",
      "Epoch 57/1000, Loss: 0.1008\n",
      "Convergence reached at epoch 57.\n",
      "Train RMSE: 0.1008\n",
      "Test RMSE: 0.1080\n",
      "Epoch 1/1000, Loss: 0.5129\n",
      "Epoch 2/1000, Loss: 0.4326\n",
      "Epoch 3/1000, Loss: 0.4069\n",
      "Epoch 4/1000, Loss: 0.3959\n",
      "Epoch 5/1000, Loss: 0.3887\n",
      "Epoch 6/1000, Loss: 0.3824\n",
      "Epoch 7/1000, Loss: 0.3764\n",
      "Epoch 8/1000, Loss: 0.3706\n",
      "Epoch 9/1000, Loss: 0.3649\n",
      "Epoch 10/1000, Loss: 0.3593\n",
      "Epoch 11/1000, Loss: 0.3538\n",
      "Epoch 12/1000, Loss: 0.3485\n",
      "Epoch 13/1000, Loss: 0.3432\n",
      "Epoch 14/1000, Loss: 0.3380\n",
      "Epoch 15/1000, Loss: 0.3329\n",
      "Epoch 16/1000, Loss: 0.3279\n",
      "Epoch 17/1000, Loss: 0.3230\n",
      "Epoch 18/1000, Loss: 0.3181\n",
      "Epoch 19/1000, Loss: 0.3134\n",
      "Epoch 20/1000, Loss: 0.3088\n",
      "Epoch 21/1000, Loss: 0.3042\n",
      "Epoch 22/1000, Loss: 0.2997\n",
      "Epoch 23/1000, Loss: 0.2954\n",
      "Epoch 24/1000, Loss: 0.2910\n",
      "Epoch 25/1000, Loss: 0.2868\n",
      "Epoch 26/1000, Loss: 0.2827\n",
      "Epoch 27/1000, Loss: 0.2786\n",
      "Epoch 28/1000, Loss: 0.2746\n",
      "Epoch 29/1000, Loss: 0.2707\n",
      "Epoch 30/1000, Loss: 0.2669\n",
      "Epoch 31/1000, Loss: 0.2631\n",
      "Epoch 32/1000, Loss: 0.2594\n",
      "Epoch 33/1000, Loss: 0.2558\n",
      "Epoch 34/1000, Loss: 0.2523\n",
      "Epoch 35/1000, Loss: 0.2488\n",
      "Epoch 36/1000, Loss: 0.2454\n",
      "Epoch 37/1000, Loss: 0.2420\n",
      "Epoch 38/1000, Loss: 0.2387\n",
      "Epoch 39/1000, Loss: 0.2355\n",
      "Epoch 40/1000, Loss: 0.2324\n",
      "Epoch 41/1000, Loss: 0.2293\n",
      "Epoch 42/1000, Loss: 0.2263\n",
      "Epoch 43/1000, Loss: 0.2233\n",
      "Epoch 44/1000, Loss: 0.2204\n",
      "Epoch 45/1000, Loss: 0.2176\n",
      "Epoch 46/1000, Loss: 0.2148\n",
      "Epoch 47/1000, Loss: 0.2121\n",
      "Epoch 48/1000, Loss: 0.2094\n",
      "Epoch 49/1000, Loss: 0.2068\n",
      "Epoch 50/1000, Loss: 0.2042\n",
      "Epoch 51/1000, Loss: 0.2017\n",
      "Epoch 52/1000, Loss: 0.1993\n",
      "Epoch 53/1000, Loss: 0.1969\n",
      "Epoch 54/1000, Loss: 0.1945\n",
      "Epoch 55/1000, Loss: 0.1922\n",
      "Epoch 56/1000, Loss: 0.1900\n",
      "Epoch 57/1000, Loss: 0.1878\n",
      "Epoch 58/1000, Loss: 0.1856\n",
      "Epoch 59/1000, Loss: 0.1835\n",
      "Epoch 60/1000, Loss: 0.1815\n",
      "Epoch 61/1000, Loss: 0.1795\n",
      "Epoch 62/1000, Loss: 0.1775\n",
      "Epoch 63/1000, Loss: 0.1756\n",
      "Epoch 64/1000, Loss: 0.1737\n",
      "Epoch 65/1000, Loss: 0.1719\n",
      "Epoch 66/1000, Loss: 0.1701\n",
      "Epoch 67/1000, Loss: 0.1683\n",
      "Epoch 68/1000, Loss: 0.1666\n",
      "Epoch 69/1000, Loss: 0.1649\n",
      "Epoch 70/1000, Loss: 0.1633\n",
      "Epoch 71/1000, Loss: 0.1617\n",
      "Epoch 72/1000, Loss: 0.1601\n",
      "Epoch 73/1000, Loss: 0.1586\n",
      "Epoch 74/1000, Loss: 0.1571\n",
      "Epoch 75/1000, Loss: 0.1557\n",
      "Epoch 76/1000, Loss: 0.1543\n",
      "Epoch 77/1000, Loss: 0.1529\n",
      "Epoch 78/1000, Loss: 0.1515\n",
      "Epoch 79/1000, Loss: 0.1502\n",
      "Epoch 80/1000, Loss: 0.1490\n",
      "Epoch 81/1000, Loss: 0.1477\n",
      "Epoch 82/1000, Loss: 0.1465\n",
      "Epoch 83/1000, Loss: 0.1453\n",
      "Epoch 84/1000, Loss: 0.1441\n",
      "Epoch 85/1000, Loss: 0.1430\n",
      "Epoch 86/1000, Loss: 0.1419\n",
      "Epoch 87/1000, Loss: 0.1409\n",
      "Epoch 88/1000, Loss: 0.1398\n",
      "Epoch 89/1000, Loss: 0.1388\n",
      "Epoch 90/1000, Loss: 0.1378\n",
      "Epoch 91/1000, Loss: 0.1368\n",
      "Epoch 92/1000, Loss: 0.1359\n",
      "Epoch 93/1000, Loss: 0.1350\n",
      "Epoch 94/1000, Loss: 0.1341\n",
      "Epoch 95/1000, Loss: 0.1332\n",
      "Epoch 96/1000, Loss: 0.1324\n",
      "Epoch 97/1000, Loss: 0.1316\n",
      "Epoch 98/1000, Loss: 0.1308\n",
      "Epoch 99/1000, Loss: 0.1300\n",
      "Epoch 100/1000, Loss: 0.1293\n",
      "Epoch 101/1000, Loss: 0.1285\n",
      "Epoch 102/1000, Loss: 0.1278\n",
      "Epoch 103/1000, Loss: 0.1271\n",
      "Epoch 104/1000, Loss: 0.1265\n",
      "Epoch 105/1000, Loss: 0.1258\n",
      "Epoch 106/1000, Loss: 0.1252\n",
      "Epoch 107/1000, Loss: 0.1245\n",
      "Epoch 108/1000, Loss: 0.1239\n",
      "Epoch 109/1000, Loss: 0.1234\n",
      "Epoch 110/1000, Loss: 0.1228\n",
      "Epoch 111/1000, Loss: 0.1222\n",
      "Epoch 112/1000, Loss: 0.1217\n",
      "Epoch 113/1000, Loss: 0.1212\n",
      "Epoch 114/1000, Loss: 0.1207\n",
      "Epoch 115/1000, Loss: 0.1202\n",
      "Epoch 116/1000, Loss: 0.1197\n",
      "Epoch 117/1000, Loss: 0.1193\n",
      "Epoch 118/1000, Loss: 0.1188\n",
      "Epoch 119/1000, Loss: 0.1184\n",
      "Epoch 120/1000, Loss: 0.1180\n",
      "Epoch 121/1000, Loss: 0.1175\n",
      "Epoch 122/1000, Loss: 0.1171\n",
      "Epoch 123/1000, Loss: 0.1168\n",
      "Epoch 124/1000, Loss: 0.1164\n",
      "Epoch 125/1000, Loss: 0.1160\n",
      "Epoch 126/1000, Loss: 0.1157\n",
      "Epoch 127/1000, Loss: 0.1153\n",
      "Epoch 128/1000, Loss: 0.1150\n",
      "Epoch 129/1000, Loss: 0.1147\n",
      "Epoch 130/1000, Loss: 0.1144\n",
      "Epoch 131/1000, Loss: 0.1141\n",
      "Epoch 132/1000, Loss: 0.1138\n",
      "Epoch 133/1000, Loss: 0.1135\n",
      "Epoch 134/1000, Loss: 0.1132\n",
      "Epoch 135/1000, Loss: 0.1130\n",
      "Epoch 136/1000, Loss: 0.1127\n",
      "Epoch 137/1000, Loss: 0.1124\n",
      "Epoch 138/1000, Loss: 0.1122\n",
      "Epoch 139/1000, Loss: 0.1120\n",
      "Epoch 140/1000, Loss: 0.1117\n",
      "Epoch 141/1000, Loss: 0.1115\n",
      "Epoch 142/1000, Loss: 0.1113\n",
      "Epoch 143/1000, Loss: 0.1111\n",
      "Epoch 144/1000, Loss: 0.1109\n",
      "Epoch 145/1000, Loss: 0.1107\n",
      "Epoch 146/1000, Loss: 0.1105\n",
      "Epoch 147/1000, Loss: 0.1103\n",
      "Epoch 148/1000, Loss: 0.1102\n",
      "Epoch 149/1000, Loss: 0.1100\n",
      "Epoch 150/1000, Loss: 0.1098\n",
      "Epoch 151/1000, Loss: 0.1097\n",
      "Convergence reached at epoch 151.\n",
      "Epoch 1/1000, Loss: 0.5115\n",
      "Epoch 2/1000, Loss: 0.4306\n",
      "Epoch 3/1000, Loss: 0.4057\n",
      "Epoch 4/1000, Loss: 0.3950\n",
      "Epoch 5/1000, Loss: 0.3878\n",
      "Epoch 6/1000, Loss: 0.3814\n",
      "Epoch 7/1000, Loss: 0.3753\n",
      "Epoch 8/1000, Loss: 0.3694\n",
      "Epoch 9/1000, Loss: 0.3635\n",
      "Epoch 10/1000, Loss: 0.3578\n",
      "Epoch 11/1000, Loss: 0.3522\n",
      "Epoch 12/1000, Loss: 0.3467\n",
      "Epoch 13/1000, Loss: 0.3413\n",
      "Epoch 14/1000, Loss: 0.3359\n",
      "Epoch 15/1000, Loss: 0.3307\n",
      "Epoch 16/1000, Loss: 0.3256\n",
      "Epoch 17/1000, Loss: 0.3206\n",
      "Epoch 18/1000, Loss: 0.3157\n",
      "Epoch 19/1000, Loss: 0.3109\n",
      "Epoch 20/1000, Loss: 0.3061\n",
      "Epoch 21/1000, Loss: 0.3015\n",
      "Epoch 22/1000, Loss: 0.2969\n",
      "Epoch 23/1000, Loss: 0.2924\n",
      "Epoch 24/1000, Loss: 0.2881\n",
      "Epoch 25/1000, Loss: 0.2838\n",
      "Epoch 26/1000, Loss: 0.2795\n",
      "Epoch 27/1000, Loss: 0.2754\n",
      "Epoch 28/1000, Loss: 0.2713\n",
      "Epoch 29/1000, Loss: 0.2674\n",
      "Epoch 30/1000, Loss: 0.2635\n",
      "Epoch 31/1000, Loss: 0.2596\n",
      "Epoch 32/1000, Loss: 0.2559\n",
      "Epoch 33/1000, Loss: 0.2522\n",
      "Epoch 34/1000, Loss: 0.2486\n",
      "Epoch 35/1000, Loss: 0.2451\n",
      "Epoch 36/1000, Loss: 0.2417\n",
      "Epoch 37/1000, Loss: 0.2383\n",
      "Epoch 38/1000, Loss: 0.2350\n",
      "Epoch 39/1000, Loss: 0.2317\n",
      "Epoch 40/1000, Loss: 0.2285\n",
      "Epoch 41/1000, Loss: 0.2254\n",
      "Epoch 42/1000, Loss: 0.2224\n",
      "Epoch 43/1000, Loss: 0.2194\n",
      "Epoch 44/1000, Loss: 0.2164\n",
      "Epoch 45/1000, Loss: 0.2136\n",
      "Epoch 46/1000, Loss: 0.2108\n",
      "Epoch 47/1000, Loss: 0.2080\n",
      "Epoch 48/1000, Loss: 0.2053\n",
      "Epoch 49/1000, Loss: 0.2027\n",
      "Epoch 50/1000, Loss: 0.2001\n",
      "Epoch 51/1000, Loss: 0.1976\n",
      "Epoch 52/1000, Loss: 0.1952\n",
      "Epoch 53/1000, Loss: 0.1928\n",
      "Epoch 54/1000, Loss: 0.1904\n",
      "Epoch 55/1000, Loss: 0.1881\n",
      "Epoch 56/1000, Loss: 0.1858\n",
      "Epoch 57/1000, Loss: 0.1836\n",
      "Epoch 58/1000, Loss: 0.1815\n",
      "Epoch 59/1000, Loss: 0.1794\n",
      "Epoch 60/1000, Loss: 0.1773\n",
      "Epoch 61/1000, Loss: 0.1753\n",
      "Epoch 62/1000, Loss: 0.1733\n",
      "Epoch 63/1000, Loss: 0.1714\n",
      "Epoch 64/1000, Loss: 0.1696\n",
      "Epoch 65/1000, Loss: 0.1677\n",
      "Epoch 66/1000, Loss: 0.1659\n",
      "Epoch 67/1000, Loss: 0.1642\n",
      "Epoch 68/1000, Loss: 0.1625\n",
      "Epoch 69/1000, Loss: 0.1608\n",
      "Epoch 70/1000, Loss: 0.1592\n",
      "Epoch 71/1000, Loss: 0.1576\n",
      "Epoch 72/1000, Loss: 0.1561\n",
      "Epoch 73/1000, Loss: 0.1546\n",
      "Epoch 74/1000, Loss: 0.1531\n",
      "Epoch 75/1000, Loss: 0.1517\n",
      "Epoch 76/1000, Loss: 0.1503\n",
      "Epoch 77/1000, Loss: 0.1489\n",
      "Epoch 78/1000, Loss: 0.1476\n",
      "Epoch 79/1000, Loss: 0.1463\n",
      "Epoch 80/1000, Loss: 0.1450\n",
      "Epoch 81/1000, Loss: 0.1438\n",
      "Epoch 82/1000, Loss: 0.1426\n",
      "Epoch 83/1000, Loss: 0.1414\n",
      "Epoch 84/1000, Loss: 0.1403\n",
      "Epoch 85/1000, Loss: 0.1392\n",
      "Epoch 86/1000, Loss: 0.1381\n",
      "Epoch 87/1000, Loss: 0.1371\n",
      "Epoch 88/1000, Loss: 0.1360\n",
      "Epoch 89/1000, Loss: 0.1350\n",
      "Epoch 90/1000, Loss: 0.1341\n",
      "Epoch 91/1000, Loss: 0.1331\n",
      "Epoch 92/1000, Loss: 0.1322\n",
      "Epoch 93/1000, Loss: 0.1313\n",
      "Epoch 94/1000, Loss: 0.1305\n",
      "Epoch 95/1000, Loss: 0.1296\n",
      "Epoch 96/1000, Loss: 0.1288\n",
      "Epoch 97/1000, Loss: 0.1280\n",
      "Epoch 98/1000, Loss: 0.1272\n",
      "Epoch 99/1000, Loss: 0.1265\n",
      "Epoch 100/1000, Loss: 0.1257\n",
      "Epoch 101/1000, Loss: 0.1250\n",
      "Epoch 102/1000, Loss: 0.1243\n",
      "Epoch 103/1000, Loss: 0.1237\n",
      "Epoch 104/1000, Loss: 0.1230\n",
      "Epoch 105/1000, Loss: 0.1224\n",
      "Epoch 106/1000, Loss: 0.1218\n",
      "Epoch 107/1000, Loss: 0.1212\n",
      "Epoch 108/1000, Loss: 0.1206\n",
      "Epoch 109/1000, Loss: 0.1200\n",
      "Epoch 110/1000, Loss: 0.1195\n",
      "Epoch 111/1000, Loss: 0.1190\n",
      "Epoch 112/1000, Loss: 0.1185\n",
      "Epoch 113/1000, Loss: 0.1180\n",
      "Epoch 114/1000, Loss: 0.1175\n",
      "Epoch 115/1000, Loss: 0.1170\n",
      "Epoch 116/1000, Loss: 0.1166\n",
      "Epoch 117/1000, Loss: 0.1161\n",
      "Epoch 118/1000, Loss: 0.1157\n",
      "Epoch 119/1000, Loss: 0.1153\n",
      "Epoch 120/1000, Loss: 0.1149\n",
      "Epoch 121/1000, Loss: 0.1145\n",
      "Epoch 122/1000, Loss: 0.1141\n",
      "Epoch 123/1000, Loss: 0.1137\n",
      "Epoch 124/1000, Loss: 0.1134\n",
      "Epoch 125/1000, Loss: 0.1130\n",
      "Epoch 126/1000, Loss: 0.1127\n",
      "Epoch 127/1000, Loss: 0.1124\n",
      "Epoch 128/1000, Loss: 0.1121\n",
      "Epoch 129/1000, Loss: 0.1118\n",
      "Epoch 130/1000, Loss: 0.1115\n",
      "Epoch 131/1000, Loss: 0.1112\n",
      "Epoch 132/1000, Loss: 0.1109\n",
      "Epoch 133/1000, Loss: 0.1106\n",
      "Epoch 134/1000, Loss: 0.1104\n",
      "Epoch 135/1000, Loss: 0.1101\n",
      "Epoch 136/1000, Loss: 0.1099\n",
      "Epoch 137/1000, Loss: 0.1097\n",
      "Epoch 138/1000, Loss: 0.1094\n",
      "Epoch 139/1000, Loss: 0.1092\n",
      "Epoch 140/1000, Loss: 0.1090\n",
      "Epoch 141/1000, Loss: 0.1088\n",
      "Epoch 142/1000, Loss: 0.1086\n",
      "Epoch 143/1000, Loss: 0.1084\n",
      "Epoch 144/1000, Loss: 0.1082\n",
      "Epoch 145/1000, Loss: 0.1080\n",
      "Epoch 146/1000, Loss: 0.1079\n",
      "Convergence reached at epoch 146.\n",
      "Epoch 1/1000, Loss: 0.5117\n",
      "Epoch 2/1000, Loss: 0.4474\n",
      "Epoch 3/1000, Loss: 0.4240\n",
      "Epoch 4/1000, Loss: 0.4126\n",
      "Epoch 5/1000, Loss: 0.4046\n",
      "Epoch 6/1000, Loss: 0.3976\n",
      "Epoch 7/1000, Loss: 0.3909\n",
      "Epoch 8/1000, Loss: 0.3844\n",
      "Epoch 9/1000, Loss: 0.3780\n",
      "Epoch 10/1000, Loss: 0.3718\n",
      "Epoch 11/1000, Loss: 0.3657\n",
      "Epoch 12/1000, Loss: 0.3597\n",
      "Epoch 13/1000, Loss: 0.3538\n",
      "Epoch 14/1000, Loss: 0.3480\n",
      "Epoch 15/1000, Loss: 0.3424\n",
      "Epoch 16/1000, Loss: 0.3368\n",
      "Epoch 17/1000, Loss: 0.3314\n",
      "Epoch 18/1000, Loss: 0.3260\n",
      "Epoch 19/1000, Loss: 0.3208\n",
      "Epoch 20/1000, Loss: 0.3157\n",
      "Epoch 21/1000, Loss: 0.3106\n",
      "Epoch 22/1000, Loss: 0.3057\n",
      "Epoch 23/1000, Loss: 0.3008\n",
      "Epoch 24/1000, Loss: 0.2961\n",
      "Epoch 25/1000, Loss: 0.2914\n",
      "Epoch 26/1000, Loss: 0.2869\n",
      "Epoch 27/1000, Loss: 0.2824\n",
      "Epoch 28/1000, Loss: 0.2780\n",
      "Epoch 29/1000, Loss: 0.2737\n",
      "Epoch 30/1000, Loss: 0.2695\n",
      "Epoch 31/1000, Loss: 0.2654\n",
      "Epoch 32/1000, Loss: 0.2614\n",
      "Epoch 33/1000, Loss: 0.2574\n",
      "Epoch 34/1000, Loss: 0.2536\n",
      "Epoch 35/1000, Loss: 0.2498\n",
      "Epoch 36/1000, Loss: 0.2460\n",
      "Epoch 37/1000, Loss: 0.2424\n",
      "Epoch 38/1000, Loss: 0.2388\n",
      "Epoch 39/1000, Loss: 0.2353\n",
      "Epoch 40/1000, Loss: 0.2319\n",
      "Epoch 41/1000, Loss: 0.2286\n",
      "Epoch 42/1000, Loss: 0.2253\n",
      "Epoch 43/1000, Loss: 0.2221\n",
      "Epoch 44/1000, Loss: 0.2189\n",
      "Epoch 45/1000, Loss: 0.2159\n",
      "Epoch 46/1000, Loss: 0.2129\n",
      "Epoch 47/1000, Loss: 0.2099\n",
      "Epoch 48/1000, Loss: 0.2070\n",
      "Epoch 49/1000, Loss: 0.2042\n",
      "Epoch 50/1000, Loss: 0.2015\n",
      "Epoch 51/1000, Loss: 0.1988\n",
      "Epoch 52/1000, Loss: 0.1961\n",
      "Epoch 53/1000, Loss: 0.1936\n",
      "Epoch 54/1000, Loss: 0.1910\n",
      "Epoch 55/1000, Loss: 0.1886\n",
      "Epoch 56/1000, Loss: 0.1862\n",
      "Epoch 57/1000, Loss: 0.1838\n",
      "Epoch 58/1000, Loss: 0.1815\n",
      "Epoch 59/1000, Loss: 0.1793\n",
      "Epoch 60/1000, Loss: 0.1771\n",
      "Epoch 61/1000, Loss: 0.1750\n",
      "Epoch 62/1000, Loss: 0.1729\n",
      "Epoch 63/1000, Loss: 0.1708\n",
      "Epoch 64/1000, Loss: 0.1688\n",
      "Epoch 65/1000, Loss: 0.1669\n",
      "Epoch 66/1000, Loss: 0.1650\n",
      "Epoch 67/1000, Loss: 0.1631\n",
      "Epoch 68/1000, Loss: 0.1613\n",
      "Epoch 69/1000, Loss: 0.1595\n",
      "Epoch 70/1000, Loss: 0.1578\n",
      "Epoch 71/1000, Loss: 0.1561\n",
      "Epoch 72/1000, Loss: 0.1545\n",
      "Epoch 73/1000, Loss: 0.1529\n",
      "Epoch 74/1000, Loss: 0.1514\n",
      "Epoch 75/1000, Loss: 0.1498\n",
      "Epoch 76/1000, Loss: 0.1484\n",
      "Epoch 77/1000, Loss: 0.1469\n",
      "Epoch 78/1000, Loss: 0.1455\n",
      "Epoch 79/1000, Loss: 0.1441\n",
      "Epoch 80/1000, Loss: 0.1428\n",
      "Epoch 81/1000, Loss: 0.1415\n",
      "Epoch 82/1000, Loss: 0.1403\n",
      "Epoch 83/1000, Loss: 0.1390\n",
      "Epoch 84/1000, Loss: 0.1378\n",
      "Epoch 85/1000, Loss: 0.1367\n",
      "Epoch 86/1000, Loss: 0.1355\n",
      "Epoch 87/1000, Loss: 0.1344\n",
      "Epoch 88/1000, Loss: 0.1334\n",
      "Epoch 89/1000, Loss: 0.1323\n",
      "Epoch 90/1000, Loss: 0.1313\n",
      "Epoch 91/1000, Loss: 0.1303\n",
      "Epoch 92/1000, Loss: 0.1294\n",
      "Epoch 93/1000, Loss: 0.1284\n",
      "Epoch 94/1000, Loss: 0.1275\n",
      "Epoch 95/1000, Loss: 0.1266\n",
      "Epoch 96/1000, Loss: 0.1258\n",
      "Epoch 97/1000, Loss: 0.1249\n",
      "Epoch 98/1000, Loss: 0.1241\n",
      "Epoch 99/1000, Loss: 0.1234\n",
      "Epoch 100/1000, Loss: 0.1226\n",
      "Epoch 101/1000, Loss: 0.1219\n",
      "Epoch 102/1000, Loss: 0.1211\n",
      "Epoch 103/1000, Loss: 0.1204\n",
      "Epoch 104/1000, Loss: 0.1198\n",
      "Epoch 105/1000, Loss: 0.1191\n",
      "Epoch 106/1000, Loss: 0.1185\n",
      "Epoch 107/1000, Loss: 0.1179\n",
      "Epoch 108/1000, Loss: 0.1173\n",
      "Epoch 109/1000, Loss: 0.1167\n",
      "Epoch 110/1000, Loss: 0.1161\n",
      "Epoch 111/1000, Loss: 0.1156\n",
      "Epoch 112/1000, Loss: 0.1150\n",
      "Epoch 113/1000, Loss: 0.1145\n",
      "Epoch 114/1000, Loss: 0.1140\n",
      "Epoch 115/1000, Loss: 0.1136\n",
      "Epoch 116/1000, Loss: 0.1131\n",
      "Epoch 117/1000, Loss: 0.1126\n",
      "Epoch 118/1000, Loss: 0.1122\n",
      "Epoch 119/1000, Loss: 0.1118\n",
      "Epoch 120/1000, Loss: 0.1114\n",
      "Epoch 121/1000, Loss: 0.1110\n",
      "Epoch 122/1000, Loss: 0.1106\n",
      "Epoch 123/1000, Loss: 0.1102\n",
      "Epoch 124/1000, Loss: 0.1098\n",
      "Epoch 125/1000, Loss: 0.1095\n",
      "Epoch 126/1000, Loss: 0.1092\n",
      "Epoch 127/1000, Loss: 0.1088\n",
      "Epoch 128/1000, Loss: 0.1085\n",
      "Epoch 129/1000, Loss: 0.1082\n",
      "Epoch 130/1000, Loss: 0.1079\n",
      "Epoch 131/1000, Loss: 0.1076\n",
      "Epoch 132/1000, Loss: 0.1073\n",
      "Epoch 133/1000, Loss: 0.1071\n",
      "Epoch 134/1000, Loss: 0.1068\n",
      "Epoch 135/1000, Loss: 0.1066\n",
      "Epoch 136/1000, Loss: 0.1063\n",
      "Epoch 137/1000, Loss: 0.1061\n",
      "Epoch 138/1000, Loss: 0.1058\n",
      "Epoch 139/1000, Loss: 0.1056\n",
      "Epoch 140/1000, Loss: 0.1054\n",
      "Epoch 141/1000, Loss: 0.1052\n",
      "Epoch 142/1000, Loss: 0.1050\n",
      "Epoch 143/1000, Loss: 0.1048\n",
      "Epoch 144/1000, Loss: 0.1046\n",
      "Convergence reached at epoch 144.\n",
      "Epoch 1/1000, Loss: 0.5094\n",
      "Epoch 2/1000, Loss: 0.4416\n",
      "Epoch 3/1000, Loss: 0.4179\n",
      "Epoch 4/1000, Loss: 0.4067\n",
      "Epoch 5/1000, Loss: 0.3989\n",
      "Epoch 6/1000, Loss: 0.3920\n",
      "Epoch 7/1000, Loss: 0.3855\n",
      "Epoch 8/1000, Loss: 0.3791\n",
      "Epoch 9/1000, Loss: 0.3729\n",
      "Epoch 10/1000, Loss: 0.3667\n",
      "Epoch 11/1000, Loss: 0.3607\n",
      "Epoch 12/1000, Loss: 0.3548\n",
      "Epoch 13/1000, Loss: 0.3491\n",
      "Epoch 14/1000, Loss: 0.3434\n",
      "Epoch 15/1000, Loss: 0.3378\n",
      "Epoch 16/1000, Loss: 0.3323\n",
      "Epoch 17/1000, Loss: 0.3270\n",
      "Epoch 18/1000, Loss: 0.3217\n",
      "Epoch 19/1000, Loss: 0.3165\n",
      "Epoch 20/1000, Loss: 0.3115\n",
      "Epoch 21/1000, Loss: 0.3065\n",
      "Epoch 22/1000, Loss: 0.3016\n",
      "Epoch 23/1000, Loss: 0.2968\n",
      "Epoch 24/1000, Loss: 0.2921\n",
      "Epoch 25/1000, Loss: 0.2875\n",
      "Epoch 26/1000, Loss: 0.2830\n",
      "Epoch 27/1000, Loss: 0.2785\n",
      "Epoch 28/1000, Loss: 0.2742\n",
      "Epoch 29/1000, Loss: 0.2699\n",
      "Epoch 30/1000, Loss: 0.2657\n",
      "Epoch 31/1000, Loss: 0.2616\n",
      "Epoch 32/1000, Loss: 0.2576\n",
      "Epoch 33/1000, Loss: 0.2536\n",
      "Epoch 34/1000, Loss: 0.2498\n",
      "Epoch 35/1000, Loss: 0.2460\n",
      "Epoch 36/1000, Loss: 0.2423\n",
      "Epoch 37/1000, Loss: 0.2386\n",
      "Epoch 38/1000, Loss: 0.2350\n",
      "Epoch 39/1000, Loss: 0.2315\n",
      "Epoch 40/1000, Loss: 0.2281\n",
      "Epoch 41/1000, Loss: 0.2247\n",
      "Epoch 42/1000, Loss: 0.2214\n",
      "Epoch 43/1000, Loss: 0.2182\n",
      "Epoch 44/1000, Loss: 0.2150\n",
      "Epoch 45/1000, Loss: 0.2119\n",
      "Epoch 46/1000, Loss: 0.2089\n",
      "Epoch 47/1000, Loss: 0.2059\n",
      "Epoch 48/1000, Loss: 0.2030\n",
      "Epoch 49/1000, Loss: 0.2001\n",
      "Epoch 50/1000, Loss: 0.1973\n",
      "Epoch 51/1000, Loss: 0.1946\n",
      "Epoch 52/1000, Loss: 0.1919\n",
      "Epoch 53/1000, Loss: 0.1893\n",
      "Epoch 54/1000, Loss: 0.1867\n",
      "Epoch 55/1000, Loss: 0.1842\n",
      "Epoch 56/1000, Loss: 0.1817\n",
      "Epoch 57/1000, Loss: 0.1793\n",
      "Epoch 58/1000, Loss: 0.1770\n",
      "Epoch 59/1000, Loss: 0.1747\n",
      "Epoch 60/1000, Loss: 0.1724\n",
      "Epoch 61/1000, Loss: 0.1702\n",
      "Epoch 62/1000, Loss: 0.1680\n",
      "Epoch 63/1000, Loss: 0.1659\n",
      "Epoch 64/1000, Loss: 0.1639\n",
      "Epoch 65/1000, Loss: 0.1618\n",
      "Epoch 66/1000, Loss: 0.1599\n",
      "Epoch 67/1000, Loss: 0.1579\n",
      "Epoch 68/1000, Loss: 0.1561\n",
      "Epoch 69/1000, Loss: 0.1542\n",
      "Epoch 70/1000, Loss: 0.1524\n",
      "Epoch 71/1000, Loss: 0.1507\n",
      "Epoch 72/1000, Loss: 0.1489\n",
      "Epoch 73/1000, Loss: 0.1473\n",
      "Epoch 74/1000, Loss: 0.1456\n",
      "Epoch 75/1000, Loss: 0.1440\n",
      "Epoch 76/1000, Loss: 0.1425\n",
      "Epoch 77/1000, Loss: 0.1410\n",
      "Epoch 78/1000, Loss: 0.1395\n",
      "Epoch 79/1000, Loss: 0.1380\n",
      "Epoch 80/1000, Loss: 0.1366\n",
      "Epoch 81/1000, Loss: 0.1352\n",
      "Epoch 82/1000, Loss: 0.1339\n",
      "Epoch 83/1000, Loss: 0.1326\n",
      "Epoch 84/1000, Loss: 0.1313\n",
      "Epoch 85/1000, Loss: 0.1300\n",
      "Epoch 86/1000, Loss: 0.1288\n",
      "Epoch 87/1000, Loss: 0.1277\n",
      "Epoch 88/1000, Loss: 0.1265\n",
      "Epoch 89/1000, Loss: 0.1254\n",
      "Epoch 90/1000, Loss: 0.1243\n",
      "Epoch 91/1000, Loss: 0.1232\n",
      "Epoch 92/1000, Loss: 0.1222\n",
      "Epoch 93/1000, Loss: 0.1212\n",
      "Epoch 94/1000, Loss: 0.1202\n",
      "Epoch 95/1000, Loss: 0.1192\n",
      "Epoch 96/1000, Loss: 0.1183\n",
      "Epoch 97/1000, Loss: 0.1174\n",
      "Epoch 98/1000, Loss: 0.1165\n",
      "Epoch 99/1000, Loss: 0.1156\n",
      "Epoch 100/1000, Loss: 0.1148\n",
      "Epoch 101/1000, Loss: 0.1140\n",
      "Epoch 102/1000, Loss: 0.1132\n",
      "Epoch 103/1000, Loss: 0.1124\n",
      "Epoch 104/1000, Loss: 0.1117\n",
      "Epoch 105/1000, Loss: 0.1110\n",
      "Epoch 106/1000, Loss: 0.1103\n",
      "Epoch 107/1000, Loss: 0.1096\n",
      "Epoch 108/1000, Loss: 0.1089\n",
      "Epoch 109/1000, Loss: 0.1083\n",
      "Epoch 110/1000, Loss: 0.1076\n",
      "Epoch 111/1000, Loss: 0.1070\n",
      "Epoch 112/1000, Loss: 0.1064\n",
      "Epoch 113/1000, Loss: 0.1059\n",
      "Epoch 114/1000, Loss: 0.1053\n",
      "Epoch 115/1000, Loss: 0.1048\n",
      "Epoch 116/1000, Loss: 0.1042\n",
      "Epoch 117/1000, Loss: 0.1037\n",
      "Epoch 118/1000, Loss: 0.1032\n",
      "Epoch 119/1000, Loss: 0.1027\n",
      "Epoch 120/1000, Loss: 0.1023\n",
      "Epoch 121/1000, Loss: 0.1018\n",
      "Epoch 122/1000, Loss: 0.1014\n",
      "Epoch 123/1000, Loss: 0.1010\n",
      "Epoch 124/1000, Loss: 0.1006\n",
      "Epoch 125/1000, Loss: 0.1002\n",
      "Epoch 126/1000, Loss: 0.0998\n",
      "Epoch 127/1000, Loss: 0.0994\n",
      "Epoch 128/1000, Loss: 0.0990\n",
      "Epoch 129/1000, Loss: 0.0987\n",
      "Epoch 130/1000, Loss: 0.0983\n",
      "Epoch 131/1000, Loss: 0.0980\n",
      "Epoch 132/1000, Loss: 0.0977\n",
      "Epoch 133/1000, Loss: 0.0974\n",
      "Epoch 134/1000, Loss: 0.0971\n",
      "Epoch 135/1000, Loss: 0.0968\n",
      "Epoch 136/1000, Loss: 0.0965\n",
      "Epoch 137/1000, Loss: 0.0962\n",
      "Epoch 138/1000, Loss: 0.0960\n",
      "Epoch 139/1000, Loss: 0.0957\n",
      "Epoch 140/1000, Loss: 0.0955\n",
      "Epoch 141/1000, Loss: 0.0952\n",
      "Epoch 142/1000, Loss: 0.0950\n",
      "Epoch 143/1000, Loss: 0.0948\n",
      "Epoch 144/1000, Loss: 0.0945\n",
      "Epoch 145/1000, Loss: 0.0943\n",
      "Epoch 146/1000, Loss: 0.0941\n",
      "Epoch 147/1000, Loss: 0.0939\n",
      "Epoch 148/1000, Loss: 0.0937\n",
      "Epoch 149/1000, Loss: 0.0936\n",
      "Epoch 150/1000, Loss: 0.0934\n",
      "Epoch 151/1000, Loss: 0.0932\n",
      "Convergence reached at epoch 151.\n",
      "Epoch 1/1000, Loss: 0.5118\n",
      "Epoch 2/1000, Loss: 0.4436\n",
      "Epoch 3/1000, Loss: 0.4194\n",
      "Epoch 4/1000, Loss: 0.4080\n",
      "Epoch 5/1000, Loss: 0.4002\n",
      "Epoch 6/1000, Loss: 0.3933\n",
      "Epoch 7/1000, Loss: 0.3869\n",
      "Epoch 8/1000, Loss: 0.3806\n",
      "Epoch 9/1000, Loss: 0.3744\n",
      "Epoch 10/1000, Loss: 0.3684\n",
      "Epoch 11/1000, Loss: 0.3625\n",
      "Epoch 12/1000, Loss: 0.3567\n",
      "Epoch 13/1000, Loss: 0.3510\n",
      "Epoch 14/1000, Loss: 0.3454\n",
      "Epoch 15/1000, Loss: 0.3399\n",
      "Epoch 16/1000, Loss: 0.3345\n",
      "Epoch 17/1000, Loss: 0.3293\n",
      "Epoch 18/1000, Loss: 0.3241\n",
      "Epoch 19/1000, Loss: 0.3190\n",
      "Epoch 20/1000, Loss: 0.3140\n",
      "Epoch 21/1000, Loss: 0.3091\n",
      "Epoch 22/1000, Loss: 0.3044\n",
      "Epoch 23/1000, Loss: 0.2997\n",
      "Epoch 24/1000, Loss: 0.2950\n",
      "Epoch 25/1000, Loss: 0.2905\n",
      "Epoch 26/1000, Loss: 0.2861\n",
      "Epoch 27/1000, Loss: 0.2818\n",
      "Epoch 28/1000, Loss: 0.2775\n",
      "Epoch 29/1000, Loss: 0.2733\n",
      "Epoch 30/1000, Loss: 0.2692\n",
      "Epoch 31/1000, Loss: 0.2652\n",
      "Epoch 32/1000, Loss: 0.2613\n",
      "Epoch 33/1000, Loss: 0.2575\n",
      "Epoch 34/1000, Loss: 0.2537\n",
      "Epoch 35/1000, Loss: 0.2500\n",
      "Epoch 36/1000, Loss: 0.2464\n",
      "Epoch 37/1000, Loss: 0.2428\n",
      "Epoch 38/1000, Loss: 0.2394\n",
      "Epoch 39/1000, Loss: 0.2360\n",
      "Epoch 40/1000, Loss: 0.2326\n",
      "Epoch 41/1000, Loss: 0.2294\n",
      "Epoch 42/1000, Loss: 0.2262\n",
      "Epoch 43/1000, Loss: 0.2230\n",
      "Epoch 44/1000, Loss: 0.2200\n",
      "Epoch 45/1000, Loss: 0.2170\n",
      "Epoch 46/1000, Loss: 0.2140\n",
      "Epoch 47/1000, Loss: 0.2112\n",
      "Epoch 48/1000, Loss: 0.2084\n",
      "Epoch 49/1000, Loss: 0.2056\n",
      "Epoch 50/1000, Loss: 0.2029\n",
      "Epoch 51/1000, Loss: 0.2003\n",
      "Epoch 52/1000, Loss: 0.1977\n",
      "Epoch 53/1000, Loss: 0.1952\n",
      "Epoch 54/1000, Loss: 0.1927\n",
      "Epoch 55/1000, Loss: 0.1903\n",
      "Epoch 56/1000, Loss: 0.1880\n",
      "Epoch 57/1000, Loss: 0.1857\n",
      "Epoch 58/1000, Loss: 0.1834\n",
      "Epoch 59/1000, Loss: 0.1812\n",
      "Epoch 60/1000, Loss: 0.1791\n",
      "Epoch 61/1000, Loss: 0.1770\n",
      "Epoch 62/1000, Loss: 0.1750\n",
      "Epoch 63/1000, Loss: 0.1730\n",
      "Epoch 64/1000, Loss: 0.1710\n",
      "Epoch 65/1000, Loss: 0.1691\n",
      "Epoch 66/1000, Loss: 0.1672\n",
      "Epoch 67/1000, Loss: 0.1654\n",
      "Epoch 68/1000, Loss: 0.1636\n",
      "Epoch 69/1000, Loss: 0.1619\n",
      "Epoch 70/1000, Loss: 0.1602\n",
      "Epoch 71/1000, Loss: 0.1586\n",
      "Epoch 72/1000, Loss: 0.1570\n",
      "Epoch 73/1000, Loss: 0.1554\n",
      "Epoch 74/1000, Loss: 0.1539\n",
      "Epoch 75/1000, Loss: 0.1524\n",
      "Epoch 76/1000, Loss: 0.1509\n",
      "Epoch 77/1000, Loss: 0.1495\n",
      "Epoch 78/1000, Loss: 0.1481\n",
      "Epoch 79/1000, Loss: 0.1468\n",
      "Epoch 80/1000, Loss: 0.1455\n",
      "Epoch 81/1000, Loss: 0.1442\n",
      "Epoch 82/1000, Loss: 0.1430\n",
      "Epoch 83/1000, Loss: 0.1417\n",
      "Epoch 84/1000, Loss: 0.1406\n",
      "Epoch 85/1000, Loss: 0.1394\n",
      "Epoch 86/1000, Loss: 0.1383\n",
      "Epoch 87/1000, Loss: 0.1372\n",
      "Epoch 88/1000, Loss: 0.1362\n",
      "Epoch 89/1000, Loss: 0.1351\n",
      "Epoch 90/1000, Loss: 0.1341\n",
      "Epoch 91/1000, Loss: 0.1332\n",
      "Epoch 92/1000, Loss: 0.1322\n",
      "Epoch 93/1000, Loss: 0.1313\n",
      "Epoch 94/1000, Loss: 0.1304\n",
      "Epoch 95/1000, Loss: 0.1295\n",
      "Epoch 96/1000, Loss: 0.1287\n",
      "Epoch 97/1000, Loss: 0.1279\n",
      "Epoch 98/1000, Loss: 0.1271\n",
      "Epoch 99/1000, Loss: 0.1263\n",
      "Epoch 100/1000, Loss: 0.1255\n",
      "Epoch 101/1000, Loss: 0.1248\n",
      "Epoch 102/1000, Loss: 0.1241\n",
      "Epoch 103/1000, Loss: 0.1234\n",
      "Epoch 104/1000, Loss: 0.1227\n",
      "Epoch 105/1000, Loss: 0.1221\n",
      "Epoch 106/1000, Loss: 0.1214\n",
      "Epoch 107/1000, Loss: 0.1208\n",
      "Epoch 108/1000, Loss: 0.1202\n",
      "Epoch 109/1000, Loss: 0.1197\n",
      "Epoch 110/1000, Loss: 0.1191\n",
      "Epoch 111/1000, Loss: 0.1186\n",
      "Epoch 112/1000, Loss: 0.1180\n",
      "Epoch 113/1000, Loss: 0.1175\n",
      "Epoch 114/1000, Loss: 0.1170\n",
      "Epoch 115/1000, Loss: 0.1165\n",
      "Epoch 116/1000, Loss: 0.1161\n",
      "Epoch 117/1000, Loss: 0.1156\n",
      "Epoch 118/1000, Loss: 0.1152\n",
      "Epoch 119/1000, Loss: 0.1148\n",
      "Epoch 120/1000, Loss: 0.1144\n",
      "Epoch 121/1000, Loss: 0.1140\n",
      "Epoch 122/1000, Loss: 0.1136\n",
      "Epoch 123/1000, Loss: 0.1132\n",
      "Epoch 124/1000, Loss: 0.1128\n",
      "Epoch 125/1000, Loss: 0.1125\n",
      "Epoch 126/1000, Loss: 0.1121\n",
      "Epoch 127/1000, Loss: 0.1118\n",
      "Epoch 128/1000, Loss: 0.1115\n",
      "Epoch 129/1000, Loss: 0.1112\n",
      "Epoch 130/1000, Loss: 0.1109\n",
      "Epoch 131/1000, Loss: 0.1106\n",
      "Epoch 132/1000, Loss: 0.1103\n",
      "Epoch 133/1000, Loss: 0.1101\n",
      "Epoch 134/1000, Loss: 0.1098\n",
      "Epoch 135/1000, Loss: 0.1095\n",
      "Epoch 136/1000, Loss: 0.1093\n",
      "Epoch 137/1000, Loss: 0.1091\n",
      "Epoch 138/1000, Loss: 0.1088\n",
      "Epoch 139/1000, Loss: 0.1086\n",
      "Epoch 140/1000, Loss: 0.1084\n",
      "Epoch 141/1000, Loss: 0.1082\n",
      "Epoch 142/1000, Loss: 0.1080\n",
      "Epoch 143/1000, Loss: 0.1078\n",
      "Epoch 144/1000, Loss: 0.1076\n",
      "Epoch 145/1000, Loss: 0.1074\n",
      "Epoch 146/1000, Loss: 0.1072\n",
      "Epoch 147/1000, Loss: 0.1071\n",
      "Epoch 148/1000, Loss: 0.1069\n",
      "Epoch 149/1000, Loss: 0.1067\n",
      "Convergence reached at epoch 149.\n",
      "Cross-validation RMSE (mean): 0.1047\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class MyLinReg():\n",
    "    def __init__(self, eta0=0.05, max_iter=1000, tol=1e-3, n_iter_no_change=50, verbose=True):\n",
    "        self.eta0 = eta0\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tol\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.verbose = verbose\n",
    "        self.coef_ = None # vores w # ellers prv med underscore (coef_)\n",
    "        self.intercept_ = None # ellers prv med underscore (intercept_)\n",
    "        self.batch_size = 32\n",
    "    \n",
    "    def Weights(self):\n",
    "        return self.coef_\n",
    "    \n",
    "    def Intercept(self):\n",
    "        return self.coef_[0]\n",
    "  \n",
    "    def __str__(self):\n",
    "        return \"MyLinReg.__str__(): hi!\"\n",
    "    \n",
    "    def fit(self, X, y, method=\"SGD\"):\n",
    "        n_samples, n_features = X.shape\n",
    "        X = add_dummy_feature(X) # Augmenter med 1 taller\n",
    "        assert X.shape[0] == y.shape[0], \"X og y skal vre lige store\"\n",
    "        self.coef_ =  np.zeros(X.shape[1])\n",
    "        best_loss = np.inf # brug numpy infity her\n",
    "        no_change_counter = 0\n",
    "        loss = None\n",
    "\n",
    "\n",
    "        for ep in range(self.max_iter):\n",
    "            if method == \"GD\":\n",
    "                # GD\n",
    "                y_pred = X @ self.coef_\n",
    "                error = y_pred - y\n",
    "                gradient = 2*(X.T @ error) / len(y) # hvor y_pred er Xw\n",
    "                self.coef_ -= gradient * self.eta0\n",
    "                # GD\n",
    "\n",
    "\n",
    "            elif method == \"SGD\":\n",
    "                # SGD\n",
    "                for i in range(n_samples):\n",
    "                    y_pred = X[i] @ self.coef_\n",
    "                    error = y_pred - y[i]\n",
    "                    gradient = 2 * X[i] * error\n",
    "                    self.coef_ -= gradient * self.eta0\n",
    "                #SGD\n",
    "            loss = np.sqrt(np.mean(((X @ self.coef_) - y) ** 2))\n",
    "            if abs(best_loss - loss) < self.tolerance:\n",
    "                no_change_counter += 1\n",
    "                if no_change_counter >= self.n_iter_no_change:\n",
    "                    break\n",
    "            else:\n",
    "                no_change_counter = 0\n",
    "                best_loss = loss\n",
    "\n",
    "\n",
    "        self.intercept_ = self.coef_[0]\n",
    "        self.coef_ = self.coef_[1:]\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = add_dummy_feature(X)\n",
    "        return X @ np.r_[self.intercept_, self.coef_]\n",
    "\n",
    "    def score(self, X, y_true):\n",
    "        y_pred = self.predict(X)\n",
    "        sum_squares_total = np.sum((y_true - np.mean(y_true))**2)\n",
    "        sum_squares_residual = np.sum((y_true - y_pred)**2)\n",
    "        return 1 - (sum_squares_residual / sum_squares_total) # Scoren\n",
    "\n",
    "regressor = MyLinReg(eta0=0.01, max_iter=1000, tol=1e-6, n_iter_no_change=10, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TODO list\n",
    "\n",
    "You must investigate and describe all major details for a linear regressor, and implement at least the following concepts (MUST):\n",
    "\n",
    "### Qa: Concepts and Implementations MUSTS\n",
    "\n",
    "* Implement: the `fit-predict` interface, for a one-dimensional output only, \n",
    "* Implement: a $R^2$ score function (re-use existing code or perhaps just inherit it), \n",
    "* Implement: loss function based on (R)MSE,\n",
    "* Implement: setting of the number of iterations and learning rate ($\\eta$) via parameters in the constructor (the signature of your `__init__` must include the named parameters `eta0` and `max_iter`),\n",
    "* (in a later exercise we will also add `tol`, `n_iter_no_change` and `verbose` to the constructor),\n",
    "* Implement: the batch-gradient decent algorithm (GD),\n",
    "* Implement: constant learning rate (maybe also adaptive learning rate if you are brave),\n",
    "* Implement: stochastic gradient descent (SGD),\n",
    "* Describe in text: epochs vs iterations,\n",
    "* Describe in text: compare the numerical optimization with the Closed-form solution.\n",
    "\n",
    "I det her kodeudsnit har vi implementeret et fit-predict interface for en linr regressor. Vi har ogs implementeret en R^2 score funktion, en loss funktion baseret p RMSE og vi har implementeret en konstant learning rate med eta parametrene. Vi har ogs implementeret batch-gradient decent algoritmen og stochastic gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "### Qb: [OPTIONAL] Additional Concepts and Implementations\n",
    "\n",
    "And perhaps you could include (SHOULD/COULD):\n",
    "\n",
    "* (stochastic) mini-bach gradient decent, \n",
    "* interface to your bias and weights via `intercept_` and `coef_` attributes on your linear regressor `class`,\n",
    "* get/set functionality of your regressor, such that it is fully compatible with other Scikit-learn algorithms, try it out in say a `cross_val_score()` call from Scikit-learn,\n",
    "* test in via the smoke tests at the end of this Notebook,\n",
    "* testing it on MNIST data.\n",
    "\n",
    "With the following no-no's (WONT):\n",
    "\n",
    "* no learning graphs, no early stopping (we will do this in a later exercise),\n",
    "* no multi-linear regression,\n",
    "* no reuse of the Scikit-learn regressor,\n",
    "* no `C/C++` optimized implementation with a _thin_ Python interface (nifty, but out-of-scope for this cause),\n",
    "* no copy-paste of code from other sources WITHOUT a clear cite/reference for your source.\n",
    "\n",
    "### Qc: Testing and Test Data\n",
    "\n",
    "Use mainly very low-dimensional data for testing, say the IRIS set, since it might be very slow. Or create a simple low-dimensionality data generator.\n",
    "\n",
    "(There is a _micro_ data set in the function `GenerateData` in the smoke tests functions below, but better is to opt for an realistic data set.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1]\n",
      " [4.9]\n",
      " [4.7]\n",
      " [4.6]\n",
      " [5. ]\n",
      " [5.4]\n",
      " [4.6]\n",
      " [5. ]\n",
      " [4.4]\n",
      " [4.9]\n",
      " [5.4]\n",
      " [4.8]\n",
      " [4.8]\n",
      " [4.3]\n",
      " [5.8]\n",
      " [5.7]\n",
      " [5.4]\n",
      " [5.1]\n",
      " [5.7]\n",
      " [5.1]\n",
      " [5.4]\n",
      " [5.1]\n",
      " [4.6]\n",
      " [5.1]\n",
      " [4.8]\n",
      " [5. ]\n",
      " [5. ]\n",
      " [5.2]\n",
      " [5.2]\n",
      " [4.7]\n",
      " [4.8]\n",
      " [5.4]\n",
      " [5.2]\n",
      " [5.5]\n",
      " [4.9]\n",
      " [5. ]\n",
      " [5.5]\n",
      " [4.9]\n",
      " [4.4]\n",
      " [5.1]\n",
      " [5. ]\n",
      " [4.5]\n",
      " [4.4]\n",
      " [5. ]\n",
      " [5.1]\n",
      " [4.8]\n",
      " [5.1]\n",
      " [4.6]\n",
      " [5.3]\n",
      " [5. ]\n",
      " [7. ]\n",
      " [6.4]\n",
      " [6.9]\n",
      " [5.5]\n",
      " [6.5]\n",
      " [5.7]\n",
      " [6.3]\n",
      " [4.9]\n",
      " [6.6]\n",
      " [5.2]\n",
      " [5. ]\n",
      " [5.9]\n",
      " [6. ]\n",
      " [6.1]\n",
      " [5.6]\n",
      " [6.7]\n",
      " [5.6]\n",
      " [5.8]\n",
      " [6.2]\n",
      " [5.6]\n",
      " [5.9]\n",
      " [6.1]\n",
      " [6.3]\n",
      " [6.1]\n",
      " [6.4]\n",
      " [6.6]\n",
      " [6.8]\n",
      " [6.7]\n",
      " [6. ]\n",
      " [5.7]\n",
      " [5.5]\n",
      " [5.5]\n",
      " [5.8]\n",
      " [6. ]\n",
      " [5.4]\n",
      " [6. ]\n",
      " [6.7]\n",
      " [6.3]\n",
      " [5.6]\n",
      " [5.5]\n",
      " [5.5]\n",
      " [6.1]\n",
      " [5.8]\n",
      " [5. ]\n",
      " [5.6]\n",
      " [5.7]\n",
      " [5.7]\n",
      " [6.2]\n",
      " [5.1]\n",
      " [5.7]\n",
      " [6.3]\n",
      " [5.8]\n",
      " [7.1]\n",
      " [6.3]\n",
      " [6.5]\n",
      " [7.6]\n",
      " [4.9]\n",
      " [7.3]\n",
      " [6.7]\n",
      " [7.2]\n",
      " [6.5]\n",
      " [6.4]\n",
      " [6.8]\n",
      " [5.7]\n",
      " [5.8]\n",
      " [6.4]\n",
      " [6.5]\n",
      " [7.7]\n",
      " [7.7]\n",
      " [6. ]\n",
      " [6.9]\n",
      " [5.6]\n",
      " [7.7]\n",
      " [6.3]\n",
      " [6.7]\n",
      " [7.2]\n",
      " [6.2]\n",
      " [6.1]\n",
      " [6.4]\n",
      " [7.2]\n",
      " [7.4]\n",
      " [7.9]\n",
      " [6.4]\n",
      " [6.3]\n",
      " [6.1]\n",
      " [7.7]\n",
      " [6.3]\n",
      " [6.4]\n",
      " [6. ]\n",
      " [6.9]\n",
      " [6.7]\n",
      " [6.9]\n",
      " [5.8]\n",
      " [6.8]\n",
      " [6.7]\n",
      " [6.7]\n",
      " [6.3]\n",
      " [6.5]\n",
      " [6.2]\n",
      " [5.9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "linRegModel = MyLinReg(0.0001)\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 0].reshape(-1, 1) # transponerer X?\n",
    "print(X)\n",
    "y = iris.data[:, 1]\n",
    "\n",
    "linRegModel.fit(X, y, \"GD\")\n",
    "\n",
    "predictions = linRegModel.predict(X)\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alt under Qc her:\n",
    "### Qd: The Journaling of Your Regressor \n",
    "\n",
    "For the journal, write a full explanation of how you implemented the linear regressor, including a code walk-through (or mini-review of the most interesting parts).\n",
    "\n",
    "Vi har implementeret en linear regressor class MyLinReg, og med mulighed for at bruge en Gradient Descent eller Stochastic Gradient Descent solver, som afgr hvordan loss funktionen bliver minimeret. I vores fit-predict interface starter vi med at stte variablerne fra constructeren p klassen selv, inden vi kalder vores solver metode (som enten er GD eller SGD).\n",
    "\n",
    "Solver metoden tager nogle iterationer, og mellem hver iteration opdaterer vi vores vgte (vores w, self.coef) ved at bruge vores loss funktion og gradienten af vores loss funktion. Vi bruger vores loss funktion til at evaluere vores model, og tjekker loss hver gang for at se om den bliver bedre hver iteration (hvor frste gang er loss super hj for at barren for improvement er meget lav). Hvis loss ikke bliver bedre, s stopper vi vores iterationer efter efter den har krt \"max_iter\" iterationer.\n",
    "\n",
    "Det her er alt sammen til for at prve at minimere loss. Vi har ogs implementeret en R^2 score funktion, som vi bruger til at evaluere vores model. Vi har ogs gjort s man kan stte en constant learning rate i form af parameteren eta0, der (sammen med gradient) styrer hvor drastisk vi skal ndre vores vgte.\n",
    "\n",
    "\n",
    "### Qe: Mathematical Foundation for Training a Linear Regressor\n",
    "\n",
    "You must also include the theoretical mathematical foundation for the linear regressor using the following equations and graphs (free to include in your journal without cite/reference), and relate them directly to your code:\n",
    "\n",
    "* Design matrix of size $(n, d)$ where each row is an input column vector $(\\mathbf{x}^{(i)})^\\top$ data sample of size $d$\n",
    "\n",
    "$$\n",
    "    \\def\\rem#1{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands v01, remember: no newlines in defs}\n",
    "    \\rem{MACRO eq: equation <#1:lhs> <#2:rhs>}\n",
    "    \\def\\eq#1#2{#1 &=& #2\\\\}\n",
    "    \\rem{MACRO arr: array <#1:columns (lcr..)> <#2:content>}\n",
    "    \\def\\ar#1#2{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\rem{MACRO ac: array column vector <#1:columns (lcr..)> <#2:content>}\n",
    "    \\def\\ac#1#2{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\rem{MACRO st: subscript text <#1:content>}\n",
    "    \\def\\st#1{_{\\textrm{#1}}}\n",
    "    \\rem{MACRO norm: norm caligari L <#1:content>}\n",
    "    \\def\\norm#1{{\\cal L}_{#1}}\n",
    "    \\rem{MACRO obs: ??}\n",
    "    \\def\\obs#1#2{#1_{\\textrm{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\rem{MACRO diff: math differetial operator <#1:content>}\n",
    "    \\def\\diff#1{\\mathrm{d}#1} \n",
    "    \\rem{MACRO half: shorthand for 1/2}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\rem{MACRO pfrac: partial fraction <#1:numenator> <#2:denumenator>}\n",
    "    \\def\\pfrac#1#2{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\rem{MACRO dfrac: differetial operator fraction <#1:numenator> <#2:denumenator>}\n",
    "    \\def\\dfrac#1#2{\\frac{\\mathrm{d}~#1}{\\mathrm{d}#2}}\n",
    "    \\rem{MACRO pown: power and parantesis (train/test..) <#1:content>}\n",
    "    \\def\\pown#1{^{(#1)}}\n",
    "    \\rem{MACROS powi, pown: shorthands for power (i) and (n)}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\rem{MACROS powtest, powertrain: power (test) and (train)}\n",
    "    \\def\\powtest{\\pown{\\textrm{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\textrm{\\scriptsize train}}}\n",
    "    \\rem{MACRO boldmatrix: bold matix/vector notation} \n",
    "    \\def\\boldmatrix#1{\\mathbf{#1}} \n",
    "    \\rem{MACROS X,Z,x,y,w: bold X,Z,x etc.} \n",
    "    \\def\\bX{\\boldmatrix{X}}\n",
    "    \\def\\bZ{\\boldmatrix{Z}}\n",
    "    \\def\\bx{\\boldmatrix{x}}\n",
    "    \\def\\by{\\boldmatrix{y}}\n",
    "    \\def\\bw{\\boldmatrix{w}}\n",
    "    \\def\\bz{\\boldmatrix{z}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\rem{MACROS stpred, sttrue: shorthand for subscript 'pred' and 'true'}\n",
    "    \\def\\stpred{\\st{pred}~}\n",
    "    \\def\\sttrue{\\st{true}~}\n",
    "    \\rem{MACROS ypred, ytrue:   shorthand for scalar y 'pred' and 'true'}\n",
    "    \\def\\ytrue{y\\sttrue}\n",
    "    \\def\\ypred{y\\stpred} \n",
    "    \\rem{MACROS bypred, bytrue: shorthand for vecor y 'pred' and 'true'} \n",
    "    \\def\\bypred{\\boldmatrix{y}\\stpred}\n",
    "    \\def\\bytrue{\\boldmatrix{y}\\sttrue} \n",
    "\\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2} \\\\\n",
    "            \\vdots      &             &        & \\vdots      \\\\\n",
    "            x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn   \\\\\n",
    "        } \n",
    "$$\n",
    "\n",
    "* Target ground-truth column vector of size $n$\n",
    "\n",
    "$$\n",
    "\\bytrue =\n",
    "  \\ac{c}{\n",
    "     y\\pown{1}\\sttrue \\\\\n",
    "     y\\pown{2}\\sttrue \\\\\n",
    "     \\vdots           \\\\\n",
    "     y\\pown{n}\\sttrue \\\\\n",
    "  } \n",
    "$$\n",
    "\n",
    "* Bias factor, and by convention in the following (prepend one)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\ac{c}{1\\\\\\bx\\powni} & \\mapsto \\bx\\powni\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Weight column vector of size $d+1$ (i.e. with bias or intercept element $w_0$ prepended)\n",
    "\n",
    "$$\n",
    "\\bw =\n",
    "    \\ac{c}{\n",
    "         w_0    \\\\\n",
    "         w_1    \\\\\n",
    "         w_2    \\\\\n",
    "         \\vdots \\\\\n",
    "         w_d    \\\\\n",
    "    }\n",
    "$$\n",
    "\n",
    "* Linear regression model hypothesis function for a column vector input $\\bx\\powni$ of size $d$ and a column weight vector $\\bw$ of size $d+1$\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  ~~~~~~~~~~~~~~~\n",
    "  h(\\bx\\powni;\\bw) &= \\ypred\\powni \\\\\n",
    "                   &= \\bw^\\top \\bx\\powni ~~~~ (\\bx\\powni~\\textrm{with bias element})\\\\ \n",
    "                   &= w_0  \\cdot 1+ w_1 x_1\\powni + w_2 x_2\\powni + \\cdots + w_d x_d\\powni & \\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Individual losses based on the $\\norm{2}^2$ (last part assuming one dimensional output)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  L\\powni &= || \\ypred \\powni         - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= || h(\\bx\\powni;\\bw)      - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= || \\bw^\\top\\bx\\powni     - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= \\left( \\bw^\\top\\bx\\powni - \\ytrue\\powni~ \\right)^2 ~~~~~ \\textrm{(only for 1D output)}\n",
    "}\n",
    "$$\n",
    "\n",
    "* MSE loss function\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "    \\textrm{MSE}(\\bX,\\bytrue;\\bw)  &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                                   &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\powni\\sttrue \\right)^2\\\\\n",
    "                                   &= \\frac{1}{n} ||\\bX \\bw - \\bytrue||_2^2\n",
    "}\n",
    "$$                   \n",
    "\n",
    "\n",
    "* Loss function, proportional to (R)MSE\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "   J &= \\frac{1}{2} ||\\bX \\bw - \\bytrue||_2^2\\\\\n",
    "     &  \\propto \\textrm{MSE}\n",
    "}\n",
    "$$\n",
    "\n",
    "* Training: computing the optimal value of the $\\bw$ weight; that is finding the $\\bw$-value that minimizes the total loss\n",
    "\n",
    "$$\n",
    "  \\bw^* = \\textrm{argmin}_\\bw~J\\\\\n",
    "$$\n",
    "\n",
    "* Visualization of $\\textrm{argmin}_\\bw$ means to the argument of $\\bw$ that minimizes the $J$ function. The minimization can in 2-D visually be drawn as finding the lowest $J$ that for linear regression always forms a convex shape \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">\n",
    "\n",
    "#### Training I: The Closed-form Solution\n",
    "\n",
    "* Finding the optimal weight in a _one-step_ analytic expression \n",
    "\n",
    "$$\n",
    "  \\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1}~ \\bX^\\top \\bytrue\n",
    "$$\n",
    "\n",
    "\n",
    "#### Training II: Numerical Optimization \n",
    "\n",
    "* The Gradient of the loss function\n",
    "\n",
    "$$   \n",
    "  \\nabla_\\bw~J = \\left[ \\frac{\\partial J}{\\partial w_1} ~~~~ \\frac{\\partial J}{\\partial w_2} ~~~~ \\ldots  ~~~~ \\frac{\\partial J}{\\partial w_d} \\right]^\\top\n",
    "$$\n",
    "\n",
    "* The Gradient for the based $J$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J &= \\frac{2}{n} \\bX^\\top \\left( \\bX \\bw - \\bytrue \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "* The Gradient Decent Algorithm (GD)\n",
    "\n",
    "$$ \n",
    "  \\bw^{(step~N+1)}~ = \\bw^{(step~N)} ~ - \\eta \\nabla_{\\bw} J\n",
    "$$\n",
    "\n",
    "* Visualization of GD, showing $J$ as a function of two $w$-dimensions\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization_gd.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qf: Smoke testing\n",
    "\n",
    "Once ready, you can test your regressor via the test stub below, or create your own _test suite_.\n",
    "\n",
    "Be aware that setting the stepsize, $\\eta$, value can be tricky, and you might want to tune `eta0` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  DATA: 'IRIS'\n",
      "       SHAPES: X_train=(112, 4), X_test=(38, 4), y_train=(112,), y_\\\n",
      "       test=(38,)\n",
      "\n",
      "INFO:  TRAINING['MyLinReg']..\n",
      "\n",
      "y_pred_test= [-0.28556843  1.13373579  0.01817117  1.68589514 -0.17580118 -0.04884282\n",
      "  1.7994768   1.85260933  0.03300777 -0.11397953  1.70595963  1.68767051\n",
      "  1.20649114  1.33121576  1.38542419  1.72978412  1.68294196  0.04368367\n",
      " -0.12748242 -0.08876366  1.85133872  1.76352193 -0.03122348  1.7011734\n",
      "  1.5189086   1.9024899   1.40553292  0.97261464  0.09529863  1.22615821\n",
      "  1.60305387  1.80337968  1.4429234   0.01338495  1.30958946  1.19497288\n",
      "  0.0071798   1.95026507]\n",
      "\n",
      "INFO:  SCORE['MyLinReg'] = 0.912\n",
      "\n",
      "INFO:  TRAINING['SGDRegressor']..\n",
      "\n",
      "y_pred_test= [-1.37082945e-01  1.26618587e+00  4.04377947e-02  1.51182748e+00\n",
      " -1.70247910e-01 -5.35396155e-02  1.81368086e+00  1.91227563e+00\n",
      " -2.13865640e-02 -2.12036317e-01  1.70126108e+00  1.75626708e+00\n",
      "  1.39348947e+00  1.29231155e+00  1.32751074e+00  1.80745058e+00\n",
      "  1.63974236e+00  5.95025105e-04 -6.64971314e-02 -4.81235284e-02\n",
      "  1.84836375e+00  1.60798353e+00  8.26026675e-05  1.61072789e+00\n",
      "  1.66369968e+00  1.83038824e+00  1.23397698e+00  1.00094169e+00\n",
      "  1.86061214e-03  1.13923094e+00  1.42138929e+00  1.58276450e+00\n",
      "  1.21443024e+00 -5.00953877e-02  1.28851612e+00  1.21134496e+00\n",
      " -1.02662670e-01  2.08237314e+00]\n",
      "\n",
      "INFO:  SCORE['SGDRegressor'] = 0.918\n",
      "\n",
      "INFO:  ##############################################\n",
      "       \n",
      "\n",
      "INFO:  DATA: 'MNIST'\n",
      "       SHAPES: X_train=(52500, 784), X_test=(17500, 784), y_train=(\\\n",
      "       52500,), y_test=(17500,)\n",
      "\n",
      "INFO:  TRAINING['MyLinReg']..\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morte\\AppData\\Local\\Temp\\ipykernel_2556\\2242980192.py:133: RuntimeWarning: overflow encountered in square\n",
      "  loss = np.sqrt(np.mean(((X @ self.coef_) - y) ** 2))\n",
      "C:\\Users\\morte\\AppData\\Local\\Temp\\ipykernel_2556\\2242980192.py:134: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  if abs(best_loss - loss) < self.tolerance:\n",
      "C:\\Users\\morte\\AppData\\Local\\Temp\\ipykernel_2556\\2242980192.py:130: RuntimeWarning: overflow encountered in multiply\n",
      "  gradient = 2 * X[i] * error\n",
      "C:\\Users\\morte\\AppData\\Local\\Temp\\ipykernel_2556\\2242980192.py:130: RuntimeWarning: invalid value encountered in multiply\n",
      "  gradient = 2 * X[i] * error\n",
      "C:\\Users\\morte\\AppData\\Local\\Temp\\ipykernel_2556\\2242980192.py:131: RuntimeWarning: invalid value encountered in subtract\n",
      "  self.coef_ -= gradient * self.eta0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_test= [nan nan nan ... nan nan nan]\n",
      "\n",
      "INFO:  SCORE['MyLinReg'] = nan\n",
      "\n",
      "INFO:  TRAINING['SGDRegressor']..\n",
      "\n",
      "y_pred_test= [-5.58728329e+09  1.87196533e+10 -3.80548545e+10 ... -9.99093024e+09\n",
      " -3.65431514e+09  1.07277652e+12]\n",
      "\n",
      "INFO:  SCORE['SGDRegressor'] = -622067138313273925959680.000\n",
      "\n",
      "INFO:  ##############################################\n",
      "       \n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "### OLD SMOKETEST\n",
    "\n",
    "from sklearn.linear_model    import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.pipeline        import Pipeline\n",
    "\n",
    "try:\n",
    "    from libitmal import dataloaders\n",
    "except Exception as ex:\n",
    "    Err(\"can not import dataloaders form libitmal, and then I can not run the TestAndCompareRegressors smoke-test, sorry!\", ex)\n",
    "\n",
    "def TestAndCompareRegressors():\n",
    "    for f in [(\"IRIS\",  dataloaders.IRIS_GetDataSet,  1E-2),\n",
    "              (\"MNIST\", dataloaders.MNIST_GetDataSet, 1E-3)]:\n",
    "        \n",
    "        # NOTE: f-tuble is (<name>, <data-loader-function-pointer>, <eps0>)\n",
    "        data = f[1]() # returns (X, y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data[0], data[1])\n",
    "        \n",
    "        Info(f\"DATA: '{f[0]}'\\n\\tSHAPES: X_train={X_train.shape}, X_test={X_test.shape}, y_train={y_train.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "        eta0 = f[2] # an adaptive learning rate is really needed here!\n",
    "        regressor0 = MyLinReg(eta0=eta0, max_iter=1000)\n",
    "        regressor1 = SGDRegressor()    \n",
    "\n",
    "        for r in [(\"MyLinReg\", regressor0), (\"SGDRegressor\", regressor1)]:\n",
    "            Info(f\"\\nTRAINING['{r[0]}']..\")\n",
    "            \n",
    "            pipe = Pipeline([('scaler', StandardScaler()), r])\n",
    "            # pipe.named_steps['mylinreg'].fit(X_train, y_train, \"SGD\")\n",
    "            # pipe.named_steps['mylinreg'].fit(X_train, y_train, method=\"SGD\")\n",
    "            pipe.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_test = pipe.predict(X_test)\n",
    "            \n",
    "            PrintMatrix(y_pred_test, label=\"y_pred_test=\", precision=4)\n",
    "            print()\n",
    "            \n",
    "            r2 = pipe.score(X_test, y_test)\n",
    "            Info(f\"SCORE['{r[0]}'] = {Col('lblue')}{r2:0.3f}{ColEnd()}\")\n",
    "            \n",
    "        Info(\"\\n##############################################\\n\")\n",
    "\n",
    "# somewhat more verbose testing, you regressor will likely fail on MNIST \n",
    "# or at least be very, very slow...\n",
    "TestAndCompareRegressors()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qg: [OPTIONAL] More Smoke-Testing\n",
    "\n",
    "Do you dare to compare your custom regressor with the SGD regressor in Scikit-learn on both the IRIS and MNIST datasets?\n",
    "\n",
    "Then run the next smoke-test function, but the code might requre `eta0` anb `max_iter` hyperparamter tuning).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  DATA: 'IRIS'\n",
      "       SHAPES: X_train=(112, 4), X_test=(38, 4), y_train=(112,), y_\\\n",
      "       test=(38,)\n",
      "\n",
      "INFO:  TRAINING['MyLinReg']..\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MyLinReg.fit() missing 1 required positional argument: 'method'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 43\u001b[0m\n\u001b[0;32m     39\u001b[0m         Info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m##############################################\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# somewhat more verbose testing, you regressor will likely fail on MNIST \u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# or at least be very, very slow...\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m TestAndCompareRegressors()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[44], line 29\u001b[0m, in \u001b[0;36mTestAndCompareRegressors\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m Info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTRAINING[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()), r])\n\u001b[1;32m---> 29\u001b[0m pipe\u001b[38;5;241m.\u001b[39mfit(X_train, y_train) \u001b[38;5;66;03m#### HER HER HER\u001b[39;00m\n\u001b[0;32m     31\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     33\u001b[0m PrintMatrix(y_pred_test, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred_test=\u001b[39m\u001b[38;5;124m\"\u001b[39m, precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\morte\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\morte\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: MyLinReg.fit() missing 1 required positional argument: 'method'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model    import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.pipeline        import Pipeline\n",
    "\n",
    "try:\n",
    "    from libitmal import dataloaders\n",
    "except Exception as ex:\n",
    "    Err(\"can not import dataloaders form libitmal, and then I can not run the TestAndCompareRegressors smoke-test, sorry!\", ex)\n",
    "\n",
    "def TestAndCompareRegressors():\n",
    "    for f in [(\"IRIS\",  dataloaders.IRIS_GetDataSet,  1E-2),\n",
    "              (\"MNIST\", dataloaders.MNIST_GetDataSet, 1E-3)]:\n",
    "        \n",
    "        # NOTE: f-tuble is (<name>, <data-loader-function-pointer>, <eps0>)\n",
    "        data = f[1]() # returns (X, y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data[0], data[1])\n",
    "        \n",
    "        Info(f\"DATA: '{f[0]}'\\n\\tSHAPES: X_train={X_train.shape}, X_test={X_test.shape}, y_train={y_train.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "        eta0 = f[2] # an adaptive learning rate is really needed here!\n",
    "        regressor0 = MyLinReg(eta0=eta0, max_iter=1000)\n",
    "        regressor1 = SGDRegressor()    \n",
    "\n",
    "        for r in [(\"MyLinReg\", regressor0), (\"SGDRegressor\", regressor1)]:\n",
    "            Info(f\"\\nTRAINING['{r[0]}']..\")\n",
    "            \n",
    "            pipe = Pipeline([('scaler', StandardScaler()), r])\n",
    "            pipe.fit(X_train, y_train) #### HER HER HER\n",
    "            \n",
    "            y_pred_test = pipe.predict(X_test)\n",
    "            \n",
    "            PrintMatrix(y_pred_test, label=\"y_pred_test=\", precision=4)\n",
    "            print()\n",
    "            \n",
    "            r2 = pipe.score(X_test, y_test)\n",
    "            Info(f\"SCORE['{r[0]}'] = {Col('lblue')}{r2:0.3f}{ColEnd()}\")\n",
    "            \n",
    "        Info(\"\\n##############################################\\n\")\n",
    "\n",
    "# somewhat more verbose testing, you regressor will likely fail on MNIST \n",
    "# or at least be very, very slow...\n",
    "TestAndCompareRegressors()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qh Conclusion\n",
    "\n",
    "As always, take some time to fine-tune your regressor, perhaps just some code-refactoring, cleaning out 'bad' code, and summarize all your findings\n",
    " above. \n",
    "\n",
    "In other words, write a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2022-12-22| CEF, initial draft. \n",
    "2023-02-26| CEF, first release.\n",
    "2023-02-28| CEF, fix a few issues related to import from libitmal, added Info and color output.\n",
    "2024-09-19| CEF, major overhaul, change math/text and code snippets.\n",
    "2024-09-25| CEF, final fixes, tests, and proof-reading. Moved early stopping and learning graphs to a later excercise.\n",
    "2024-10-04| CEF, clarified Qa with respect to what-is-to-be implemented and what-is-to-be described in text only."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
