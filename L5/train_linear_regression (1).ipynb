{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "(In the following you need not present your journal in the Qa+b+c+ etc. order. You could just present the final code with test and comments.)\n",
    "\n",
    "## Training Your Own Linear Regressor\n",
    "\n",
    "Create a linear regressor, with a Scikit-learn compatible fit-predict interface. You should implement every detail of the linear regressor in Python, using whatever libraries, say `numpy`, you want (except a linear regressor itself).\n",
    "\n",
    "Below is a primitive _get-started_ skeleton for your implementation. Keep the class name `MyLinReg`, which is used in the test sequence later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "\n",
    "class MyLinReg():\n",
    "    def __init__(self, eta0=0.01, max_iter=10000, tol=1e-3, n_iter_no_change=5, verbose=True):\n",
    "        self.eta0 = eta0\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tol\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.verbose = verbose\n",
    "        self.coef = None # vores w\n",
    "        self.intercept = None    \n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"MyLinReg.__str__(): hi!\"\n",
    "\n",
    "    def fit(self, X, y, method):\n",
    "        X = add_dummy_feature(X) # Augmenter med 1 taller\n",
    "        assert X.shape[0] == y.shape[0], \"X og y skal være lige store\"\n",
    "        self.coef = np.zeros(X.shape[1])\n",
    "        prev_loss = 99999999999999999999\n",
    "        no_change_counter = 0\n",
    "\n",
    "        for ep in range(self.max_iter):\n",
    "            if method == \"GD\":\n",
    "                # GD\n",
    "                y_pred = X @ self.coef\n",
    "                error = y_pred - y\n",
    "                gradient = X.T @ error / len(y) # hvor y_pred er Xw\n",
    "                self.coef -= gradient * self.eta0\n",
    "                # GD\n",
    "            elif method == \"SGD\":\n",
    "                # SGD\n",
    "                for i in range(len(y)):\n",
    "                    y_pred = X[i] @ self.coef\n",
    "                    error = y_pred - y[i]\n",
    "                    gradient = X[i] * error\n",
    "                    self.coef -= gradient * self.eta0\n",
    "                #SGD\n",
    "\n",
    "            loss = np.sqrt(np.mean(((X @ self.coef) - y) ** 2)) # vores cost RMSE, som svarer til gennemsnit af (y_pred - y)^2 \n",
    "\n",
    "            print(\"it: \", ep+1, \"\\nLoss: \", loss)\n",
    "\n",
    "            # tjek om den improver\n",
    "            if abs(prev_loss - loss) < self.tolerance:\n",
    "                no_change_counter += 1\n",
    "            else:\n",
    "                no_change_counter = 0\n",
    "\n",
    "            if no_change_counter >= self.n_iter_no_change:\n",
    "                break\n",
    "            prev_loss = loss\n",
    "        \n",
    "        self.intercept = self.coef[0]\n",
    "        self.coef = self.coef[1:]\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = add_dummy_feature(X)\n",
    "        return X @ np.r_[self.intercept, self.coef]\n",
    "        # assert False, \"TODO: implement me\"\n",
    "\n",
    "    def score(self, X, y_true):\n",
    "        y_pred = self.predict(X)\n",
    "        sum_squares_total = np.sum((y_true - np.mean(y_true))**2)\n",
    "        sum_squares_residual = np.sum((y_true - y_pred)**2)\n",
    "        return 1 - (sum_squares_residual / sum_squares_total) # Scoren\n",
    "\n",
    "regressor = MyLinReg(eta0=0.01, max_iter=1000, tol=1e-6, n_iter_no_change=10, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TODO list\n",
    "\n",
    "You must investigate and describe all major details for a linear regressor, and implement at least the following concepts (MUST):\n",
    "\n",
    "### Qa: Concepts and Implementations MUSTS\n",
    "\n",
    "* Implement: the `fit-predict` interface, for a one-dimensional output only, \n",
    "* Implement: a $R^2$ score function (re-use existing code or perhaps just inherit it), \n",
    "* Implement: loss function based on (R)MSE,\n",
    "* Implement: setting of the number of iterations and learning rate ($\\eta$) via parameters in the constructor (the signature of your `__init__` must include the named parameters `eta0` and `max_iter`),\n",
    "* (in a later exercise we will also add `tol`, `n_iter_no_change` and `verbose` to the constructor),\n",
    "* Implement: the batch-gradient decent algorithm (GD),\n",
    "* Implement: constant learning rate (maybe also adaptive learning rate if you are brave),\n",
    "* Implement: stochastic gradient descent (SGD),\n",
    "* Describe in text: epochs vs iterations,\n",
    "* Describe in text: compare the numerical optimization with the Closed-form solution.\n",
    "\n",
    "I det her kodeudsnit har vi implementeret et fit-predict interface for en linær regressor. Vi har også implementeret en R^2 score funktion, en loss funktion baseret på RMSE og vi har implementeret en konstant learning rate med eta parametrene. Vi har også implementeret batch-gradient decent algoritmen og stochastic gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "### Qb: [OPTIONAL] Additional Concepts and Implementations\n",
    "\n",
    "And perhaps you could include (SHOULD/COULD):\n",
    "\n",
    "* (stochastic) mini-bach gradient decent, \n",
    "* interface to your bias and weights via `intercept_` and `coef_` attributes on your linear regressor `class`,\n",
    "* get/set functionality of your regressor, such that it is fully compatible with other Scikit-learn algorithms, try it out in say a `cross_val_score()` call from Scikit-learn,\n",
    "* test in via the smoke tests at the end of this Notebook,\n",
    "* testing it on MNIST data.\n",
    "\n",
    "With the following no-no's (WONT):\n",
    "\n",
    "* no learning graphs, no early stopping (we will do this in a later exercise),\n",
    "* no multi-linear regression,\n",
    "* no reuse of the Scikit-learn regressor,\n",
    "* no `C/C++` optimized implementation with a _thin_ Python interface (nifty, but out-of-scope for this cause),\n",
    "* no copy-paste of code from other sources WITHOUT a clear cite/reference for your source.\n",
    "\n",
    "### Qc: Testing and Test Data\n",
    "\n",
    "Use mainly very low-dimensional data for testing, say the IRIS set, since it might be very slow. Or create a simple low-dimensionality data generator.\n",
    "\n",
    "(There is a _micro_ data set in the function `GenerateData` in the smoke tests functions below, but better is to opt for an realistic data set.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1]\n",
      " [4.9]\n",
      " [4.7]\n",
      " [4.6]\n",
      " [5. ]\n",
      " [5.4]\n",
      " [4.6]\n",
      " [5. ]\n",
      " [4.4]\n",
      " [4.9]\n",
      " [5.4]\n",
      " [4.8]\n",
      " [4.8]\n",
      " [4.3]\n",
      " [5.8]\n",
      " [5.7]\n",
      " [5.4]\n",
      " [5.1]\n",
      " [5.7]\n",
      " [5.1]\n",
      " [5.4]\n",
      " [5.1]\n",
      " [4.6]\n",
      " [5.1]\n",
      " [4.8]\n",
      " [5. ]\n",
      " [5. ]\n",
      " [5.2]\n",
      " [5.2]\n",
      " [4.7]\n",
      " [4.8]\n",
      " [5.4]\n",
      " [5.2]\n",
      " [5.5]\n",
      " [4.9]\n",
      " [5. ]\n",
      " [5.5]\n",
      " [4.9]\n",
      " [4.4]\n",
      " [5.1]\n",
      " [5. ]\n",
      " [4.5]\n",
      " [4.4]\n",
      " [5. ]\n",
      " [5.1]\n",
      " [4.8]\n",
      " [5.1]\n",
      " [4.6]\n",
      " [5.3]\n",
      " [5. ]\n",
      " [7. ]\n",
      " [6.4]\n",
      " [6.9]\n",
      " [5.5]\n",
      " [6.5]\n",
      " [5.7]\n",
      " [6.3]\n",
      " [4.9]\n",
      " [6.6]\n",
      " [5.2]\n",
      " [5. ]\n",
      " [5.9]\n",
      " [6. ]\n",
      " [6.1]\n",
      " [5.6]\n",
      " [6.7]\n",
      " [5.6]\n",
      " [5.8]\n",
      " [6.2]\n",
      " [5.6]\n",
      " [5.9]\n",
      " [6.1]\n",
      " [6.3]\n",
      " [6.1]\n",
      " [6.4]\n",
      " [6.6]\n",
      " [6.8]\n",
      " [6.7]\n",
      " [6. ]\n",
      " [5.7]\n",
      " [5.5]\n",
      " [5.5]\n",
      " [5.8]\n",
      " [6. ]\n",
      " [5.4]\n",
      " [6. ]\n",
      " [6.7]\n",
      " [6.3]\n",
      " [5.6]\n",
      " [5.5]\n",
      " [5.5]\n",
      " [6.1]\n",
      " [5.8]\n",
      " [5. ]\n",
      " [5.6]\n",
      " [5.7]\n",
      " [5.7]\n",
      " [6.2]\n",
      " [5.1]\n",
      " [5.7]\n",
      " [6.3]\n",
      " [5.8]\n",
      " [7.1]\n",
      " [6.3]\n",
      " [6.5]\n",
      " [7.6]\n",
      " [4.9]\n",
      " [7.3]\n",
      " [6.7]\n",
      " [7.2]\n",
      " [6.5]\n",
      " [6.4]\n",
      " [6.8]\n",
      " [5.7]\n",
      " [5.8]\n",
      " [6.4]\n",
      " [6.5]\n",
      " [7.7]\n",
      " [7.7]\n",
      " [6. ]\n",
      " [6.9]\n",
      " [5.6]\n",
      " [7.7]\n",
      " [6.3]\n",
      " [6.7]\n",
      " [7.2]\n",
      " [6.2]\n",
      " [6.1]\n",
      " [6.4]\n",
      " [7.2]\n",
      " [7.4]\n",
      " [7.9]\n",
      " [6.4]\n",
      " [6.3]\n",
      " [6.1]\n",
      " [7.7]\n",
      " [6.3]\n",
      " [6.4]\n",
      " [6. ]\n",
      " [6.9]\n",
      " [6.7]\n",
      " [6.9]\n",
      " [5.8]\n",
      " [6.8]\n",
      " [6.7]\n",
      " [6.7]\n",
      " [6.3]\n",
      " [6.5]\n",
      " [6.2]\n",
      " [5.9]]\n",
      "it:  1 \n",
      "Loss:  3.0774529589008432\n",
      "it:  2 \n",
      "Loss:  3.0669039927349955\n",
      "it:  3 \n",
      "Loss:  3.0563944155179628\n",
      "it:  4 \n",
      "Loss:  3.045924091265593\n",
      "it:  5 \n",
      "Loss:  3.035492884491261\n",
      "it:  6 \n",
      "Loss:  3.0251006602040516\n",
      "it:  7 \n",
      "Loss:  3.0147472839069485\n",
      "it:  8 \n",
      "Loss:  3.004432621595038\n",
      "it:  9 \n",
      "Loss:  2.9941565397537038\n",
      "it:  10 \n",
      "Loss:  2.9839189053568385\n",
      "it:  11 \n",
      "Loss:  2.9737195858650574\n",
      "it:  12 \n",
      "Loss:  2.9635584492239175\n",
      "it:  13 \n",
      "Loss:  2.953435363862141\n",
      "it:  14 \n",
      "Loss:  2.9433501986898474\n",
      "it:  15 \n",
      "Loss:  2.9333028230967897\n",
      "it:  16 \n",
      "Loss:  2.9232931069505943\n",
      "it:  17 \n",
      "Loss:  2.913320920595009\n",
      "it:  18 \n",
      "Loss:  2.903386134848155\n",
      "it:  19 \n",
      "Loss:  2.8934886210007877\n",
      "it:  20 \n",
      "Loss:  2.8836282508145556\n",
      "it:  21 \n",
      "Loss:  2.873804896520272\n",
      "it:  22 \n",
      "Loss:  2.864018430816189\n",
      "it:  23 \n",
      "Loss:  2.854268726866276\n",
      "it:  24 \n",
      "Loss:  2.8445556582985048\n",
      "it:  25 \n",
      "Loss:  2.8348790992031394\n",
      "it:  26 \n",
      "Loss:  2.825238924131034\n",
      "it:  27 \n",
      "Loss:  2.8156350080919292\n",
      "it:  28 \n",
      "Loss:  2.806067226552762\n",
      "it:  29 \n",
      "Loss:  2.796535455435974\n",
      "it:  30 \n",
      "Loss:  2.7870395711178273\n",
      "it:  31 \n",
      "Loss:  2.77757945042673\n",
      "it:  32 \n",
      "Loss:  2.768154970641557\n",
      "it:  33 \n",
      "Loss:  2.758766009489986\n",
      "it:  34 \n",
      "Loss:  2.7494124451468314\n",
      "it:  35 \n",
      "Loss:  2.740094156232386\n",
      "it:  36 \n",
      "Loss:  2.73081102181077\n",
      "it:  37 \n",
      "Loss:  2.72156292138828\n",
      "it:  38 \n",
      "Loss:  2.7123497349117462\n",
      "it:  39 \n",
      "Loss:  2.703171342766895\n",
      "it:  40 \n",
      "Loss:  2.6940276257767137\n",
      "it:  41 \n",
      "Loss:  2.684918465199824\n",
      "it:  42 \n",
      "Loss:  2.6758437427288557\n",
      "it:  43 \n",
      "Loss:  2.6668033404888285\n",
      "it:  44 \n",
      "Loss:  2.6577971410355388\n",
      "it:  45 \n",
      "Loss:  2.648825027353949\n",
      "it:  46 \n",
      "Loss:  2.6398868828565845\n",
      "it:  47 \n",
      "Loss:  2.630982591381931\n",
      "it:  48 \n",
      "Loss:  2.6221120371928412\n",
      "it:  49 \n",
      "Loss:  2.6132751049749463\n",
      "it:  50 \n",
      "Loss:  2.6044716798350636\n",
      "it:  51 \n",
      "Loss:  2.595701647299623\n",
      "it:  52 \n",
      "Loss:  2.5869648933130853\n",
      "it:  53 \n",
      "Loss:  2.5782613042363725\n",
      "it:  54 \n",
      "Loss:  2.569590766845298\n",
      "it:  55 \n",
      "Loss:  2.5609531683290085\n",
      "it:  56 \n",
      "Loss:  2.552348396288418\n",
      "it:  57 \n",
      "Loss:  2.5437763387346624\n",
      "it:  58 \n",
      "Loss:  2.5352368840875434\n",
      "it:  59 \n",
      "Loss:  2.5267299211739873\n",
      "it:  60 \n",
      "Loss:  2.5182553392265037\n",
      "it:  61 \n",
      "Loss:  2.509813027881648\n",
      "it:  62 \n",
      "Loss:  2.5014028771784904\n",
      "it:  63 \n",
      "Loss:  2.493024777557091\n",
      "it:  64 \n",
      "Loss:  2.4846786198569712\n",
      "it:  65 \n",
      "Loss:  2.4763642953156024\n",
      "it:  66 \n",
      "Loss:  2.4680816955668843\n",
      "it:  67 \n",
      "Loss:  2.459830712639639\n",
      "it:  68 \n",
      "Loss:  2.4516112389561044\n",
      "it:  69 \n",
      "Loss:  2.443423167330432\n",
      "it:  70 \n",
      "Loss:  2.4352663909671883\n",
      "it:  71 \n",
      "Loss:  2.4271408034598654\n",
      "it:  72 \n",
      "Loss:  2.419046298789387\n",
      "it:  73 \n",
      "Loss:  2.410982771322626\n",
      "it:  74 \n",
      "Loss:  2.402950115810924\n",
      "it:  75 \n",
      "Loss:  2.3949482273886127\n",
      "it:  76 \n",
      "Loss:  2.3869770015715432\n",
      "it:  77 \n",
      "Loss:  2.379036334255615\n",
      "it:  78 \n",
      "Loss:  2.371126121715314\n",
      "it:  79 \n",
      "Loss:  2.3632462606022497\n",
      "it:  80 \n",
      "Loss:  2.3553966479436994\n",
      "it:  81 \n",
      "Loss:  2.3475771811411557\n",
      "it:  82 \n",
      "Loss:  2.3397877579688773\n",
      "it:  83 \n",
      "Loss:  2.3320282765724443\n",
      "it:  84 \n",
      "Loss:  2.324298635467318\n",
      "it:  85 \n",
      "Loss:  2.3165987335374023\n",
      "it:  86 \n",
      "Loss:  2.308928470033613\n",
      "it:  87 \n",
      "Loss:  2.301287744572444\n",
      "it:  88 \n",
      "Loss:  2.293676457134548\n",
      "it:  89 \n",
      "Loss:  2.2860945080633104\n",
      "it:  90 \n",
      "Loss:  2.278541798063432\n",
      "it:  91 \n",
      "Loss:  2.271018228199515\n",
      "it:  92 \n",
      "Loss:  2.2635236998946544\n",
      "it:  93 \n",
      "Loss:  2.2560581149290293\n",
      "it:  94 \n",
      "Loss:  2.2486213754384994\n",
      "it:  95 \n",
      "Loss:  2.241213383913209\n",
      "it:  96 \n",
      "Loss:  2.2338340431961865\n",
      "it:  97 \n",
      "Loss:  2.2264832564819574\n",
      "it:  98 \n",
      "Loss:  2.2191609273151522\n",
      "it:  99 \n",
      "Loss:  2.211866959589124\n",
      "it:  100 \n",
      "Loss:  2.2046012575445655\n",
      "it:  101 \n",
      "Loss:  2.1973637257681333\n",
      "it:  102 \n",
      "Loss:  2.190154269191072\n",
      "it:  103 \n",
      "Loss:  2.1829727930878455\n",
      "it:  104 \n",
      "Loss:  2.1758192030747687\n",
      "it:  105 \n",
      "Loss:  2.168693405108644\n",
      "it:  106 \n",
      "Loss:  2.161595305485404\n",
      "it:  107 \n",
      "Loss:  2.1545248108387494\n",
      "it:  108 \n",
      "Loss:  2.147481828138802\n",
      "it:  109 \n",
      "Loss:  2.140466264690751\n",
      "it:  110 \n",
      "Loss:  2.13347802813351\n",
      "it:  111 \n",
      "Loss:  2.1265170264383695\n",
      "it:  112 \n",
      "Loss:  2.1195831679076633\n",
      "it:  113 \n",
      "Loss:  2.1126763611734267\n",
      "it:  114 \n",
      "Loss:  2.105796515196069\n",
      "it:  115 \n",
      "Loss:  2.0989435392630402\n",
      "it:  116 \n",
      "Loss:  2.0921173429875064\n",
      "it:  117 \n",
      "Loss:  2.085317836307027\n",
      "it:  118 \n",
      "Loss:  2.0785449294822356\n",
      "it:  119 \n",
      "Loss:  2.071798533095523\n",
      "it:  120 \n",
      "Loss:  2.065078558049725\n",
      "it:  121 \n",
      "Loss:  2.0583849155668106\n",
      "it:  122 \n",
      "Loss:  2.05171751718658\n",
      "it:  123 \n",
      "Loss:  2.045076274765355\n",
      "it:  124 \n",
      "Loss:  2.0384611004746858\n",
      "it:  125 \n",
      "Loss:  2.0318719068000477\n",
      "it:  126 \n",
      "Loss:  2.0253086065395522\n",
      "it:  127 \n",
      "Loss:  2.0187711128026535\n",
      "it:  128 \n",
      "Loss:  2.0122593390088626\n",
      "it:  129 \n",
      "Loss:  2.0057731988864607\n",
      "it:  130 \n",
      "Loss:  1.9993126064712217\n",
      "it:  131 \n",
      "Loss:  1.9928774761051309\n",
      "it:  132 \n",
      "Loss:  1.9864677224351108\n",
      "it:  133 \n",
      "Loss:  1.9800832604117502\n",
      "it:  134 \n",
      "Loss:  1.973724005288034\n",
      "it:  135 \n",
      "Loss:  1.9673898726180776\n",
      "it:  136 \n",
      "Loss:  1.961080778255864\n",
      "it:  137 \n",
      "Loss:  1.9547966383539848\n",
      "it:  138 \n",
      "Loss:  1.9485373693623826\n",
      "it:  139 \n",
      "Loss:  1.9423028880270978\n",
      "it:  140 \n",
      "Loss:  1.9360931113890165\n",
      "it:  141 \n",
      "Loss:  1.9299079567826245\n",
      "it:  142 \n",
      "Loss:  1.923747341834761\n",
      "it:  143 \n",
      "Loss:  1.9176111844633772\n",
      "it:  144 \n",
      "Loss:  1.9114994028762982\n",
      "it:  145 \n",
      "Loss:  1.9054119155699851\n",
      "it:  146 \n",
      "Loss:  1.8993486413283036\n",
      "it:  147 \n",
      "Loss:  1.8933094992212922\n",
      "it:  148 \n",
      "Loss:  1.8872944086039376\n",
      "it:  149 \n",
      "Loss:  1.8813032891149466\n",
      "it:  150 \n",
      "Loss:  1.875336060675528\n",
      "it:  151 \n",
      "Loss:  1.8693926434881725\n",
      "it:  152 \n",
      "Loss:  1.863472958035436\n",
      "it:  153 \n",
      "Loss:  1.8575769250787286\n",
      "it:  154 \n",
      "Loss:  1.8517044656571038\n",
      "it:  155 \n",
      "Loss:  1.8458555010860511\n",
      "it:  156 \n",
      "Loss:  1.8400299529562916\n",
      "it:  157 \n",
      "Loss:  1.8342277431325764\n",
      "it:  158 \n",
      "Loss:  1.8284487937524896\n",
      "it:  159 \n",
      "Loss:  1.8226930272252497\n",
      "it:  160 \n",
      "Loss:  1.8169603662305183\n",
      "it:  161 \n",
      "Loss:  1.811250733717209\n",
      "it:  162 \n",
      "Loss:  1.8055640529023018\n",
      "it:  163 \n",
      "Loss:  1.799900247269656\n",
      "it:  164 \n",
      "Loss:  1.794259240568829\n",
      "it:  165 \n",
      "Loss:  1.7886409568138983\n",
      "it:  166 \n",
      "Loss:  1.783045320282285\n",
      "it:  167 \n",
      "Loss:  1.7774722555135785\n",
      "it:  168 \n",
      "Loss:  1.7719216873083672\n",
      "it:  169 \n",
      "Loss:  1.766393540727071\n",
      "it:  170 \n",
      "Loss:  1.760887741088775\n",
      "it:  171 \n",
      "Loss:  1.755404213970068\n",
      "it:  172 \n",
      "Loss:  1.749942885203881\n",
      "it:  173 \n",
      "Loss:  1.7445036808783314\n",
      "it:  174 \n",
      "Loss:  1.7390865273355705\n",
      "it:  175 \n",
      "Loss:  1.7336913511706278\n",
      "it:  176 \n",
      "Loss:  1.728318079230267\n",
      "it:  177 \n",
      "Loss:  1.7229666386118363\n",
      "it:  178 \n",
      "Loss:  1.7176369566621268\n",
      "it:  179 \n",
      "Loss:  1.7123289609762322\n",
      "it:  180 \n",
      "Loss:  1.7070425793964108\n",
      "it:  181 \n",
      "Loss:  1.701777740010949\n",
      "it:  182 \n",
      "Loss:  1.6965343711530319\n",
      "it:  183 \n",
      "Loss:  1.6913124013996115\n",
      "it:  184 \n",
      "Loss:  1.6861117595702801\n",
      "it:  185 \n",
      "Loss:  1.6809323747261466\n",
      "it:  186 \n",
      "Loss:  1.6757741761687162\n",
      "it:  187 \n",
      "Loss:  1.6706370934387698\n",
      "it:  188 \n",
      "Loss:  1.6655210563152492\n",
      "it:  189 \n",
      "Loss:  1.6604259948141447\n",
      "it:  190 \n",
      "Loss:  1.6553518391873825\n",
      "it:  191 \n",
      "Loss:  1.6502985199217204\n",
      "it:  192 \n",
      "Loss:  1.6452659677376398\n",
      "it:  193 \n",
      "Loss:  1.6402541135882451\n",
      "it:  194 \n",
      "Loss:  1.635262888658164\n",
      "it:  195 \n",
      "Loss:  1.6302922243624518\n",
      "it:  196 \n",
      "Loss:  1.6253420523454953\n",
      "it:  197 \n",
      "Loss:  1.6204123044799246\n",
      "it:  198 \n",
      "Loss:  1.6155029128655236\n",
      "it:  199 \n",
      "Loss:  1.610613809828143\n",
      "it:  200 \n",
      "Loss:  1.60574492791862\n",
      "it:  201 \n",
      "Loss:  1.6008961999116975\n",
      "it:  202 \n",
      "Loss:  1.5960675588049456\n",
      "it:  203 \n",
      "Loss:  1.5912589378176898\n",
      "it:  204 \n",
      "Loss:  1.586470270389938\n",
      "it:  205 \n",
      "Loss:  1.5817014901813111\n",
      "it:  206 \n",
      "Loss:  1.5769525310699788\n",
      "it:  207 \n",
      "Loss:  1.572223327151596\n",
      "it:  208 \n",
      "Loss:  1.5675138127382415\n",
      "it:  209 \n",
      "Loss:  1.562823922357362\n",
      "it:  210 \n",
      "Loss:  1.5581535907507171\n",
      "it:  211 \n",
      "Loss:  1.553502752873328\n",
      "it:  212 \n",
      "Loss:  1.5488713438924284\n",
      "it:  213 \n",
      "Loss:  1.5442592991864184\n",
      "it:  214 \n",
      "Loss:  1.5396665543438217\n",
      "it:  215 \n",
      "Loss:  1.5350930451622464\n",
      "it:  216 \n",
      "Loss:  1.530538707647346\n",
      "it:  217 \n",
      "Loss:  1.5260034780117866\n",
      "it:  218 \n",
      "Loss:  1.5214872926742162\n",
      "it:  219 \n",
      "Loss:  1.5169900882582343\n",
      "it:  220 \n",
      "Loss:  1.5125118015913683\n",
      "it:  221 \n",
      "Loss:  1.508052369704049\n",
      "it:  222 \n",
      "Loss:  1.5036117298285954\n",
      "it:  223 \n",
      "Loss:  1.4991898193981923\n",
      "it:  224 \n",
      "Loss:  1.4947865760458814\n",
      "it:  225 \n",
      "Loss:  1.4904019376035504\n",
      "it:  226 \n",
      "Loss:  1.4860358421009232\n",
      "it:  227 \n",
      "Loss:  1.4816882277645578\n",
      "it:  228 \n",
      "Loss:  1.4773590330168447\n",
      "it:  229 \n",
      "Loss:  1.473048196475008\n",
      "it:  230 \n",
      "Loss:  1.4687556569501106\n",
      "it:  231 \n",
      "Loss:  1.4644813534460623\n",
      "it:  232 \n",
      "Loss:  1.4602252251586318\n",
      "it:  233 \n",
      "Loss:  1.4559872114744596\n",
      "it:  234 \n",
      "Loss:  1.4517672519700764\n",
      "it:  235 \n",
      "Loss:  1.447565286410923\n",
      "it:  236 \n",
      "Loss:  1.4433812547503748\n",
      "it:  237 \n",
      "Loss:  1.439215097128769\n",
      "it:  238 \n",
      "Loss:  1.435066753872434\n",
      "it:  239 \n",
      "Loss:  1.4309361654927233\n",
      "it:  240 \n",
      "Loss:  1.4268232726850532\n",
      "it:  241 \n",
      "Loss:  1.4227280163279408\n",
      "it:  242 \n",
      "Loss:  1.4186503374820487\n",
      "it:  243 \n",
      "Loss:  1.4145901773892322\n",
      "it:  244 \n",
      "Loss:  1.4105474774715883\n",
      "it:  245 \n",
      "Loss:  1.4065221793305092\n",
      "it:  246 \n",
      "Loss:  1.4025142247457387\n",
      "it:  247 \n",
      "Loss:  1.3985235556744338\n",
      "it:  248 \n",
      "Loss:  1.394550114250226\n",
      "it:  249 \n",
      "Loss:  1.3905938427822901\n",
      "it:  250 \n",
      "Loss:  1.3866546837544143\n",
      "it:  251 \n",
      "Loss:  1.3827325798240728\n",
      "it:  252 \n",
      "Loss:  1.3788274738215052\n",
      "it:  253 \n",
      "Loss:  1.3749393087487953\n",
      "it:  254 \n",
      "Loss:  1.371068027778958\n",
      "it:  255 \n",
      "Loss:  1.3672135742550242\n",
      "it:  256 \n",
      "Loss:  1.3633758916891356\n",
      "it:  257 \n",
      "Loss:  1.359554923761638\n",
      "it:  258 \n",
      "Loss:  1.35575061432018\n",
      "it:  259 \n",
      "Loss:  1.3519629073788166\n",
      "it:  260 \n",
      "Loss:  1.3481917471171152\n",
      "it:  261 \n",
      "Loss:  1.3444370778792643\n",
      "it:  262 \n",
      "Loss:  1.3406988441731873\n",
      "it:  263 \n",
      "Loss:  1.336976990669662\n",
      "it:  264 \n",
      "Loss:  1.333271462201439\n",
      "it:  265 \n",
      "Loss:  1.3295822037623675\n",
      "it:  266 \n",
      "Loss:  1.3259091605065239\n",
      "it:  267 \n",
      "Loss:  1.3222522777473458\n",
      "it:  268 \n",
      "Loss:  1.3186115009567663\n",
      "it:  269 \n",
      "Loss:  1.3149867757643543\n",
      "it:  270 \n",
      "Loss:  1.311378047956461\n",
      "it:  271 \n",
      "Loss:  1.3077852634753657\n",
      "it:  272 \n",
      "Loss:  1.3042083684184287\n",
      "it:  273 \n",
      "Loss:  1.3006473090372481\n",
      "it:  274 \n",
      "Loss:  1.2971020317368185\n",
      "it:  275 \n",
      "Loss:  1.2935724830746964\n",
      "it:  276 \n",
      "Loss:  1.290058609760167\n",
      "it:  277 \n",
      "Loss:  1.2865603586534187\n",
      "it:  278 \n",
      "Loss:  1.2830776767647172\n",
      "it:  279 \n",
      "Loss:  1.2796105112535874\n",
      "it:  280 \n",
      "Loss:  1.2761588094279976\n",
      "it:  281 \n",
      "Loss:  1.272722518743549\n",
      "it:  282 \n",
      "Loss:  1.2693015868026676\n",
      "it:  283 \n",
      "Loss:  1.2658959613538032\n",
      "it:  284 \n",
      "Loss:  1.2625055902906295\n",
      "it:  285 \n",
      "Loss:  1.2591304216512513\n",
      "it:  286 \n",
      "Loss:  1.2557704036174138\n",
      "it:  287 \n",
      "Loss:  1.252425484513718\n",
      "it:  288 \n",
      "Loss:  1.2490956128068387\n",
      "it:  289 \n",
      "Loss:  1.2457807371047493\n",
      "it:  290 \n",
      "Loss:  1.2424808061559487\n",
      "it:  291 \n",
      "Loss:  1.2391957688486928\n",
      "it:  292 \n",
      "Loss:  1.2359255742102333\n",
      "it:  293 \n",
      "Loss:  1.2326701714060573\n",
      "it:  294 \n",
      "Loss:  1.2294295097391332\n",
      "it:  295 \n",
      "Loss:  1.2262035386491623\n",
      "it:  296 \n",
      "Loss:  1.2229922077118325\n",
      "it:  297 \n",
      "Loss:  1.219795466638078\n",
      "it:  298 \n",
      "Loss:  1.216613265273344\n",
      "it:  299 \n",
      "Loss:  1.2134455535968547\n",
      "it:  300 \n",
      "Loss:  1.2102922817208885\n",
      "it:  301 \n",
      "Loss:  1.2071533998900543\n",
      "it:  302 \n",
      "Loss:  1.2040288584805758\n",
      "it:  303 \n",
      "Loss:  1.200918607999578\n",
      "it:  304 \n",
      "Loss:  1.1978225990843814\n",
      "it:  305 \n",
      "Loss:  1.1947407825017982\n",
      "it:  306 \n",
      "Loss:  1.1916731091474344\n",
      "it:  307 \n",
      "Loss:  1.1886195300449975\n",
      "it:  308 \n",
      "Loss:  1.1855799963456088\n",
      "it:  309 \n",
      "Loss:  1.182554459327118\n",
      "it:  310 \n",
      "Loss:  1.1795428703934285\n",
      "it:  311 \n",
      "Loss:  1.1765451810738212\n",
      "it:  312 \n",
      "Loss:  1.173561343022287\n",
      "it:  313 \n",
      "Loss:  1.1705913080168662\n",
      "it:  314 \n",
      "Loss:  1.1676350279589849\n",
      "it:  315 \n",
      "Loss:  1.1646924548728066\n",
      "it:  316 \n",
      "Loss:  1.161763540904583\n",
      "it:  317 \n",
      "Loss:  1.1588482383220096\n",
      "it:  318 \n",
      "Loss:  1.155946499513589\n",
      "it:  319 \n",
      "Loss:  1.1530582769879978\n",
      "it:  320 \n",
      "Loss:  1.1501835233734603\n",
      "it:  321 \n",
      "Loss:  1.147322191417124\n",
      "it:  322 \n",
      "Loss:  1.1444742339844443\n",
      "it:  323 \n",
      "Loss:  1.1416396040585717\n",
      "it:  324 \n",
      "Loss:  1.1388182547397452\n",
      "it:  325 \n",
      "Loss:  1.1360101392446915\n",
      "it:  326 \n",
      "Loss:  1.1332152109060287\n",
      "it:  327 \n",
      "Loss:  1.130433423171675\n",
      "it:  328 \n",
      "Loss:  1.1276647296042641\n",
      "it:  329 \n",
      "Loss:  1.1249090838805644\n",
      "it:  330 \n",
      "Loss:  1.1221664397909055\n",
      "it:  331 \n",
      "Loss:  1.119436751238607\n",
      "it:  332 \n",
      "Loss:  1.1167199722394163\n",
      "it:  333 \n",
      "Loss:  1.1140160569209492\n",
      "it:  334 \n",
      "Loss:  1.111324959522138\n",
      "it:  335 \n",
      "Loss:  1.1086466343926815\n",
      "it:  336 \n",
      "Loss:  1.105981035992506\n",
      "it:  337 \n",
      "Loss:  1.1033281188912265\n",
      "it:  338 \n",
      "Loss:  1.100687837767616\n",
      "it:  339 \n",
      "Loss:  1.098060147409081\n",
      "it:  340 \n",
      "Loss:  1.0954450027111393\n",
      "it:  341 \n",
      "Loss:  1.092842358676908\n",
      "it:  342 \n",
      "Loss:  1.0902521704165933\n",
      "it:  343 \n",
      "Loss:  1.0876743931469863\n",
      "it:  344 \n",
      "Loss:  1.0851089821909663\n",
      "it:  345 \n",
      "Loss:  1.082555892977009\n",
      "it:  346 \n",
      "Loss:  1.0800150810386986\n",
      "it:  347 \n",
      "Loss:  1.077486502014247\n",
      "it:  348 \n",
      "Loss:  1.0749701116460206\n",
      "it:  349 \n",
      "Loss:  1.0724658657800668\n",
      "it:  350 \n",
      "Loss:  1.069973720365653\n",
      "it:  351 \n",
      "Loss:  1.0674936314548071\n",
      "it:  352 \n",
      "Loss:  1.0650255552018655\n",
      "it:  353 \n",
      "Loss:  1.0625694478630234\n",
      "it:  354 \n",
      "Loss:  1.060125265795897\n",
      "it:  355 \n",
      "Loss:  1.0576929654590852\n",
      "it:  356 \n",
      "Loss:  1.0552725034117407\n",
      "it:  357 \n",
      "Loss:  1.0528638363131453\n",
      "it:  358 \n",
      "Loss:  1.0504669209222905\n",
      "it:  359 \n",
      "Loss:  1.0480817140974659\n",
      "it:  360 \n",
      "Loss:  1.045708172795851\n",
      "it:  361 \n",
      "Loss:  1.0433462540731138\n",
      "it:  362 \n",
      "Loss:  1.0409959150830146\n",
      "it:  363 \n",
      "Loss:  1.0386571130770166\n",
      "it:  364 \n",
      "Loss:  1.036329805403901\n",
      "it:  365 \n",
      "Loss:  1.0340139495093876\n",
      "it:  366 \n",
      "Loss:  1.031709502935763\n",
      "it:  367 \n",
      "Loss:  1.0294164233215113\n",
      "it:  368 \n",
      "Loss:  1.0271346684009548\n",
      "it:  369 \n",
      "Loss:  1.024864196003896\n",
      "it:  370 \n",
      "Loss:  1.0226049640552684\n",
      "it:  371 \n",
      "Loss:  1.020356930574791\n",
      "it:  372 \n",
      "Loss:  1.0181200536766306\n",
      "it:  373 \n",
      "Loss:  1.0158942915690665\n",
      "it:  374 \n",
      "Loss:  1.0136796025541654\n",
      "it:  375 \n",
      "Loss:  1.0114759450274577\n",
      "it:  376 \n",
      "Loss:  1.0092832774776204\n",
      "it:  377 \n",
      "Loss:  1.007101558486169\n",
      "it:  378 \n",
      "Loss:  1.00493074672715\n",
      "it:  379 \n",
      "Loss:  1.002770800966843\n",
      "it:  380 \n",
      "Loss:  1.000621680063466\n",
      "it:  381 \n",
      "Loss:  0.9984833429668865\n",
      "it:  382 \n",
      "Loss:  0.9963557487183404\n",
      "it:  383 \n",
      "Loss:  0.9942388564501544\n",
      "it:  384 \n",
      "Loss:  0.9921326253854731\n",
      "it:  385 \n",
      "Loss:  0.9900370148379947\n",
      "it:  386 \n",
      "Loss:  0.9879519842117082\n",
      "it:  387 \n",
      "Loss:  0.9858774930006415\n",
      "it:  388 \n",
      "Loss:  0.9838135007886092\n",
      "it:  389 \n",
      "Loss:  0.9817599672489693\n",
      "it:  390 \n",
      "Loss:  0.9797168521443851\n",
      "it:  391 \n",
      "Loss:  0.9776841153265909\n",
      "it:  392 \n",
      "Loss:  0.9756617167361648\n",
      "it:  393 \n",
      "Loss:  0.9736496164023063\n",
      "it:  394 \n",
      "Loss:  0.9716477744426184\n",
      "it:  395 \n",
      "Loss:  0.9696561510628973\n",
      "it:  396 \n",
      "Loss:  0.9676747065569241\n",
      "it:  397 \n",
      "Loss:  0.9657034013062656\n",
      "it:  398 \n",
      "Loss:  0.9637421957800765\n",
      "it:  399 \n",
      "Loss:  0.9617910505349094\n",
      "it:  400 \n",
      "Loss:  0.9598499262145298\n",
      "it:  401 \n",
      "Loss:  0.9579187835497355\n",
      "it:  402 \n",
      "Loss:  0.9559975833581811\n",
      "it:  403 \n",
      "Loss:  0.9540862865442078\n",
      "it:  404 \n",
      "Loss:  0.9521848540986796\n",
      "it:  405 \n",
      "Loss:  0.9502932470988216\n",
      "it:  406 \n",
      "Loss:  0.9484114267080673\n",
      "it:  407 \n",
      "Loss:  0.9465393541759066\n",
      "it:  408 \n",
      "Loss:  0.9446769908377423\n",
      "it:  409 \n",
      "Loss:  0.9428242981147493\n",
      "it:  410 \n",
      "Loss:  0.9409812375137406\n",
      "it:  411 \n",
      "Loss:  0.9391477706270356\n",
      "it:  412 \n",
      "Loss:  0.9373238591323352\n",
      "it:  413 \n",
      "Loss:  0.935509464792602\n",
      "it:  414 \n",
      "Loss:  0.9337045494559439\n",
      "it:  415 \n",
      "Loss:  0.931909075055502\n",
      "it:  416 \n",
      "Loss:  0.9301230036093457\n",
      "it:  417 \n",
      "Loss:  0.9283462972203698\n",
      "it:  418 \n",
      "Loss:  0.926578918076197\n",
      "it:  419 \n",
      "Loss:  0.9248208284490868\n",
      "it:  420 \n",
      "Loss:  0.9230719906958447\n",
      "it:  421 \n",
      "Loss:  0.9213323672577415\n",
      "it:  422 \n",
      "Loss:  0.9196019206604308\n",
      "it:  423 \n",
      "Loss:  0.9178806135138767\n",
      "it:  424 \n",
      "Loss:  0.9161684085122822\n",
      "it:  425 \n",
      "Loss:  0.9144652684340224\n",
      "it:  426 \n",
      "Loss:  0.9127711561415834\n",
      "it:  427 \n",
      "Loss:  0.9110860345815036\n",
      "it:  428 \n",
      "Loss:  0.9094098667843202\n",
      "it:  429 \n",
      "Loss:  0.9077426158645205\n",
      "it:  430 \n",
      "Loss:  0.906084245020495\n",
      "it:  431 \n",
      "Loss:  0.9044347175344972\n",
      "it:  432 \n",
      "Loss:  0.902793996772605\n",
      "it:  433 \n",
      "Loss:  0.9011620461846875\n",
      "it:  434 \n",
      "Loss:  0.8995388293043758\n",
      "it:  435 \n",
      "Loss:  0.8979243097490358\n",
      "it:  436 \n",
      "Loss:  0.8963184512197477\n",
      "it:  437 \n",
      "Loss:  0.8947212175012861\n",
      "it:  438 \n",
      "Loss:  0.8931325724621056\n",
      "it:  439 \n",
      "Loss:  0.8915524800543305\n",
      "it:  440 \n",
      "Loss:  0.8899809043137453\n",
      "it:  441 \n",
      "Loss:  0.888417809359793\n",
      "it:  442 \n",
      "Loss:  0.8868631593955721\n",
      "it:  443 \n",
      "Loss:  0.8853169187078415\n",
      "it:  444 \n",
      "Loss:  0.8837790516670249\n",
      "it:  445 \n",
      "Loss:  0.8822495227272209\n",
      "it:  446 \n",
      "Loss:  0.8807282964262156\n",
      "it:  447 \n",
      "Loss:  0.8792153373854986\n",
      "it:  448 \n",
      "Loss:  0.8777106103102812\n",
      "it:  449 \n",
      "Loss:  0.8762140799895191\n",
      "it:  450 \n",
      "Loss:  0.8747257112959373\n",
      "it:  451 \n",
      "Loss:  0.8732454691860575\n",
      "it:  452 \n",
      "Loss:  0.8717733187002297\n",
      "it:  453 \n",
      "Loss:  0.8703092249626655\n",
      "it:  454 \n",
      "Loss:  0.8688531531814744\n",
      "it:  455 \n",
      "Loss:  0.8674050686487039\n",
      "it:  456 \n",
      "Loss:  0.86596493674038\n",
      "it:  457 \n",
      "Loss:  0.864532722916553\n",
      "it:  458 \n",
      "Loss:  0.8631083927213432\n",
      "it:  459 \n",
      "Loss:  0.8616919117829919\n",
      "it:  460 \n",
      "Loss:  0.8602832458139121\n",
      "it:  461 \n",
      "Loss:  0.8588823606107436\n",
      "it:  462 \n",
      "Loss:  0.8574892220544098\n",
      "it:  463 \n",
      "Loss:  0.8561037961101748\n",
      "it:  464 \n",
      "Loss:  0.854726048827708\n",
      "it:  465 \n",
      "Loss:  0.8533559463411431\n",
      "it:  466 \n",
      "Loss:  0.8519934548691467\n",
      "it:  467 \n",
      "Loss:  0.8506385407149843\n",
      "it:  468 \n",
      "Loss:  0.8492911702665896\n",
      "it:  469 \n",
      "Loss:  0.8479513099966361\n",
      "it:  470 \n",
      "Loss:  0.846618926462609\n",
      "it:  471 \n",
      "Loss:  0.8452939863068816\n",
      "it:  472 \n",
      "Loss:  0.8439764562567906\n",
      "it:  473 \n",
      "Loss:  0.842666303124715\n",
      "it:  474 \n",
      "Loss:  0.8413634938081545\n",
      "it:  475 \n",
      "Loss:  0.8400679952898132\n",
      "it:  476 \n",
      "Loss:  0.8387797746376797\n",
      "it:  477 \n",
      "Loss:  0.8374987990051133\n",
      "it:  478 \n",
      "Loss:  0.8362250356309283\n",
      "it:  479 \n",
      "Loss:  0.8349584518394819\n",
      "it:  480 \n",
      "Loss:  0.8336990150407604\n",
      "it:  481 \n",
      "Loss:  0.8324466927304709\n",
      "it:  482 \n",
      "Loss:  0.8312014524901296\n",
      "it:  483 \n",
      "Loss:  0.829963261987154\n",
      "it:  484 \n",
      "Loss:  0.828732088974956\n",
      "it:  485 \n",
      "Loss:  0.8275079012930342\n",
      "it:  486 \n",
      "Loss:  0.8262906668670682\n",
      "it:  487 \n",
      "Loss:  0.8250803537090141\n",
      "it:  488 \n",
      "Loss:  0.8238769299172001\n",
      "it:  489 \n",
      "Loss:  0.8226803636764232\n",
      "it:  490 \n",
      "Loss:  0.8214906232580456\n",
      "it:  491 \n",
      "Loss:  0.8203076770200936\n",
      "it:  492 \n",
      "Loss:  0.8191314934073552\n",
      "it:  493 \n",
      "Loss:  0.8179620409514785\n",
      "it:  494 \n",
      "Loss:  0.816799288271072\n",
      "it:  495 \n",
      "Loss:  0.8156432040718022\n",
      "it:  496 \n",
      "Loss:  0.8144937571464957\n",
      "it:  497 \n",
      "Loss:  0.8133509163752377\n",
      "it:  498 \n",
      "Loss:  0.8122146507254727\n",
      "it:  499 \n",
      "Loss:  0.811084929252105\n",
      "it:  500 \n",
      "Loss:  0.8099617210975988\n",
      "it:  501 \n",
      "Loss:  0.8088449954920792\n",
      "it:  502 \n",
      "Loss:  0.8077347217534326\n",
      "it:  503 \n",
      "Loss:  0.8066308692874061\n",
      "it:  504 \n",
      "Loss:  0.8055334075877094\n",
      "it:  505 \n",
      "Loss:  0.8044423062361133\n",
      "it:  506 \n",
      "Loss:  0.8033575349025508\n",
      "it:  507 \n",
      "Loss:  0.8022790633452154\n",
      "it:  508 \n",
      "Loss:  0.8012068614106608\n",
      "it:  509 \n",
      "Loss:  0.8001408990339006\n",
      "it:  510 \n",
      "Loss:  0.7990811462385042\n",
      "it:  511 \n",
      "Loss:  0.7980275731366979\n",
      "it:  512 \n",
      "Loss:  0.7969801499294592\n",
      "it:  513 \n",
      "Loss:  0.7959388469066146\n",
      "it:  514 \n",
      "Loss:  0.7949036344469363\n",
      "it:  515 \n",
      "Loss:  0.7938744830182369\n",
      "it:  516 \n",
      "Loss:  0.7928513631774632\n",
      "it:  517 \n",
      "Loss:  0.7918342455707915\n",
      "it:  518 \n",
      "Loss:  0.7908231009337192\n",
      "it:  519 \n",
      "Loss:  0.7898179000911582\n",
      "it:  520 \n",
      "Loss:  0.7888186139575243\n",
      "it:  521 \n",
      "Loss:  0.7878252135368288\n",
      "it:  522 \n",
      "Loss:  0.7868376699227668\n",
      "it:  523 \n",
      "Loss:  0.7858559542988065\n",
      "it:  524 \n",
      "Loss:  0.7848800379382737\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "linRegModel = MyLinReg(0.0001)\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 0].reshape(-1, 1) # transponerer X?\n",
    "print(X)\n",
    "y = iris.data[:, 1]\n",
    "\n",
    "linRegModel.fit(X, y, \"GD\")\n",
    "\n",
    "predictions = linRegModel.predict(X)\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alt under Qc her:\n",
    "### Qd: The Journaling of Your Regressor \n",
    "\n",
    "For the journal, write a full explanation of how you implemented the linear regressor, including a code walk-through (or mini-review of the most interesting parts).\n",
    "\n",
    "Vi har implementeret en linear regressor class “MyLinReg”, og med mulighed for at bruge en Gradient Descent eller Stochastic Gradient Descent solver, som afgør hvordan loss funktionen bliver minimeret. I vores fit-predict interface starter vi med at sætte variablerne fra constructeren på klassen selv, inden vi kalder vores solver metode (som enten er GD eller SGD).\n",
    "\n",
    "Solver metoden tager nogle iterationer, og mellem hver iteration opdaterer vi vores vægte (vores w, self.coef) ved at bruge vores loss funktion og gradienten af vores loss funktion. Vi bruger vores loss funktion til at evaluere vores model, og tjekker loss hver gang for at se om den bliver bedre hver iteration (hvor første gang er loss super høj for at barren for improvement er meget lav). Hvis loss ikke bliver bedre, så stopper vi vores iterationer efter efter den har kørt \"max_iter\" iterationer.\n",
    "\n",
    "Det her er alt sammen til for at prøve at minimere loss. Vi har også implementeret en R^2 score funktion, som vi bruger til at evaluere vores model. Vi har også gjort så man kan sætte en constant learning rate i form af parameteren eta0, der (sammen med gradient) styrer hvor drastisk vi skal ændre vores vægte.\n",
    "\n",
    "\n",
    "### Qe: Mathematical Foundation for Training a Linear Regressor\n",
    "\n",
    "You must also include the theoretical mathematical foundation for the linear regressor using the following equations and graphs (free to include in your journal without cite/reference), and relate them directly to your code:\n",
    "\n",
    "* Design matrix of size $(n, d)$ where each row is an input column vector $(\\mathbf{x}^{(i)})^\\top$ data sample of size $d$\n",
    "\n",
    "$$\n",
    "    \\def\\rem#1{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands v01, remember: no newlines in defs}\n",
    "    \\rem{MACRO eq: equation <#1:lhs> <#2:rhs>}\n",
    "    \\def\\eq#1#2{#1 &=& #2\\\\}\n",
    "    \\rem{MACRO arr: array <#1:columns (lcr..)> <#2:content>}\n",
    "    \\def\\ar#1#2{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\rem{MACRO ac: array column vector <#1:columns (lcr..)> <#2:content>}\n",
    "    \\def\\ac#1#2{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\rem{MACRO st: subscript text <#1:content>}\n",
    "    \\def\\st#1{_{\\textrm{#1}}}\n",
    "    \\rem{MACRO norm: norm caligari L <#1:content>}\n",
    "    \\def\\norm#1{{\\cal L}_{#1}}\n",
    "    \\rem{MACRO obs: ??}\n",
    "    \\def\\obs#1#2{#1_{\\textrm{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\rem{MACRO diff: math differetial operator <#1:content>}\n",
    "    \\def\\diff#1{\\mathrm{d}#1} \n",
    "    \\rem{MACRO half: shorthand for 1/2}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\rem{MACRO pfrac: partial fraction <#1:numenator> <#2:denumenator>}\n",
    "    \\def\\pfrac#1#2{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\rem{MACRO dfrac: differetial operator fraction <#1:numenator> <#2:denumenator>}\n",
    "    \\def\\dfrac#1#2{\\frac{\\mathrm{d}~#1}{\\mathrm{d}#2}}\n",
    "    \\rem{MACRO pown: power and parantesis (train/test..) <#1:content>}\n",
    "    \\def\\pown#1{^{(#1)}}\n",
    "    \\rem{MACROS powi, pown: shorthands for power (i) and (n)}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\rem{MACROS powtest, powertrain: power (test) and (train)}\n",
    "    \\def\\powtest{\\pown{\\textrm{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\textrm{\\scriptsize train}}}\n",
    "    \\rem{MACRO boldmatrix: bold matix/vector notation} \n",
    "    \\def\\boldmatrix#1{\\mathbf{#1}} \n",
    "    \\rem{MACROS X,Z,x,y,w: bold X,Z,x etc.} \n",
    "    \\def\\bX{\\boldmatrix{X}}\n",
    "    \\def\\bZ{\\boldmatrix{Z}}\n",
    "    \\def\\bx{\\boldmatrix{x}}\n",
    "    \\def\\by{\\boldmatrix{y}}\n",
    "    \\def\\bw{\\boldmatrix{w}}\n",
    "    \\def\\bz{\\boldmatrix{z}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\rem{MACROS stpred, sttrue: shorthand for subscript 'pred' and 'true'}\n",
    "    \\def\\stpred{\\st{pred}~}\n",
    "    \\def\\sttrue{\\st{true}~}\n",
    "    \\rem{MACROS ypred, ytrue:   shorthand for scalar y 'pred' and 'true'}\n",
    "    \\def\\ytrue{y\\sttrue}\n",
    "    \\def\\ypred{y\\stpred} \n",
    "    \\rem{MACROS bypred, bytrue: shorthand for vecor y 'pred' and 'true'} \n",
    "    \\def\\bypred{\\boldmatrix{y}\\stpred}\n",
    "    \\def\\bytrue{\\boldmatrix{y}\\sttrue} \n",
    "\\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2} \\\\\n",
    "            \\vdots      &             &        & \\vdots      \\\\\n",
    "            x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn   \\\\\n",
    "        } \n",
    "$$\n",
    "\n",
    "* Target ground-truth column vector of size $n$\n",
    "\n",
    "$$\n",
    "\\bytrue =\n",
    "  \\ac{c}{\n",
    "     y\\pown{1}\\sttrue \\\\\n",
    "     y\\pown{2}\\sttrue \\\\\n",
    "     \\vdots           \\\\\n",
    "     y\\pown{n}\\sttrue \\\\\n",
    "  } \n",
    "$$\n",
    "\n",
    "* Bias factor, and by convention in the following (prepend one)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\ac{c}{1\\\\\\bx\\powni} & \\mapsto \\bx\\powni\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Weight column vector of size $d+1$ (i.e. with bias or intercept element $w_0$ prepended)\n",
    "\n",
    "$$\n",
    "\\bw =\n",
    "    \\ac{c}{\n",
    "         w_0    \\\\\n",
    "         w_1    \\\\\n",
    "         w_2    \\\\\n",
    "         \\vdots \\\\\n",
    "         w_d    \\\\\n",
    "    }\n",
    "$$\n",
    "\n",
    "* Linear regression model hypothesis function for a column vector input $\\bx\\powni$ of size $d$ and a column weight vector $\\bw$ of size $d+1$\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  ~~~~~~~~~~~~~~~\n",
    "  h(\\bx\\powni;\\bw) &= \\ypred\\powni \\\\\n",
    "                   &= \\bw^\\top \\bx\\powni ~~~~ (\\bx\\powni~\\textrm{with bias element})\\\\ \n",
    "                   &= w_0  \\cdot 1+ w_1 x_1\\powni + w_2 x_2\\powni + \\cdots + w_d x_d\\powni & \\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Individual losses based on the $\\norm{2}^2$ (last part assuming one dimensional output)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  L\\powni &= || \\ypred \\powni         - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= || h(\\bx\\powni;\\bw)      - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= || \\bw^\\top\\bx\\powni     - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= \\left( \\bw^\\top\\bx\\powni - \\ytrue\\powni~ \\right)^2 ~~~~~ \\textrm{(only for 1D output)}\n",
    "}\n",
    "$$\n",
    "\n",
    "* MSE loss function\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "    \\textrm{MSE}(\\bX,\\bytrue;\\bw)  &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                                   &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\powni\\sttrue \\right)^2\\\\\n",
    "                                   &= \\frac{1}{n} ||\\bX \\bw - \\bytrue||_2^2\n",
    "}\n",
    "$$                   \n",
    "\n",
    "\n",
    "* Loss function, proportional to (R)MSE\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "   J &= \\frac{1}{2} ||\\bX \\bw - \\bytrue||_2^2\\\\\n",
    "     &  \\propto \\textrm{MSE}\n",
    "}\n",
    "$$\n",
    "\n",
    "* Training: computing the optimal value of the $\\bw$ weight; that is finding the $\\bw$-value that minimizes the total loss\n",
    "\n",
    "$$\n",
    "  \\bw^* = \\textrm{argmin}_\\bw~J\\\\\n",
    "$$\n",
    "\n",
    "* Visualization of $\\textrm{argmin}_\\bw$ means to the argument of $\\bw$ that minimizes the $J$ function. The minimization can in 2-D visually be drawn as finding the lowest $J$ that for linear regression always forms a convex shape \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">\n",
    "\n",
    "#### Training I: The Closed-form Solution\n",
    "\n",
    "* Finding the optimal weight in a _one-step_ analytic expression \n",
    "\n",
    "$$\n",
    "  \\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1}~ \\bX^\\top \\bytrue\n",
    "$$\n",
    "\n",
    "\n",
    "#### Training II: Numerical Optimization \n",
    "\n",
    "* The Gradient of the loss function\n",
    "\n",
    "$$   \n",
    "  \\nabla_\\bw~J = \\left[ \\frac{\\partial J}{\\partial w_1} ~~~~ \\frac{\\partial J}{\\partial w_2} ~~~~ \\ldots  ~~~~ \\frac{\\partial J}{\\partial w_d} \\right]^\\top\n",
    "$$\n",
    "\n",
    "* The Gradient for the based $J$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J &= \\frac{2}{n} \\bX^\\top \\left( \\bX \\bw - \\bytrue \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "* The Gradient Decent Algorithm (GD)\n",
    "\n",
    "$$ \n",
    "  \\bw^{(step~N+1)}~ = \\bw^{(step~N)} ~ - \\eta \\nabla_{\\bw} J\n",
    "$$\n",
    "\n",
    "* Visualization of GD, showing $J$ as a function of two $w$-dimensions\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization_gd.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qf: Smoke testing\n",
    "\n",
    "Once ready, you can test your regressor via the test stub below, or create your own _test suite_.\n",
    "\n",
    "Be aware that setting the stepsize, $\\eta$, value can be tricky, and you might want to tune `eta0` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN:  This mini smoke-test may produce false-positives and/or\n",
      "       false-negatives..\n",
      "\n",
      "it:  1 \n",
      "Loss:  5.573421534393207\n",
      "it:  2 \n",
      "Loss:  5.573421534393207\n",
      "it:  3 \n",
      "Loss:  5.573421534393207\n",
      "it:  4 \n",
      "Loss:  5.573421534393207\n",
      "it:  5 \n",
      "Loss:  5.573421534393207\n",
      "it:  6 \n",
      "Loss:  5.573421534393207\n",
      "INFO:  y_pred = [0. 0. 0. 0.]\n",
      "\n",
      "INFO:  SCORE = -15.357515037714645\n",
      "\n",
      "INFO:  bias         = 0.0\n",
      "\n",
      "INFO:  coefficients = [0.]\n",
      "\n",
      "       w         = [0. 0.]\n",
      "       w_expected= [4.04687901 1.88012149]\n",
      "\n",
      "ERROR: mini-smoketest on your regressor failed\n",
      "\n",
      "       EXCEPTION: \n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 258\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    257\u001b[0m Warn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis mini smoke-test may produce false-positives and/or\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m false-negatives..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 258\u001b[0m TestMyLinReg()\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[57], line 253\u001b[0m, in \u001b[0;36mTestMyLinReg\u001b[1;34m()\u001b[0m\n\u001b[0;32m    251\u001b[0m         Warn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot test due to missing w information\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m--> 253\u001b[0m     Err(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmini-smoketest on your regressor failed\u001b[39m\u001b[38;5;124m\"\u001b[39m, ex)\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[57], line 114\u001b[0m, in \u001b[0;36mErr\u001b[1;34m(msg, ex)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mErr\u001b[39m(msg, ex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    113\u001b[0m     PrintOutput(msg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mERROR: \u001b[39m\u001b[38;5;124m\"\u001b[39m, ex, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlred\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(msg) \u001b[38;5;28;01mif\u001b[39;00m ex \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ex\n",
      "Cell \u001b[1;32mIn[57], line 246\u001b[0m, in \u001b[0;36mTestMyLinReg\u001b[1;34m()\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m    245\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1E-2\u001b[39m \u001b[38;5;66;03m# somewhat big epsilon, allowing some slack..\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m AssertInRange(w, w_expected, eps)\n\u001b[0;32m    247\u001b[0m Info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWell, good news, your w and the expected w-vector seem to be very close numerically, so the smoke-test has passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m regressor\n",
      "Cell \u001b[1;32mIn[57], line 127\u001b[0m, in \u001b[0;36mSimpleAssertInRange\u001b[1;34m(x, expected, eps)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mSimpleAssertInRange\u001b[39m(x, expected, eps):\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m#assert isinstance(x, numpy.ndarray)\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m#assert isinstance(expected, numpy.ndarray)\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m#assert x.ndim==1 and expected.ndim==1\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m#assert x.shape==expected.shape\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m eps\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m numpy\u001b[38;5;241m.\u001b[39mallclose(x, expected, eps)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Mini smoke test for your linear regressor: TestMyLinReg\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "\n",
    "### SOME NIFTY HELPER FUNS ###\n",
    "\n",
    "def isVector(y, expected_n=-1):\n",
    "    assert isinstance(y, numpy.ndarray), f\"expected type 'numpy.array' but got {type(y)}\"\n",
    "    assert y.ndim==1, f\"expected y.ndim==1 but got {y.ndim}\"\n",
    "    assert expected_n<0 or expected_n==y.shape[0], f\"expected vector of size {expected_n} but got size {y.shape}\"\n",
    "    return True\n",
    "\n",
    "def isMatrix(X, expected_m=-1, expected_n=-1):\n",
    "    assert isinstance(X, numpy.ndarray), f\"expected type 'numpy.array' but got {type(X)}\"\n",
    "    assert X.ndim==2, f\"expected X.ndim==2 but got {X.ndim}\"\n",
    "    assert expected_m<0 or expected_m==y.shape[0], f\"expected matrix of size {expected_m}x{expected_n} but got size {X.shape}\"\n",
    "    assert expected_n<0 or expected_n==y.shape[1], f\"expected vector of size {expected_m}x{expected_n} but got size {X.shape}\"\n",
    "    return True\n",
    "\n",
    "def PrintMatrix(x, label=\"\", precision=12, linewidth=60):\n",
    "    hasFancy = False\n",
    "    # try:\n",
    "    #     # NOTE: how does multiple import behave, any performance issues?\n",
    "    #     from libitmal.utils import PrintMatrix as FancyPrintMatrix\n",
    "    #     hasFancy = True\n",
    "    # except Exception as ex:\n",
    "    #     Warn(\"could not import PrintMatrix from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    if hasFancy:\n",
    "        FancyPrintMatrix(x, label=label, precision=precision, linewidth=linewidth)\n",
    "    else:\n",
    "        # default simple implementation\n",
    "        print(f\"{label}{' ' if len(label)>0 else ''}{x}\")\n",
    "\n",
    "def Col(color):\n",
    "    hasFancy = False\n",
    "    # try:\n",
    "    #     from libitmal.Utils.colors import Col as FancyCol\n",
    "    #     hasFancy = True\n",
    "    # except Exception as ex:\n",
    "    #     Warn(\"could not import Col from libitmal.Utils.colors, defaulting to simple function..\")\n",
    "\n",
    "    if hasFancy:\n",
    "        return FancyCol(color)\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def ColEnd():\n",
    "    hasFancy = False\n",
    "    # try:\n",
    "    #     from libitmal.Utils.colors import ColEnd as FancyColEnd\n",
    "    #     hasFancy = True\n",
    "    # except Exception as ex:\n",
    "    #     Warn(\"could not import Col from libitmal.Utils.colors, defaulting to simple function..\")\n",
    "\n",
    "    if hasFancy:\n",
    "        return FancyColEnd()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def PrintOutput(msg, pre_msg, ex=None, color=\"\", filestream=sys.stdout):\n",
    "\n",
    "    def FormatTxt(txt, linewidth=60, prefix=\"\", replacetabs=True):\n",
    "        assert isinstance(txt, str)\n",
    "        assert isinstance(linewidth, int) and linewidth > 0\n",
    "        assert isinstance(prefix, str)\n",
    "\n",
    "        if replacetabs:\n",
    "            txt = txt.replace(\"\\t\",\"    \")\n",
    "\n",
    "        r = \"\"\n",
    "        n = 0\n",
    "        m = 0\n",
    "        for i in txt:\n",
    "            m += 1\n",
    "            if n >= linewidth:\n",
    "                if not i.isspace() and m < len(txt) and not txt[m].isspace():\n",
    "                    r += \"\\\\\" # add hypen\n",
    "                r += \"\\n\" + prefix\n",
    "                n = 0\n",
    "\n",
    "            if n == 0 and i.isspace():\n",
    "                continue # skip leading space\n",
    "\n",
    "            r += i\n",
    "            n += 1\n",
    "\n",
    "            if i == \"\\n\":\n",
    "                r += prefix\n",
    "                n = 0\n",
    "\n",
    "        return r\n",
    "\n",
    "    col_beg = Col(color)\n",
    "    col_end = ColEnd()\n",
    "\n",
    "    prefix = \"\".ljust(len(pre_msg)) \n",
    "    msg = FormatTxt(msg, prefix=prefix)\n",
    "    \n",
    "    print(f\"{col_beg}{pre_msg}{msg}{col_end}\\n\", file=filestream)\n",
    "\n",
    "    if ex is not None:\n",
    "        #msg += f\"\\n   EXCEPTION: {ex} ({type(ex)})\"\n",
    "        PrintOutput(str(ex), prefix + \"EXCEPTION: \", None, \"red\", filestream)\n",
    "\n",
    "\n",
    "def Warn(msg, ex=None):\n",
    "    PrintOutput(msg, \"WARN:  \", ex, \"lyellow\")\n",
    "\n",
    "\n",
    "def Err(msg, ex=None):\n",
    "    PrintOutput(msg, \"ERROR: \", ex, \"lred\" )\n",
    "    raise Exception(msg) if ex is None else ex\n",
    "\n",
    "\n",
    "def Info(msg):\n",
    "    PrintOutput(msg, \"INFO:  \", None, \"lpurple\")\n",
    "\n",
    "\n",
    "def SimpleAssertInRange(x, expected, eps):\n",
    "    #assert isinstance(x, numpy.ndarray)\n",
    "    #assert isinstance(expected, numpy.ndarray)\n",
    "    #assert x.ndim==1 and expected.ndim==1\n",
    "    #assert x.shape==expected.shape\n",
    "    assert eps>0\n",
    "    assert numpy.allclose(x, expected, eps) # should rtol or atol be set to eps?\n",
    "\n",
    "\n",
    "def GenerateData():\n",
    "    X = numpy.array([[8.34044009e-01],[1.44064899e+00],[2.28749635e-04],[6.04665145e-01]])\n",
    "    y = numpy.array([5.97396028, 7.24897834, 4.86609388, 3.51245674])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def TestMyLinReg():\n",
    "    X, y = GenerateData()\n",
    "\n",
    "    try:\n",
    "        # assume that your regressor class is named 'MyLinReg', please update/change\n",
    "        regressor = MyLinReg()\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor has another name, than 'MyLinReg', please change the name in this smoke test\", ex)\n",
    "\n",
    "    try:\n",
    "        regressor = MyLinReg(max_iter=200)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ for parameter 'max_iter'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(eta0=0.01)\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ for parameter 'eta0'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(verbose=False)\n",
    "    except Exception as ex:\n",
    "        Warn(\"your regressor can not be constructed via the __init_ for parameter 'verbose'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(tol=1e-3)\n",
    "    except Exception as ex:\n",
    "        Warn(\"your regressor can not be constructed via the __init_ for parameter 'tol'\", ex)\n",
    "    try:\n",
    "        regressor = MyLinReg(n_iter_no_change=1e-3)\n",
    "    except Exception as ex:\n",
    "        Warn(\"your regressor can not be constructed via the __init_ for parameter 'n_iter_no_change'\", ex)\n",
    "\n",
    "    # create regressor with default hyperparameter values\n",
    "    # to be used for training, prediction and score..\n",
    "    try:\n",
    "        regressor = MyLinReg()\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not be constructed via the __init_ with default parameters\", ex)\n",
    "\n",
    "\n",
    "    try:\n",
    "        regressor.fit(X, y, \"sgd\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not fit\", ex)\n",
    "\n",
    "    try:\n",
    "        y_pred = regressor.predict(X)\n",
    "        Info(f\"y_pred = {y_pred}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor can not predict\", ex)\n",
    "\n",
    "\n",
    "    try:\n",
    "        score  = regressor.score(X, y)\n",
    "        Info(f\"SCORE = {Col('lblue')}{score}{ColEnd()}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor fails in the score call\", ex)\n",
    "\n",
    "\n",
    "    try:\n",
    "        w    = None # default\n",
    "        bias = None # default\n",
    "        try:\n",
    "            w = regressor.coef\n",
    "            bias = regressor.intercept\n",
    "        except Exception as ex:\n",
    "            w = None\n",
    "            Warn(\"your regressor has no coef_/intercept_ atrributes, trying Weights() instead..\", ex)\n",
    "        try:\n",
    "            if w is None:\n",
    "                w = regressor.Weights() # maybe a Weigths function is avalible on you model?\n",
    "                try:\n",
    "                    assert w.ndim == 1,     \"can only handle vector like w's for now\"\n",
    "                    assert w.shape[0] >= 2, \"expected length of to be at least 2, that is one bias one coefficient\"\n",
    "                    bias = w[0]\n",
    "                    w = w[1:]\n",
    "                except Exception as ex:\n",
    "                    w = None\n",
    "                    Err(\"having a hard time concantenating our bias and coefficients, giving up!\", ex)\n",
    "        except Exception as ex:\n",
    "            w = None\n",
    "            Err(\"your regressor also has no Weights() function, giving up!\", ex)\n",
    "        Info(f\"bias         = {bias}\")\n",
    "        Info(f\"coefficients = {w}\")\n",
    "    except Exception as ex:\n",
    "        Err(\"your regressor fails during extraction of bias and weights (but is a COULD)\", ex)\n",
    "\n",
    "    # try:\n",
    "    #     from libitmal.utils import PrintMatrix\n",
    "    # except Exception as ex:\n",
    "    #     PrintMatrix = SimplePrintMatrix # fall-back\n",
    "    #     Warn(\"could not import PrintMatrix from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    try:\n",
    "        from libitmal.utils import AssertInRange\n",
    "    except Exception as ex:\n",
    "        AssertInRange = SimpleAssertInRange # fall-back\n",
    "        # Warn(\"could not import AssertInRange from libitmal.utils, defaulting to simple function..\")\n",
    "\n",
    "    try:\n",
    "        if w is not None:\n",
    "            if bias is not None:\n",
    "                w = numpy.concatenate(([bias], w)) # re-concat bias an coefficients, may be incorrect for your implementation!\n",
    "            \n",
    "            # TEST VECTOR:\n",
    "            w_expected = numpy.array([4.046879011698, 1.880121487278])\n",
    "            \n",
    "            PrintMatrix(w,          label=\"       w         =\")\n",
    "            PrintMatrix(w_expected, label=\"       w_expected=\")\n",
    "            print()\n",
    "            \n",
    "            eps = 1E-2 # somewhat big epsilon, allowing some slack..\n",
    "            AssertInRange(w, w_expected, eps) #### SPØRG Carsten hvad der gør at vores ikke er god nok\n",
    "            Info(\"Well, good news, your w and the expected w-vector seem to be very close numerically, so the smoke-test has passed!\")\n",
    "            \n",
    "            return regressor\n",
    "        else:\n",
    "            Warn(\"cannot test due to missing w information\")\n",
    "    except Exception as ex:\n",
    "        Err(\"mini-smoketest on your regressor failed\", ex)\n",
    "    \n",
    "    return None\n",
    "\n",
    "Warn(\"This mini smoke-test may produce false-positives and/or\\n false-negatives..\")\n",
    "TestMyLinReg()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qg: [OPTIONAL] More Smoke-Testing\n",
    "\n",
    "Do you dare to compare your custom regressor with the SGD regressor in Scikit-learn on both the IRIS and MNIST datasets?\n",
    "\n",
    "Then run the next smoke-test function, but the code might requre `eta0` anb `max_iter` hyperparamter tuning).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model    import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.pipeline        import Pipeline\n",
    "\n",
    "try:\n",
    "    from libitmal import dataloaders\n",
    "except Exception as ex:\n",
    "    Err(\"can not import dataloaders form libitmal, and then I can not run the TestAndCompareRegressors smoke-test, sorry!\", ex)\n",
    "\n",
    "def TestAndCompareRegressors():\n",
    "    for f in [(\"IRIS\",  dataloaders.IRIS_GetDataSet,  1E-2),\n",
    "              (\"MNIST\", dataloaders.MNIST_GetDataSet, 1E-3)]:\n",
    "        \n",
    "        # NOTE: f-tuble is (<name>, <data-loader-function-pointer>, <eps0>)\n",
    "        data = f[1]() # returns (X, y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data[0], data[1])\n",
    "        \n",
    "        Info(f\"DATA: '{f[0]}'\\n\\tSHAPES: X_train={X_train.shape}, X_test={X_test.shape}, y_train={y_train.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "        eta0 = f[2] # an adaptive learning rate is really needed here!\n",
    "        regressor0 = MyLinReg(eta0=eta0, max_iter=1000)\n",
    "        regressor1 = SGDRegressor()    \n",
    "\n",
    "        for r in [(\"MyLinReg\", regressor0), (\"SGDRegressor\", regressor1)]:\n",
    "            Info(f\"\\nTRAINING['{r[0]}']..\")\n",
    "            \n",
    "            pipe = Pipeline([('scaler', StandardScaler()), r])\n",
    "            pipe.fit(X_train, y_train)\n",
    "            \n",
    "            y_pred_test = pipe.predict(X_test)\n",
    "            \n",
    "            PrintMatrix(y_pred_test, label=\"y_pred_test=\", precision=4)\n",
    "            print()\n",
    "            \n",
    "            r2 = pipe.score(X_test, y_test)\n",
    "            Info(f\"SCORE['{r[0]}'] = {Col('lblue')}{r2:0.3f}{ColEnd()}\")\n",
    "            \n",
    "        Info(\"\\n##############################################\\n\")\n",
    "\n",
    "# somewhat more verbose testing, you regressor will likely fail on MNIST \n",
    "# or at least be very, very slow...\n",
    "TestAndCompareRegressors()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qh Conclusion\n",
    "\n",
    "As always, take some time to fine-tune your regressor, perhaps just some code-refactoring, cleaning out 'bad' code, and summarize all your findings\n",
    " above. \n",
    "\n",
    "In other words, write a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2022-12-22| CEF, initial draft. \n",
    "2023-02-26| CEF, first release.\n",
    "2023-02-28| CEF, fix a few issues related to import from libitmal, added Info and color output.\n",
    "2024-09-19| CEF, major overhaul, change math/text and code snippets.\n",
    "2024-09-25| CEF, final fixes, tests, and proof-reading. Moved early stopping and learning graphs to a later excercise.\n",
    "2024-10-04| CEF, clarified Qa with respect to what-is-to-be implemented and what-is-to-be described in text only."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
