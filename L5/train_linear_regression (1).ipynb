{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "(In the following you need not present your journal in the Qa+b+c+ etc. order. You could just present the final code with test and comments.)\n",
    "\n",
    "## Training Your Own Linear Regressor\n",
    "\n",
    "Create a linear regressor, with a Scikit-learn compatible fit-predict interface. You should implement every detail of the linear regressor in Python, using whatever libraries, say `numpy`, you want (except a linear regressor itself).\n",
    "\n",
    "Below is a primitive _get-started_ skeleton for your implementation. Keep the class name `MyLinReg`, which is used in the test sequence later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyLinReg():\n",
    "    def __init__(self, eta0=0.05, max_iter=1000, tol=1e-3, n_iter_no_change=50, verbose=True):\n",
    "        self.eta0 = eta0\n",
    "        self.max_iter = max_iter\n",
    "        self.tolerance = tol\n",
    "        self.n_iter_no_change = n_iter_no_change\n",
    "        self.verbose = verbose\n",
    "        self.coef_ = None # vores w # ellers prøv med underscore (coef_)\n",
    "        self.intercept_ = None # ellers prøv med underscore (intercept_)\n",
    "        self.batch_size = 32\n",
    "    \n",
    "    def Weights(self):\n",
    "        return self.coef_\n",
    "    \n",
    "    def Intercept(self):\n",
    "        return self.coef_[0]\n",
    "  \n",
    "    def __str__(self):\n",
    "        return \"MyLinReg.__str__(): hi!\"\n",
    "    \n",
    "    def fit(self, X, y, method=\"SGD\"):\n",
    "        n_samples, n_features = X.shape\n",
    "        X = add_dummy_feature(X) # Augmenter med 1 taller\n",
    "        assert X.shape[0] == y.shape[0], \"X og y skal være lige store\"\n",
    "        self.coef_ =  np.zeros(X.shape[1])\n",
    "        best_loss = np.inf # brug numpy infity her\n",
    "        no_change_counter = 0\n",
    "        loss = None\n",
    "\n",
    "\n",
    "        for ep in range(self.max_iter):\n",
    "            if method == \"GD\":\n",
    "                # GD\n",
    "                y_pred = X @ self.coef_\n",
    "                error = y_pred - y\n",
    "                gradient = 2*(X.T @ error) / len(y) # hvor y_pred er Xw\n",
    "                self.coef_ -= gradient * self.eta0\n",
    "                # GD\n",
    "\n",
    "\n",
    "            elif method == \"SGD\":\n",
    "                # SGD\n",
    "                for i in range(n_samples):\n",
    "                    y_pred = X[i] @ self.coef_\n",
    "                    error = y_pred - y[i]\n",
    "                    gradient = 2 * X[i] * error\n",
    "                    self.coef_ -= gradient * self.eta0\n",
    "                #SGD\n",
    "            loss = np.sqrt(np.mean(((X @ self.coef_) - y) ** 2))\n",
    "            if abs(best_loss - loss) < self.tolerance:\n",
    "                no_change_counter += 1\n",
    "                if no_change_counter >= self.n_iter_no_change:\n",
    "                    break\n",
    "            else:\n",
    "                no_change_counter = 0\n",
    "                best_loss = loss\n",
    "\n",
    "\n",
    "        self.intercept_ = self.coef_[0]\n",
    "        self.coef_ = self.coef_[1:]\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = add_dummy_feature(X)\n",
    "        return X @ np.r_[self.intercept_, self.coef_]\n",
    "\n",
    "    def score(self, X, y_true):\n",
    "        y_pred = self.predict(X)\n",
    "        sum_squares_total = np.sum((y_true - np.mean(y_true))**2)\n",
    "        sum_squares_residual = np.sum((y_true - y_pred)**2)\n",
    "        return 1 - (sum_squares_residual / sum_squares_total) # Scoren\n",
    "\n",
    "regressor = MyLinReg(eta0=0.01, max_iter=1000, tol=1e-6, n_iter_no_change=10, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The TODO list\n",
    "\n",
    "You must investigate and describe all major details for a linear regressor, and implement at least the following concepts (MUST):\n",
    "\n",
    "### Qa: Concepts and Implementations MUSTS\n",
    "\n",
    "* Implement: the `fit-predict` interface, for a one-dimensional output only, \n",
    "* Implement: a $R^2$ score function (re-use existing code or perhaps just inherit it), \n",
    "* Implement: loss function based on (R)MSE,\n",
    "* Implement: setting of the number of iterations and learning rate ($\\eta$) via parameters in the constructor (the signature of your `__init__` must include the named parameters `eta0` and `max_iter`),\n",
    "* (in a later exercise we will also add `tol`, `n_iter_no_change` and `verbose` to the constructor),\n",
    "* Implement: the batch-gradient decent algorithm (GD),\n",
    "* Implement: constant learning rate (maybe also adaptive learning rate if you are brave),\n",
    "* Implement: stochastic gradient descent (SGD),\n",
    "* Describe in text: epochs vs iterations,\n",
    "* Describe in text: compare the numerical optimization with the Closed-form solution.\n",
    "\n",
    "I det her kodeudsnit har vi implementeret et fit-predict interface for en linær regressor. Vi har også implementeret en R^2 score funktion, en loss funktion baseret på RMSE og vi har implementeret en konstant learning rate med eta parametrene. Vi har også implementeret batch-gradient decent algoritmen og stochastic gradient descent.\n",
    "\n",
    "\n",
    "\n",
    "### Qb: [OPTIONAL] Additional Concepts and Implementations\n",
    "\n",
    "And perhaps you could include (SHOULD/COULD):\n",
    "\n",
    "* (stochastic) mini-bach gradient decent, \n",
    "* interface to your bias and weights via `intercept_` and `coef_` attributes on your linear regressor `class`,\n",
    "* get/set functionality of your regressor, such that it is fully compatible with other Scikit-learn algorithms, try it out in say a `cross_val_score()` call from Scikit-learn,\n",
    "* test in via the smoke tests at the end of this Notebook,\n",
    "* testing it on MNIST data.\n",
    "\n",
    "With the following no-no's (WONT):\n",
    "\n",
    "* no learning graphs, no early stopping (we will do this in a later exercise),\n",
    "* no multi-linear regression,\n",
    "* no reuse of the Scikit-learn regressor,\n",
    "* no `C/C++` optimized implementation with a _thin_ Python interface (nifty, but out-of-scope for this cause),\n",
    "* no copy-paste of code from other sources WITHOUT a clear cite/reference for your source.\n",
    "\n",
    "### Qc: Testing and Test Data\n",
    "\n",
    "Use mainly very low-dimensional data for testing, say the IRIS set, since it might be very slow. Or create a simple low-dimensionality data generator.\n",
    "\n",
    "(There is a _micro_ data set in the function `GenerateData` in the smoke tests functions below, but better is to opt for an realistic data set.)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1]\n",
      " [4.9]\n",
      " [4.7]\n",
      " [4.6]\n",
      " [5. ]\n",
      " [5.4]\n",
      " [4.6]\n",
      " [5. ]\n",
      " [4.4]\n",
      " [4.9]\n",
      " [5.4]\n",
      " [4.8]\n",
      " [4.8]\n",
      " [4.3]\n",
      " [5.8]\n",
      " [5.7]\n",
      " [5.4]\n",
      " [5.1]\n",
      " [5.7]\n",
      " [5.1]\n",
      " [5.4]\n",
      " [5.1]\n",
      " [4.6]\n",
      " [5.1]\n",
      " [4.8]\n",
      " [5. ]\n",
      " [5. ]\n",
      " [5.2]\n",
      " [5.2]\n",
      " [4.7]\n",
      " [4.8]\n",
      " [5.4]\n",
      " [5.2]\n",
      " [5.5]\n",
      " [4.9]\n",
      " [5. ]\n",
      " [5.5]\n",
      " [4.9]\n",
      " [4.4]\n",
      " [5.1]\n",
      " [5. ]\n",
      " [4.5]\n",
      " [4.4]\n",
      " [5. ]\n",
      " [5.1]\n",
      " [4.8]\n",
      " [5.1]\n",
      " [4.6]\n",
      " [5.3]\n",
      " [5. ]\n",
      " [7. ]\n",
      " [6.4]\n",
      " [6.9]\n",
      " [5.5]\n",
      " [6.5]\n",
      " [5.7]\n",
      " [6.3]\n",
      " [4.9]\n",
      " [6.6]\n",
      " [5.2]\n",
      " [5. ]\n",
      " [5.9]\n",
      " [6. ]\n",
      " [6.1]\n",
      " [5.6]\n",
      " [6.7]\n",
      " [5.6]\n",
      " [5.8]\n",
      " [6.2]\n",
      " [5.6]\n",
      " [5.9]\n",
      " [6.1]\n",
      " [6.3]\n",
      " [6.1]\n",
      " [6.4]\n",
      " [6.6]\n",
      " [6.8]\n",
      " [6.7]\n",
      " [6. ]\n",
      " [5.7]\n",
      " [5.5]\n",
      " [5.5]\n",
      " [5.8]\n",
      " [6. ]\n",
      " [5.4]\n",
      " [6. ]\n",
      " [6.7]\n",
      " [6.3]\n",
      " [5.6]\n",
      " [5.5]\n",
      " [5.5]\n",
      " [6.1]\n",
      " [5.8]\n",
      " [5. ]\n",
      " [5.6]\n",
      " [5.7]\n",
      " [5.7]\n",
      " [6.2]\n",
      " [5.1]\n",
      " [5.7]\n",
      " [6.3]\n",
      " [5.8]\n",
      " [7.1]\n",
      " [6.3]\n",
      " [6.5]\n",
      " [7.6]\n",
      " [4.9]\n",
      " [7.3]\n",
      " [6.7]\n",
      " [7.2]\n",
      " [6.5]\n",
      " [6.4]\n",
      " [6.8]\n",
      " [5.7]\n",
      " [5.8]\n",
      " [6.4]\n",
      " [6.5]\n",
      " [7.7]\n",
      " [7.7]\n",
      " [6. ]\n",
      " [6.9]\n",
      " [5.6]\n",
      " [7.7]\n",
      " [6.3]\n",
      " [6.7]\n",
      " [7.2]\n",
      " [6.2]\n",
      " [6.1]\n",
      " [6.4]\n",
      " [7.2]\n",
      " [7.4]\n",
      " [7.9]\n",
      " [6.4]\n",
      " [6.3]\n",
      " [6.1]\n",
      " [7.7]\n",
      " [6.3]\n",
      " [6.4]\n",
      " [6. ]\n",
      " [6.9]\n",
      " [6.7]\n",
      " [6.9]\n",
      " [5.8]\n",
      " [6.8]\n",
      " [6.7]\n",
      " [6.7]\n",
      " [6.3]\n",
      " [6.5]\n",
      " [6.2]\n",
      " [5.9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "linRegModel = MyLinReg(0.0001)\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 0].reshape(-1, 1) # transponerer X?\n",
    "print(X)\n",
    "y = iris.data[:, 1]\n",
    "\n",
    "linRegModel.fit(X, y, \"GD\")\n",
    "\n",
    "predictions = linRegModel.predict(X)\n",
    "# print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alt under Qc her:\n",
    "### Qd: The Journaling of Your Regressor \n",
    "\n",
    "For the journal, write a full explanation of how you implemented the linear regressor, including a code walk-through (or mini-review of the most interesting parts).\n",
    "\n",
    "Vi har implementeret en linear regressor class “MyLinReg”, og med mulighed for at bruge en Gradient Descent eller Stochastic Gradient Descent solver, som afgør hvordan loss funktionen bliver minimeret. I vores fit-predict interface starter vi med at sætte variablerne fra constructeren på klassen selv, inden vi kalder vores solver metode (som enten er GD eller SGD).\n",
    "\n",
    "Solver metoden tager nogle iterationer, og mellem hver iteration opdaterer vi vores vægte (vores w, self.coef) ved at bruge vores loss funktion og gradienten af vores loss funktion. Vi bruger vores loss funktion til at evaluere vores model, og tjekker loss hver gang for at se om den bliver bedre hver iteration (hvor første gang er loss super høj for at barren for improvement er meget lav). Hvis loss ikke bliver bedre, så stopper vi vores iterationer efter efter den har kørt \"max_iter\" iterationer.\n",
    "\n",
    "Det her er alt sammen til for at prøve at minimere loss. Vi har også implementeret en R^2 score funktion, som vi bruger til at evaluere vores model. Vi har også gjort så man kan sætte en constant learning rate i form af parameteren eta0, der (sammen med gradient) styrer hvor drastisk vi skal ændre vores vægte.\n",
    "\n",
    "\n",
    "### Qe: Mathematical Foundation for Training a Linear Regressor\n",
    "\n",
    "You must also include the theoretical mathematical foundation for the linear regressor using the following equations and graphs (free to include in your journal without cite/reference), and relate them directly to your code:\n",
    "\n",
    "* Design matrix of size $(n, d)$ where each row is an input column vector $(\\mathbf{x}^{(i)})^\\top$ data sample of size $d$\n",
    "\n",
    "$$\n",
    "    \\def\\rem#1{}\n",
    "    \\rem{ITMAL: CEF def and LaTeX commands v01, remember: no newlines in defs}\n",
    "    \\rem{MACRO eq: equation <#1:lhs> <#2:rhs>}\n",
    "    \\def\\eq#1#2{#1 &=& #2\\\\}\n",
    "    \\rem{MACRO arr: array <#1:columns (lcr..)> <#2:content>}\n",
    "    \\def\\ar#1#2{\\begin{array}{#1}#2\\end{array}}\n",
    "    \\rem{MACRO ac: array column vector <#1:columns (lcr..)> <#2:content>}\n",
    "    \\def\\ac#1#2{\\left[\\ar{#1}{#2}\\right]}\n",
    "    \\rem{MACRO st: subscript text <#1:content>}\n",
    "    \\def\\st#1{_{\\textrm{#1}}}\n",
    "    \\rem{MACRO norm: norm caligari L <#1:content>}\n",
    "    \\def\\norm#1{{\\cal L}_{#1}}\n",
    "    \\rem{MACRO obs: ??}\n",
    "    \\def\\obs#1#2{#1_{\\textrm{\\scriptsize obs}}^{\\left(#2\\right)}}\n",
    "    \\rem{MACRO diff: math differetial operator <#1:content>}\n",
    "    \\def\\diff#1{\\mathrm{d}#1} \n",
    "    \\rem{MACRO half: shorthand for 1/2}\n",
    "    \\def\\half{\\frac{1}{2}}\n",
    "    \\rem{MACRO pfrac: partial fraction <#1:numenator> <#2:denumenator>}\n",
    "    \\def\\pfrac#1#2{\\frac{\\partial~#1}{\\partial~#2}}\n",
    "    \\rem{MACRO dfrac: differetial operator fraction <#1:numenator> <#2:denumenator>}\n",
    "    \\def\\dfrac#1#2{\\frac{\\mathrm{d}~#1}{\\mathrm{d}#2}}\n",
    "    \\rem{MACRO pown: power and parantesis (train/test..) <#1:content>}\n",
    "    \\def\\pown#1{^{(#1)}}\n",
    "    \\rem{MACROS powi, pown: shorthands for power (i) and (n)}\n",
    "    \\def\\powni{\\pown{i}}\n",
    "    \\def\\pownn{\\pown{n}}\n",
    "    \\rem{MACROS powtest, powertrain: power (test) and (train)}\n",
    "    \\def\\powtest{\\pown{\\textrm{\\scriptsize test}}}\n",
    "    \\def\\powtrain{\\pown{\\textrm{\\scriptsize train}}}\n",
    "    \\rem{MACRO boldmatrix: bold matix/vector notation} \n",
    "    \\def\\boldmatrix#1{\\mathbf{#1}} \n",
    "    \\rem{MACROS X,Z,x,y,w: bold X,Z,x etc.} \n",
    "    \\def\\bX{\\boldmatrix{X}}\n",
    "    \\def\\bZ{\\boldmatrix{Z}}\n",
    "    \\def\\bx{\\boldmatrix{x}}\n",
    "    \\def\\by{\\boldmatrix{y}}\n",
    "    \\def\\bw{\\boldmatrix{w}}\n",
    "    \\def\\bz{\\boldmatrix{z}}\n",
    "    \\def\\btheta{{\\boldsymbol\\theta}}\n",
    "    \\def\\bSigma{{\\boldsymbol\\Sigma}}\n",
    "    \\rem{MACROS stpred, sttrue: shorthand for subscript 'pred' and 'true'}\n",
    "    \\def\\stpred{\\st{pred}~}\n",
    "    \\def\\sttrue{\\st{true}~}\n",
    "    \\rem{MACROS ypred, ytrue:   shorthand for scalar y 'pred' and 'true'}\n",
    "    \\def\\ytrue{y\\sttrue}\n",
    "    \\def\\ypred{y\\stpred} \n",
    "    \\rem{MACROS bypred, bytrue: shorthand for vecor y 'pred' and 'true'} \n",
    "    \\def\\bypred{\\boldmatrix{y}\\stpred}\n",
    "    \\def\\bytrue{\\boldmatrix{y}\\sttrue} \n",
    "\\bX =\n",
    "        \\ac{cccc}{\n",
    "            x_1\\pown{1} & x_2\\pown{1} & \\cdots & x_d\\pown{1} \\\\\n",
    "            x_1\\pown{2} & x_2\\pown{2} & \\cdots & x_d\\pown{2} \\\\\n",
    "            \\vdots      &             &        & \\vdots      \\\\\n",
    "            x_1\\pownn   & x_2\\pownn   & \\cdots & x_d\\pownn   \\\\\n",
    "        } \n",
    "$$\n",
    "\n",
    "* Target ground-truth column vector of size $n$\n",
    "\n",
    "$$\n",
    "\\bytrue =\n",
    "  \\ac{c}{\n",
    "     y\\pown{1}\\sttrue \\\\\n",
    "     y\\pown{2}\\sttrue \\\\\n",
    "     \\vdots           \\\\\n",
    "     y\\pown{n}\\sttrue \\\\\n",
    "  } \n",
    "$$\n",
    "\n",
    "* Bias factor, and by convention in the following (prepend one)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\ac{c}{1\\\\\\bx\\powni} & \\mapsto \\bx\\powni\\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Weight column vector of size $d+1$ (i.e. with bias or intercept element $w_0$ prepended)\n",
    "\n",
    "$$\n",
    "\\bw =\n",
    "    \\ac{c}{\n",
    "         w_0    \\\\\n",
    "         w_1    \\\\\n",
    "         w_2    \\\\\n",
    "         \\vdots \\\\\n",
    "         w_d    \\\\\n",
    "    }\n",
    "$$\n",
    "\n",
    "* Linear regression model hypothesis function for a column vector input $\\bx\\powni$ of size $d$ and a column weight vector $\\bw$ of size $d+1$\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  ~~~~~~~~~~~~~~~\n",
    "  h(\\bx\\powni;\\bw) &= \\ypred\\powni \\\\\n",
    "                   &= \\bw^\\top \\bx\\powni ~~~~ (\\bx\\powni~\\textrm{with bias element})\\\\ \n",
    "                   &= w_0  \\cdot 1+ w_1 x_1\\powni + w_2 x_2\\powni + \\cdots + w_d x_d\\powni & \\\\\n",
    "}\n",
    "$$\n",
    "\n",
    "* Individual losses based on the $\\norm{2}^2$ (last part assuming one dimensional output)\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  L\\powni &= || \\ypred \\powni         - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= || h(\\bx\\powni;\\bw)      - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= || \\bw^\\top\\bx\\powni     - \\ytrue\\powni~ ||_2^2\\\\\n",
    "          &= \\left( \\bw^\\top\\bx\\powni - \\ytrue\\powni~ \\right)^2 ~~~~~ \\textrm{(only for 1D output)}\n",
    "}\n",
    "$$\n",
    "\n",
    "* MSE loss function\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "    \\textrm{MSE}(\\bX,\\bytrue;\\bw)  &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni \\\\\n",
    "                                   &= \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\bw^\\top\\bx\\powni - y\\powni\\sttrue \\right)^2\\\\\n",
    "                                   &= \\frac{1}{n} ||\\bX \\bw - \\bytrue||_2^2\n",
    "}\n",
    "$$                   \n",
    "\n",
    "\n",
    "* Loss function, proportional to (R)MSE\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "   J &= \\frac{1}{2} ||\\bX \\bw - \\bytrue||_2^2\\\\\n",
    "     &  \\propto \\textrm{MSE}\n",
    "}\n",
    "$$\n",
    "\n",
    "* Training: computing the optimal value of the $\\bw$ weight; that is finding the $\\bw$-value that minimizes the total loss\n",
    "\n",
    "$$\n",
    "  \\bw^* = \\textrm{argmin}_\\bw~J\\\\\n",
    "$$\n",
    "\n",
    "* Visualization of $\\textrm{argmin}_\\bw$ means to the argument of $\\bw$ that minimizes the $J$ function. The minimization can in 2-D visually be drawn as finding the lowest $J$ that for linear regression always forms a convex shape \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">\n",
    "\n",
    "#### Training I: The Closed-form Solution\n",
    "\n",
    "* Finding the optimal weight in a _one-step_ analytic expression \n",
    "\n",
    "$$\n",
    "  \\bw^* ~=~ \\left( \\bX^\\top \\bX \\right)^{-1}~ \\bX^\\top \\bytrue\n",
    "$$\n",
    "\n",
    "\n",
    "#### Training II: Numerical Optimization \n",
    "\n",
    "* The Gradient of the loss function\n",
    "\n",
    "$$   \n",
    "  \\nabla_\\bw~J = \\left[ \\frac{\\partial J}{\\partial w_1} ~~~~ \\frac{\\partial J}{\\partial w_2} ~~~~ \\ldots  ~~~~ \\frac{\\partial J}{\\partial w_d} \\right]^\\top\n",
    "$$\n",
    "\n",
    "* The Gradient for the based $J$\n",
    "\n",
    "$$\n",
    "\\ar{rl}{\n",
    "  \\nabla_\\bw J &= \\frac{2}{n} \\bX^\\top \\left( \\bX \\bw - \\bytrue \\right)\n",
    "}\n",
    "$$\n",
    "\n",
    "* The Gradient Decent Algorithm (GD)\n",
    "\n",
    "$$ \n",
    "  \\bw^{(step~N+1)}~ = \\bw^{(step~N)} ~ - \\eta \\nabla_{\\bw} J\n",
    "$$\n",
    "\n",
    "* Visualization of GD, showing $J$ as a function of two $w$-dimensions\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L05/Figs/minimization_gd.png\" alt=\"WARNING: could not get image from the server.\" style=\"height:240px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qf: Smoke testing\n",
    "\n",
    "Once ready, you can test your regressor via the test stub below, or create your own _test suite_.\n",
    "\n",
    "Be aware that setting the stepsize, $\\eta$, value can be tricky, and you might want to tune `eta0` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  DATA: 'IRIS'\n",
      "       SHAPES: X_train=(112, 4), X_test=(38, 4), y_train=(112,), y_\\\n",
      "       test=(38,)\n",
      "\n",
      "INFO:  TRAINING['MyLinReg']..\n",
      "\n",
      "y_pred_test= [-0.28556843  1.13373579  0.01817117  1.68589514 -0.17580118 -0.04884282\n",
      "  1.7994768   1.85260933  0.03300777 -0.11397953  1.70595963  1.68767051\n",
      "  1.20649114  1.33121576  1.38542419  1.72978412  1.68294196  0.04368367\n",
      " -0.12748242 -0.08876366  1.85133872  1.76352193 -0.03122348  1.7011734\n",
      "  1.5189086   1.9024899   1.40553292  0.97261464  0.09529863  1.22615821\n",
      "  1.60305387  1.80337968  1.4429234   0.01338495  1.30958946  1.19497288\n",
      "  0.0071798   1.95026507]\n",
      "\n",
      "INFO:  SCORE['MyLinReg'] = 0.912\n",
      "\n",
      "INFO:  TRAINING['SGDRegressor']..\n",
      "\n",
      "y_pred_test= [-1.37082945e-01  1.26618587e+00  4.04377947e-02  1.51182748e+00\n",
      " -1.70247910e-01 -5.35396155e-02  1.81368086e+00  1.91227563e+00\n",
      " -2.13865640e-02 -2.12036317e-01  1.70126108e+00  1.75626708e+00\n",
      "  1.39348947e+00  1.29231155e+00  1.32751074e+00  1.80745058e+00\n",
      "  1.63974236e+00  5.95025105e-04 -6.64971314e-02 -4.81235284e-02\n",
      "  1.84836375e+00  1.60798353e+00  8.26026675e-05  1.61072789e+00\n",
      "  1.66369968e+00  1.83038824e+00  1.23397698e+00  1.00094169e+00\n",
      "  1.86061214e-03  1.13923094e+00  1.42138929e+00  1.58276450e+00\n",
      "  1.21443024e+00 -5.00953877e-02  1.28851612e+00  1.21134496e+00\n",
      " -1.02662670e-01  2.08237314e+00]\n",
      "\n",
      "INFO:  SCORE['SGDRegressor'] = 0.918\n",
      "\n",
      "INFO:  ##############################################\n",
      "       \n",
      "\n",
      "INFO:  DATA: 'MNIST'\n",
      "       SHAPES: X_train=(52500, 784), X_test=(17500, 784), y_train=(\\\n",
      "       52500,), y_test=(17500,)\n",
      "\n",
      "INFO:  TRAINING['MyLinReg']..\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\morte\\AppData\\Local\\Temp\\ipykernel_2556\\2242980192.py:133: RuntimeWarning: overflow encountered in square\n",
      "  loss = np.sqrt(np.mean(((X @ self.coef_) - y) ** 2))\n",
      "C:\\Users\\morte\\AppData\\Local\\Temp\\ipykernel_2556\\2242980192.py:134: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  if abs(best_loss - loss) < self.tolerance:\n",
      "C:\\Users\\morte\\AppData\\Local\\Temp\\ipykernel_2556\\2242980192.py:130: RuntimeWarning: overflow encountered in multiply\n",
      "  gradient = 2 * X[i] * error\n",
      "C:\\Users\\morte\\AppData\\Local\\Temp\\ipykernel_2556\\2242980192.py:130: RuntimeWarning: invalid value encountered in multiply\n",
      "  gradient = 2 * X[i] * error\n",
      "C:\\Users\\morte\\AppData\\Local\\Temp\\ipykernel_2556\\2242980192.py:131: RuntimeWarning: invalid value encountered in subtract\n",
      "  self.coef_ -= gradient * self.eta0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred_test= [nan nan nan ... nan nan nan]\n",
      "\n",
      "INFO:  SCORE['MyLinReg'] = nan\n",
      "\n",
      "INFO:  TRAINING['SGDRegressor']..\n",
      "\n",
      "y_pred_test= [-5.58728329e+09  1.87196533e+10 -3.80548545e+10 ... -9.99093024e+09\n",
      " -3.65431514e+09  1.07277652e+12]\n",
      "\n",
      "INFO:  SCORE['SGDRegressor'] = -622067138313273925959680.000\n",
      "\n",
      "INFO:  ##############################################\n",
      "       \n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "### OLD SMOKETEST\n",
    "\n",
    "from sklearn.linear_model    import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.pipeline        import Pipeline\n",
    "\n",
    "try:\n",
    "    from libitmal import dataloaders\n",
    "except Exception as ex:\n",
    "    Err(\"can not import dataloaders form libitmal, and then I can not run the TestAndCompareRegressors smoke-test, sorry!\", ex)\n",
    "\n",
    "def TestAndCompareRegressors():\n",
    "    for f in [(\"IRIS\",  dataloaders.IRIS_GetDataSet,  1E-2),\n",
    "              (\"MNIST\", dataloaders.MNIST_GetDataSet, 1E-3)]:\n",
    "        \n",
    "        # NOTE: f-tuble is (<name>, <data-loader-function-pointer>, <eps0>)\n",
    "        data = f[1]() # returns (X, y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data[0], data[1])\n",
    "        \n",
    "        Info(f\"DATA: '{f[0]}'\\n\\tSHAPES: X_train={X_train.shape}, X_test={X_test.shape}, y_train={y_train.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "        eta0 = f[2] # an adaptive learning rate is really needed here!\n",
    "        regressor0 = MyLinReg(eta0=eta0, max_iter=1000)\n",
    "        regressor1 = SGDRegressor()    \n",
    "\n",
    "        for r in [(\"MyLinReg\", regressor0), (\"SGDRegressor\", regressor1)]:\n",
    "            Info(f\"\\nTRAINING['{r[0]}']..\")\n",
    "            \n",
    "            pipe = Pipeline([('scaler', StandardScaler()), r])\n",
    "            # pipe.named_steps['mylinreg'].fit(X_train, y_train, \"SGD\")\n",
    "            # pipe.named_steps['mylinreg'].fit(X_train, y_train, method=\"SGD\")\n",
    "            pipe.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_test = pipe.predict(X_test)\n",
    "            \n",
    "            PrintMatrix(y_pred_test, label=\"y_pred_test=\", precision=4)\n",
    "            print()\n",
    "            \n",
    "            r2 = pipe.score(X_test, y_test)\n",
    "            Info(f\"SCORE['{r[0]}'] = {Col('lblue')}{r2:0.3f}{ColEnd()}\")\n",
    "            \n",
    "        Info(\"\\n##############################################\\n\")\n",
    "\n",
    "# somewhat more verbose testing, you regressor will likely fail on MNIST \n",
    "# or at least be very, very slow...\n",
    "TestAndCompareRegressors()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qg: [OPTIONAL] More Smoke-Testing\n",
    "\n",
    "Do you dare to compare your custom regressor with the SGD regressor in Scikit-learn on both the IRIS and MNIST datasets?\n",
    "\n",
    "Then run the next smoke-test function, but the code might requre `eta0` anb `max_iter` hyperparamter tuning).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  DATA: 'IRIS'\n",
      "       SHAPES: X_train=(112, 4), X_test=(38, 4), y_train=(112,), y_\\\n",
      "       test=(38,)\n",
      "\n",
      "INFO:  TRAINING['MyLinReg']..\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "MyLinReg.fit() missing 1 required positional argument: 'method'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 43\u001b[0m\n\u001b[0;32m     39\u001b[0m         Info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m##############################################\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# somewhat more verbose testing, you regressor will likely fail on MNIST \u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# or at least be very, very slow...\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m TestAndCompareRegressors()\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOK\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[44], line 29\u001b[0m, in \u001b[0;36mTestAndCompareRegressors\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m Info(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTRAINING[\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m pipe \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaler\u001b[39m\u001b[38;5;124m'\u001b[39m, StandardScaler()), r])\n\u001b[1;32m---> 29\u001b[0m pipe\u001b[38;5;241m.\u001b[39mfit(X_train, y_train) \u001b[38;5;66;03m#### HER HER HER\u001b[39;00m\n\u001b[0;32m     31\u001b[0m y_pred_test \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     33\u001b[0m PrintMatrix(y_pred_test, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred_test=\u001b[39m\u001b[38;5;124m\"\u001b[39m, precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\morte\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\morte\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlast_step_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: MyLinReg.fit() missing 1 required positional argument: 'method'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model    import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.pipeline        import Pipeline\n",
    "\n",
    "try:\n",
    "    from libitmal import dataloaders\n",
    "except Exception as ex:\n",
    "    Err(\"can not import dataloaders form libitmal, and then I can not run the TestAndCompareRegressors smoke-test, sorry!\", ex)\n",
    "\n",
    "def TestAndCompareRegressors():\n",
    "    for f in [(\"IRIS\",  dataloaders.IRIS_GetDataSet,  1E-2),\n",
    "              (\"MNIST\", dataloaders.MNIST_GetDataSet, 1E-3)]:\n",
    "        \n",
    "        # NOTE: f-tuble is (<name>, <data-loader-function-pointer>, <eps0>)\n",
    "        data = f[1]() # returns (X, y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data[0], data[1])\n",
    "        \n",
    "        Info(f\"DATA: '{f[0]}'\\n\\tSHAPES: X_train={X_train.shape}, X_test={X_test.shape}, y_train={y_train.shape}, y_test={y_test.shape}\")\n",
    "\n",
    "        eta0 = f[2] # an adaptive learning rate is really needed here!\n",
    "        regressor0 = MyLinReg(eta0=eta0, max_iter=1000)\n",
    "        regressor1 = SGDRegressor()    \n",
    "\n",
    "        for r in [(\"MyLinReg\", regressor0), (\"SGDRegressor\", regressor1)]:\n",
    "            Info(f\"\\nTRAINING['{r[0]}']..\")\n",
    "            \n",
    "            pipe = Pipeline([('scaler', StandardScaler()), r])\n",
    "            pipe.fit(X_train, y_train) #### HER HER HER\n",
    "            \n",
    "            y_pred_test = pipe.predict(X_test)\n",
    "            \n",
    "            PrintMatrix(y_pred_test, label=\"y_pred_test=\", precision=4)\n",
    "            print()\n",
    "            \n",
    "            r2 = pipe.score(X_test, y_test)\n",
    "            Info(f\"SCORE['{r[0]}'] = {Col('lblue')}{r2:0.3f}{ColEnd()}\")\n",
    "            \n",
    "        Info(\"\\n##############################################\\n\")\n",
    "\n",
    "# somewhat more verbose testing, you regressor will likely fail on MNIST \n",
    "# or at least be very, very slow...\n",
    "TestAndCompareRegressors()\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qh Conclusion\n",
    "\n",
    "As always, take some time to fine-tune your regressor, perhaps just some code-refactoring, cleaning out 'bad' code, and summarize all your findings\n",
    " above. \n",
    "\n",
    "In other words, write a conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":- | :- |\n",
    "2022-12-22| CEF, initial draft. \n",
    "2023-02-26| CEF, first release.\n",
    "2023-02-28| CEF, fix a few issues related to import from libitmal, added Info and color output.\n",
    "2024-09-19| CEF, major overhaul, change math/text and code snippets.\n",
    "2024-09-25| CEF, final fixes, tests, and proof-reading. Moved early stopping and learning graphs to a later excercise.\n",
    "2024-10-04| CEF, clarified Qa with respect to what-is-to-be implemented and what-is-to-be described in text only."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
