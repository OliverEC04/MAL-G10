{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:(in code and text..)\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "mnist_SGD = SGDClassifier()\n",
    "\n",
    "\n",
    "# vi starter med at load mnist dataeN:\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData('mnist')\n",
    "\n",
    "# så scaler vi dataen:\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# og så laver vi en grid search med parametrene:\n",
    "tuning_parameters = {\n",
    "    'loss': ['log_loss', 'perceptron'],\n",
    "    'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "    'learning_rate': ['optimal', 'invscaling', 'adaptive'],\n",
    "    'n_iter_no_change': [50, 500, 1000, 5000, 10000],\n",
    "}\n",
    "CV = 5\n",
    "grid_tuned = GridSearchCV(mnist_SGD,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "# så træner vi modellen:\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "# og til sidst printer vi hvad de bedste parametre var:\n",
    "b2, m2 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "\n",
    "# #Det første vi tunede var “temperature” når vi predictede. En lavere temperatur lavede stadig ord der lød dansk, og skabte sammenhængende sætninger, men da vi tredoblede temperaturen lød den pludselig svensk. Dette er fordi temperaturen indikerer hvor tilfældig en prediction er, og hvis man gerne vil have en lidt mere kreativ respons, kan man øge sin temp parameter for at få den til at sige nogle lidt mere forskellige ting. \n",
    "# Vi satte antal lag op fra 3 til 10 og observerede, at den outputtede flere reelle ord end før hvor der var meget der ikke var ord. Det er til at forvente, siden flere lag betyder at den har flere muligheder for at lære og finde mønstre i dataen, og det lader ikke til at den har overfittet.  \n",
    "\n",
    "\n",
    "\n",
    "# #Skriv noget om tweakede parametre og træningsprocessen\n",
    "# For træningsprocessen gik vi ind og ændrede i config filen for at øge antallet af lag, og i Makefilen for at øge antallet af iterationer den kørte. Vi ændrede også nogle indstillinger for predictions i Makefilen, som fx temperature og hvor mange samples den skulle lave. Vi skulle også ændre i navnet på datasættet i Makefilen, så den kunne finde det rigtige datasæt. \n",
    "\n",
    "\n",
    "# Relatér outputtet fra nanoGPT til koncepter vi kender, som iterations, train- and val- loss optimizers (AdamW) o.s.v.\n",
    "# noget om config files?\n",
    "\n",
    "# Vi har prøvet at træne nanoGPT til at forudsige det næste ord gennem en masse iterationer, hvor modellen er blevet en smule bedre efter vi har øget antallet af iterationerne. Den er blevet en smule bedre efter hver iteration, takket være vores optimizer funktion “AdamW”. \n",
    "# AdamW er en optimizer der er lavet til at kunne træne transformere, og det gør den ved at bruge \"momentum\", for at kunne komme over bakkerne i loss.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# assert False, \"participate in the Search Quest---remember to publish your result(s) on Brightspace.\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
