{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75a3a250",
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras import layers, models\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "IMG_SIZE = 512\n",
        "# ------ Dette kodestykke er blevet lavet i samarbejde med ChatGPT ------\n",
        "def load_images_from_folder(folder_path, label, img_size):\n",
        "    data = []\n",
        "    labels = []\n",
        "    for img_name in os.listdir(folder_path):\n",
        "        img_path = os.path.join(folder_path, img_name)\n",
        "        img = load_img(img_path, target_size=(img_size, img_size))\n",
        "        img_array = img_to_array(img) / 255.0\n",
        "        data.append(img_array)\n",
        "        labels.append(label)\n",
        "    return data, labels\n",
        "# ------ Dette kodestykke er blevet lavet i samarbejde med ChatGPT ------\n",
        "\n",
        "# training data\n",
        "train_is_path = \"ice_cream/train/ice_cream\"\n",
        "train_not_path = \"ice_cream/train/non_ice_cream\"\n",
        "train_data_is, labels_is = load_images_from_folder(train_is_path, 1, IMG_SIZE)\n",
        "train_data_not, labels_not = load_images_from_folder(train_not_path, 0, IMG_SIZE)\n",
        "\n",
        "X_train = np.array(train_data_is + train_data_not)\n",
        "y_train = np.array(labels_is + labels_not)\n",
        "\n",
        "# testing data\n",
        "test_is_path = \"ice_cream/test/ice_cream\"\n",
        "test_not_path = \"ice_cream/test/non_ice_cream\"\n",
        "test_data_is, test_labels_is = load_images_from_folder(test_is_path, 1, IMG_SIZE)\n",
        "test_data_not, test_labels_not = load_images_from_folder(test_not_path, 0, IMG_SIZE)\n",
        "\n",
        "X_test = np.array(test_data_is + test_data_not)\n",
        "y_test = np.array(test_labels_is + test_labels_not)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e018faa7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK(function setup)\n"
          ]
        }
      ],
      "source": [
        "# Rapport generator funktioner\n",
        "from time import time\n",
        "import numpy as np\n",
        "import sys\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from sklearn import datasets\n",
        "\n",
        "# from libitmal import dataloaders as itmaldataloaders # Needed for load of iris, moon and mnist\n",
        "\n",
        "currmode=\"N/A\" # GLOBAL var!\n",
        "\n",
        "def SearchReport(model): \n",
        "    \n",
        "    def GetBestModelCTOR(model, best_params):\n",
        "        def GetParams(best_params):\n",
        "            ret_str=\"\"          \n",
        "            for key in sorted(best_params):\n",
        "                value = best_params[key]\n",
        "                temp_str = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n",
        "                if len(ret_str)>0:\n",
        "                    ret_str += ','\n",
        "                ret_str += f'{key}={temp_str}{value}{temp_str}'  \n",
        "            return ret_str          \n",
        "        try:\n",
        "            param_str = GetParams(best_params)\n",
        "            return type(model).__name__ + '(' + param_str + ')' \n",
        "        except:\n",
        "            return \"N/A(1)\"\n",
        "        \n",
        "    print(\"\\nBest model set found on train set:\")\n",
        "    print()\n",
        "    print(f\"\\tbest parameters={model.best_params_}\")\n",
        "    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n",
        "    print(f\"\\tbest index={model.best_index_}\")\n",
        "    print()\n",
        "    print(f\"Best estimator CTOR:\")\n",
        "    print(f\"\\t{model.best_estimator_}\")\n",
        "    print()\n",
        "    try:\n",
        "        print(f\"Grid scores ('{model.scoring}') on development set:\")\n",
        "        means = model.cv_results_['mean_test_score']\n",
        "        stds  = model.cv_results_['std_test_score']\n",
        "        i=0\n",
        "        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
        "            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
        "            i += 1\n",
        "    except:\n",
        "        print(\"WARNING: the random search do not provide means/stds\")\n",
        "    \n",
        "    global currmode                \n",
        "    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"   \n",
        "    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_ \n",
        "\n",
        "def ClassificationReport(model, X_test, y_test, target_names=None):\n",
        "    assert X_test.shape[0]==y_test.shape[0]\n",
        "    print(\"\\nDetailed classification report:\")\n",
        "    print(\"\\tThe model is trained on the full development set.\")\n",
        "    print(\"\\tThe scores are computed on the full evaluation set.\")\n",
        "    print()\n",
        "    y_true, y_pred = y_test, model.predict(X_test)                 \n",
        "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
        "    print()\n",
        "    \n",
        "def FullReport(model, X_test, y_test, t):\n",
        "    print(f\"SEARCH TIME: {t:0.2f} sec\")\n",
        "    beststr, bestmodel = SearchReport(model)\n",
        "    ClassificationReport(model, X_test, y_test)    \n",
        "    print(f\"CTOR for best model: {bestmodel}\\n\")\n",
        "    print(f\"{beststr}\\n\")\n",
        "    return beststr, bestmodel\n",
        "    \n",
        "# def LoadAndSetupData(mode, test_size=0.3):\n",
        "#     assert test_size>=0.0 and test_size<=1.0\n",
        "    \n",
        "#     def ShapeToString(Z):\n",
        "#         n = Z.ndim\n",
        "#         s = \"(\"\n",
        "#         for i in range(n):\n",
        "#             s += f\"{Z.shape[i]:5d}\"\n",
        "#             if i+1!=n:\n",
        "#                 s += \";\"\n",
        "#         return s+\")\"\n",
        "\n",
        "#     global currmode\n",
        "#     currmode=mode\n",
        "#     print(f\"DATA: {currmode}..\")\n",
        "    \n",
        "#     if mode=='moon':\n",
        "#         X, y = itmaldataloaders.MOON_GetDataSet(n_samples=5000, noise=0.2)\n",
        "#         itmaldataloaders.MOON_Plot(X, y)\n",
        "#     elif mode=='mnist':\n",
        "#         X, y = itmaldataloaders.MNIST_GetDataSet(load_mode=0)\n",
        "#         if X.ndim==3:\n",
        "#             X=np.reshape(X, (X.shape[0], -1))\n",
        "#     elif mode=='iris':\n",
        "#         X, y = itmaldataloaders.IRIS_GetDataSet()\n",
        "#     else:\n",
        "#         raise ValueError(f\"could not load data for that particular mode='{mode}', only 'moon'/'mnist'/'iris' supported\")\n",
        "        \n",
        "#     print(f'  org. data:  X.shape      ={ShapeToString(X)}, y.shape      ={ShapeToString(y)}')\n",
        "\n",
        "#     assert X.ndim==2\n",
        "#     assert X.shape[0]==y.shape[0]\n",
        "#     assert y.ndim==1 or (y.ndim==2 and y.shape[1]==0)    \n",
        "    \n",
        "#     X_train, X_test, y_train, y_test = train_test_split(\n",
        "#         X, y, test_size=test_size, random_state=0, shuffle=True\n",
        "#     )\n",
        "    \n",
        "#     print(f'  train data: X_train.shape={ShapeToString(X_train)}, y_train.shape={ShapeToString(y_train)}')\n",
        "#     print(f'  test data:  X_test.shape ={ShapeToString(X_test)}, y_test.shape ={ShapeToString(y_test)}')\n",
        "#     print()\n",
        "    \n",
        "#     return X_train, X_test, y_train, y_test\n",
        "\n",
        "def TryKerasImport(verbose=True):\n",
        "    \n",
        "    kerasok = True\n",
        "    try:\n",
        "        import keras as keras_try\n",
        "    except:\n",
        "        kerasok = False\n",
        "\n",
        "    tensorflowkerasok = True\n",
        "    try:\n",
        "        import tensorflow.keras as tensorflowkeras_try\n",
        "    except:\n",
        "        tensorflowkerasok = False\n",
        "        \n",
        "    ok = kerasok or tensorflowkerasok\n",
        "    \n",
        "    if not ok and verbose:\n",
        "        if not kerasok:\n",
        "            print(\"WARNING: importing 'keras' failed\", file=sys.stderr)\n",
        "        if not tensorflowkerasok:\n",
        "            print(\"WARNING: importing 'tensorflow.keras' failed\", file=sys.stderr)\n",
        "\n",
        "    return ok\n",
        "    \n",
        "print(f\"OK(function setup\" + (\"\" if TryKerasImport() else \", hope MNIST loads works because it seems you miss the installation of Keras or Tensorflow!\") + \")\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1f40521",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x_test:  (200, 512, 512, 3)\n",
            "y_test:  (200,)\n",
            "x_train:  (1800, 512, 512, 3)\n",
            "y_train:  (1800,)\n",
            "Epoch 1/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 3s/step - accuracy: 0.5322 - loss: 2.1089 - val_accuracy: 0.6850 - val_loss: 0.6503\n",
            "Epoch 2/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 3s/step - accuracy: 0.6695 - loss: 0.6343 - val_accuracy: 0.6600 - val_loss: 0.6350\n",
            "Epoch 3/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 3s/step - accuracy: 0.6936 - loss: 0.6009 - val_accuracy: 0.6800 - val_loss: 0.6031\n",
            "Epoch 4/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 3s/step - accuracy: 0.7589 - loss: 0.4954 - val_accuracy: 0.7600 - val_loss: 0.4887\n",
            "Epoch 5/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 3s/step - accuracy: 0.8520 - loss: 0.3529 - val_accuracy: 0.6750 - val_loss: 0.7139\n",
            "Epoch 6/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 3s/step - accuracy: 0.9463 - loss: 0.1471 - val_accuracy: 0.6850 - val_loss: 0.7455\n",
            "Epoch 7/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 3s/step - accuracy: 0.9813 - loss: 0.0689 - val_accuracy: 0.7450 - val_loss: 0.9615\n",
            "Epoch 8/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 3s/step - accuracy: 0.9948 - loss: 0.0188 - val_accuracy: 0.7050 - val_loss: 0.9256\n",
            "Epoch 9/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 3s/step - accuracy: 0.9944 - loss: 0.0309 - val_accuracy: 0.6100 - val_loss: 1.1621\n",
            "Epoch 10/10\n",
            "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 3s/step - accuracy: 0.9877 - loss: 0.0347 - val_accuracy: 0.6450 - val_loss: 1.3612\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">510</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">510</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">255</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">255</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">253</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">253</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">124</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">984064</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │    <span style=\"color: #00af00; text-decoration-color: #00af00\">62,980,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m510\u001b[0m, \u001b[38;5;34m510\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m255\u001b[0m, \u001b[38;5;34m255\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m253\u001b[0m, \u001b[38;5;34m253\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m124\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m984064\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │    \u001b[38;5;34m62,980,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">189,109,637</span> (721.40 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m189,109,637\u001b[0m (721.40 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">63,036,545</span> (240.47 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m63,036,545\u001b[0m (240.47 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">126,073,092</span> (480.93 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m126,073,092\u001b[0m (480.93 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Data information\n",
        "print(\"Data Shapes:\")\n",
        "print(f\"x_train: {X_train.shape} | y_train: {y_train.shape}\")\n",
        "print(f\"x_test: {X_test.shape} | y_test: {y_test.shape}\\n\")\n",
        "\n",
        "# Shuffle data\n",
        "X_train, y_train = shuffle(X_train, y_train, random_state=42)\n",
        "X_test, y_test = shuffle(X_test, y_test, random_state=42)\n",
        "\n",
        "# Enhanced CNN model with regularization\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.2),  # Added dropout for regularization\n",
        "    \n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.3),  # Increased dropout\n",
        "    \n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),  # Added more filters\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    \n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),  # Increased units\n",
        "    layers.Dropout(0.5),  # Higher dropout for dense layer\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile with lower learning rate\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy', Precision(), Recall()])\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "# Train model with validation\n",
        "print(\"Training model...\\n\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    epochs=30,  # Increased epochs\n",
        "    batch_size=32,\n",
        "    validation_data=(X_test, y_test),\n",
        "    callbacks=[early_stopping],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# Evaluate model\n",
        "print(\"\\nModel Evaluation:\")\n",
        "test_loss, test_acc, test_precision, test_recall = model.evaluate(X_test, y_test)\n",
        "test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
        "\n",
        "print(f'\\nTest accuracy: {test_acc:.4f}')\n",
        "print(f'Test precision: {test_precision:.4f}')\n",
        "print(f'Test recall: {test_recall:.4f}')\n",
        "print(f'Test F1-score: {test_f1:.4f}')\n",
        "\n",
        "# Generate predictions\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = (y_pred > 0.5).astype(\"int32\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred_classes, \n",
        "                          target_names=['Not Ice Cream', 'Ice Cream']))\n",
        "\n",
        "# Enhanced confusion matrix plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "conf_mat = confusion_matrix(y_test, y_pred_classes)\n",
        "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Not Ice Cream', 'Ice Cream'],\n",
        "            yticklabels=['Not Ice Cream', 'Ice Cream'],\n",
        "            annot_kws={\"size\": 14})\n",
        "plt.title('Confusion Matrix', fontsize=16)\n",
        "plt.ylabel('True Label', fontsize=14)\n",
        "plt.xlabel('Predicted Label', fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "# Enhanced training history plots\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "# Accuracy plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train', linewidth=2)\n",
        "plt.plot(history.history['val_accuracy'], label='Validation', linewidth=2)\n",
        "plt.title('Model Accuracy', fontsize=14)\n",
        "plt.ylabel('Accuracy', fontsize=12)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Loss plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train', linewidth=2)\n",
        "plt.plot(history.history['val_loss'], label='Validation', linewidth=2)\n",
        "plt.title('Model Loss', fontsize=14)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.legend(fontsize=12)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save model\n",
        "model.save('ice_cream_classifier_improved.h5')\n",
        "print(\"\\nModel saved as 'ice_cream_classifier_improved.h5'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b7f78e04",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 2.19.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: C:\\Users\\hassa\\anaconda3\\Lib\\site-packages\n",
            "Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, requests, setuptools, six, tensorboard, termcolor, typing-extensions, wrapt\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "# !pip install scikeras\n",
        "# !pip install --upgrade keras\n",
        "# !pip install --upgrade tensorflow\n",
        "!pip show tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "10897a37",
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'scikeras'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscikeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasClassifier\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RandomizedSearchCV\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scikeras'"
          ]
        }
      ],
      "source": [
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from time import time\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import tensorflow as tf\n",
        "\n",
        "IMG_SIZE = 512\n",
        "\n",
        "def build_model(activation='relu'):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation=activation, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation=activation),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation=activation),\n",
        "        Flatten(),\n",
        "        Dense(64, activation=activation),\n",
        "        Dense(1, activation='sigmoid')  \n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_model(activation='relu'):\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (3, 3), activation=activation, input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation=activation),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(64, (3, 3), activation=activation),\n",
        "        Flatten(),\n",
        "        Dense(64, activation=activation),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model  \n",
        "\n",
        "param_grid = {\n",
        "    \"batch_size\": [16, 32],\n",
        "    \"epochs\": [5, 10],\n",
        "    \"model__optimizer\": ['adam', 'sgd'],\n",
        "    \"model__activation\": ['relu', 'tanh']\n",
        "}\n",
        "\n",
        "random_tuned = RandomizedSearchCV(\n",
        "    estimator=keras_clf,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=5,\n",
        "    scoring='accuracy',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "start = time()\n",
        "random_tuned.fit(X_train, y_train)  \n",
        "t = time() - start\n",
        "\n",
        "y_pred = random_tuned.predict(X_test)\n",
        "print(\"\\n--- Classification Report ---\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"\\n--- Confusion Matrix ---\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(f\"\\nBest Parameters: {random_tuned.best_params_}\")\n",
        "print(f\"Search took {t:.2f} seconds.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
